[{"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0002", "title": "sparsity-certifying graph decompositions", "abstract": "we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte-nash-williams characterization of arboricity. we also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.", "date_create": "2007-03-30", "area": "math.co cs.cg", "authors": ["streinu", "theran"]}, {"idpaper": "0704.0046", "title": "a limit relation for entropy and channel capacity per unit cost", "abstract": "in a quantum mechanical model, diosi, feldmann and kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity. the conjecture is proven in this paper for density matrices. the first proof is analytic and uses the quantum law of large numbers. the second one clarifies the relation to channel capacity per unit cost for classical-quantum channels. both proofs lead to generalization of the conjecture.", "date_create": "2007-04-01", "area": "quant-ph cs.it math.it", "authors": ["csiszar", "hiai", "petz"]}, {"idpaper": "0704.0046", "title": "a limit relation for entropy and channel capacity per unit cost", "abstract": "in a quantum mechanical model, diosi, feldmann and kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity. the conjecture is proven in this paper for density matrices. the first proof is analytic and uses the quantum law of large numbers. the second one clarifies the relation to channel capacity per unit cost for classical-quantum channels. both proofs lead to generalization of the conjecture.", "date_create": "2007-04-01", "area": "quant-ph cs.it math.it", "authors": ["csiszar", "hiai", "petz"]}, {"idpaper": "0704.0046", "title": "a limit relation for entropy and channel capacity per unit cost", "abstract": "in a quantum mechanical model, diosi, feldmann and kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity. the conjecture is proven in this paper for density matrices. the first proof is analytic and uses the quantum law of large numbers. the second one clarifies the relation to channel capacity per unit cost for classical-quantum channels. both proofs lead to generalization of the conjecture.", "date_create": "2007-04-01", "area": "quant-ph cs.it math.it", "authors": ["csiszar", "hiai", "petz"]}, {"idpaper": "0704.0047", "title": "intelligent location of simultaneously active acoustic emission sources:   part i", "abstract": "the intelligent acoustic emission locator is described in part i, while part ii discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. this article describes an intelligent acoustic emission source locator. the intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. locator performance was tested on different test specimens. tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. this is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", "date_create": "2007-04-01", "area": "cs.ne cs.ai", "authors": ["kosel", "grabec"]}, {"idpaper": "0704.0047", "title": "intelligent location of simultaneously active acoustic emission sources:   part i", "abstract": "the intelligent acoustic emission locator is described in part i, while part ii discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. this article describes an intelligent acoustic emission source locator. the intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. locator performance was tested on different test specimens. tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. this is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", "date_create": "2007-04-01", "area": "cs.ne cs.ai", "authors": ["kosel", "grabec"]}, {"idpaper": "0704.0047", "title": "intelligent location of simultaneously active acoustic emission sources:   part i", "abstract": "the intelligent acoustic emission locator is described in part i, while part ii discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. this article describes an intelligent acoustic emission source locator. the intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. locator performance was tested on different test specimens. tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. this is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", "date_create": "2007-04-01", "area": "cs.ne cs.ai", "authors": ["kosel", "grabec"]}, {"idpaper": "0704.0047", "title": "intelligent location of simultaneously active acoustic emission sources:   part i", "abstract": "the intelligent acoustic emission locator is described in part i, while part ii discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. this article describes an intelligent acoustic emission source locator. the intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. locator performance was tested on different test specimens. tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. this is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", "date_create": "2007-04-01", "area": "cs.ne cs.ai", "authors": ["kosel", "grabec"]}, {"idpaper": "0704.0047", "title": "intelligent location of simultaneously active acoustic emission sources:   part i", "abstract": "the intelligent acoustic emission locator is described in part i, while part ii discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources.   the location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. this article describes an intelligent acoustic emission source locator. the intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. locator performance was tested on different test specimens. tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. the location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. this is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", "date_create": "2007-04-01", "area": "cs.ne cs.ai", "authors": ["kosel", "grabec"]}, {"idpaper": "0704.0062", "title": "on-line viterbi algorithm and its relationship to random walks", "abstract": "in this paper, we introduce the on-line viterbi algorithm for decoding hidden markov models (hmms) in much smaller than linear space. our analysis on two-state hmms suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state hmm can be as low as $\\theta(m\\log n)$, without a significant slow-down compared to the classical viterbi algorithm. classical viterbi algorithm requires $o(mn)$ space, which is impractical for analysis of long dna sequences (such as complete human genome chromosomes) and for continuous data streams. we also experimentally demonstrate the performance of the on-line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences.", "date_create": "2007-03-31", "area": "cs.ds", "authors": ["\u0161r\u00e1mek", "brejov\u00e1", "vina\u0159"]}, {"idpaper": "0704.0062", "title": "on-line viterbi algorithm and its relationship to random walks", "abstract": "in this paper, we introduce the on-line viterbi algorithm for decoding hidden markov models (hmms) in much smaller than linear space. our analysis on two-state hmms suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state hmm can be as low as $\\theta(m\\log n)$, without a significant slow-down compared to the classical viterbi algorithm. classical viterbi algorithm requires $o(mn)$ space, which is impractical for analysis of long dna sequences (such as complete human genome chromosomes) and for continuous data streams. we also experimentally demonstrate the performance of the on-line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences.", "date_create": "2007-03-31", "area": "cs.ds", "authors": ["\u0161r\u00e1mek", "brejov\u00e1", "vina\u0159"]}, {"idpaper": "0704.0062", "title": "on-line viterbi algorithm and its relationship to random walks", "abstract": "in this paper, we introduce the on-line viterbi algorithm for decoding hidden markov models (hmms) in much smaller than linear space. our analysis on two-state hmms suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state hmm can be as low as $\\theta(m\\log n)$, without a significant slow-down compared to the classical viterbi algorithm. classical viterbi algorithm requires $o(mn)$ space, which is impractical for analysis of long dna sequences (such as complete human genome chromosomes) and for continuous data streams. we also experimentally demonstrate the performance of the on-line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences.", "date_create": "2007-03-31", "area": "cs.ds", "authors": ["\u0161r\u00e1mek", "brejov\u00e1", "vina\u0159"]}, {"idpaper": "0704.0062", "title": "on-line viterbi algorithm and its relationship to random walks", "abstract": "in this paper, we introduce the on-line viterbi algorithm for decoding hidden markov models (hmms) in much smaller than linear space. our analysis on two-state hmms suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state hmm can be as low as $\\theta(m\\log n)$, without a significant slow-down compared to the classical viterbi algorithm. classical viterbi algorithm requires $o(mn)$ space, which is impractical for analysis of long dna sequences (such as complete human genome chromosomes) and for continuous data streams. we also experimentally demonstrate the performance of the on-line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences.", "date_create": "2007-03-31", "area": "cs.ds", "authors": ["\u0161r\u00e1mek", "brejov\u00e1", "vina\u0159"]}, {"idpaper": "0704.0062", "title": "on-line viterbi algorithm and its relationship to random walks", "abstract": "in this paper, we introduce the on-line viterbi algorithm for decoding hidden markov models (hmms) in much smaller than linear space. our analysis on two-state hmms suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state hmm can be as low as $\\theta(m\\log n)$, without a significant slow-down compared to the classical viterbi algorithm. classical viterbi algorithm requires $o(mn)$ space, which is impractical for analysis of long dna sequences (such as complete human genome chromosomes) and for continuous data streams. we also experimentally demonstrate the performance of the on-line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences.", "date_create": "2007-03-31", "area": "cs.ds", "authors": ["\u0161r\u00e1mek", "brejov\u00e1", "vina\u0159"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0098", "title": "sparsely-spread cdma - a statistical mechanics based analysis", "abstract": "sparse code division multiple access (cdma), a variation on the standard cdma method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. the analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. we present results for both cases of regular and irregular spreading matrices for the binary additive white gaussian noise channel (biawgn) with a comparison to the canonical (dense) random spreading code.", "date_create": "2007-04-01", "area": "cs.it math.it", "authors": ["raymond", "saad"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0217", "title": "capacity of a multiple-antenna fading channel with a quantized precoding   matrix", "abstract": "given a multiple-input multiple-output (mimo) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. here we analyze the performance of random vector quantization (rvq), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. we assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using b bits. we first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of b, where large system refers to the limit as b and the number of transmit and receive antennas all go to infinity with fixed ratios. with beamforming rvq is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. the performance of rvq is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. we subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic rvq performance with optimal and linear receivers (matched filter and minimum mean squared error (mmse)). numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear mmse receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", "date_create": "2007-04-02", "area": "cs.it math.it", "authors": ["santipach", "honig"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0282", "title": "on punctured pragmatic space-time codes in block fading channel", "abstract": "this paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. we show that good performance can be achieved even when puncturation is adopted and that we can still employ the same viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", "date_create": "2007-04-02", "area": "cs.it cs.cc math.it", "authors": ["bandi", "stabellini", "conti", "tralli"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0492", "title": "refuting the pseudo attack on the reesse1+ cryptosystem", "abstract": "we illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition z/m - l/ak < 1/(2 ak^2) is not sufficient for f(i) + f(j) = f(k). illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * d at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. we explain why cx = ax * w^f(x) (% m) is changed to cx = (ax * w^f(x))^d (% m) in reesse1+ v2.1. to the signature fraud, we point out that [8] misunderstands the existence of t^-1 and q^-1 % (m-1), and forging of q can be easily avoided through moving h. therefore, the conclusion of [8] that reesse1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in reesse1+) is fully incorrect, and as long as the parameter omega is fitly selected, reesse1+ with cx = ax * w^f(x) (% m) is secure.", "date_create": "2007-04-04", "area": "cs.cr", "authors": ["su", "lu"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0499", "title": "optimal routing for decode-and-forward based cooperation in wireless   networks", "abstract": "we investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. we study different coding strategies in the single-source single-destination network with many relay nodes. given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. we find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. we construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. the heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. we implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["ong", "motani"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0528", "title": "many-to-one throughput capacity of ieee 802.11 multi-hop wireless   networks", "abstract": "this paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of ieee 802.11 multi-hop networks. it has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity l. throughput capacity l is not achievable under 802.11. this paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. we show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3l/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690l (0.740l) when the source nodes are many hops away. we conjecture that 3l/4 is also the upper bound for general networks. when all links have equal length, 2l/3 can be shown to be the upper bound for general networks. our simulations show that 802.11 networks with random topologies operated with aodv routing can only achieve throughputs far below the upper bounds. fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", "date_create": "2007-04-04", "area": "cs.ni cs.it math.it", "authors": ["chan", "liew", "chan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0540", "title": "on the achievable rate regions for interference channels with degraded   message sets", "abstract": "the interference channel with degraded message sets (ic-dms) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. a coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. with resorting to this coding scheme, achievable rate regions of the ic-dms in both discrete memoryless and gaussian cases are derived, which, in general, include several previously known rate regions. numerical examples for the gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", "date_create": "2007-04-04", "area": "cs.it math.it", "authors": ["jiang", "yan"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0730", "title": "revisiting the issues on netflow sample and export performance", "abstract": "the high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. the sampling process in nearly all cases is a deterministic process of choosing 1 in every n packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. however, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. the main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. to date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. in this paper, we assess the performance of the sampling process as used in netflow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", "date_create": "2007-04-05", "area": "cs.pf cs.ni", "authors": ["haddadi", "landa", "rio", "bhatti"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0831", "title": "on packet lengths and overhead for random linear coding over the erasure   channel", "abstract": "we assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. we show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. this result is due to an erasure probability that increases with packet length. numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. our results suggest a reduction in the performance gains offered by random network coding.", "date_create": "2007-04-05", "area": "cs.it math.it", "authors": ["shrader", "ephremides"]}, {"idpaper": "0704.0834", "title": "p-adic arithmetic coding", "abstract": "a new incremental algorithm for data compression is presented. for a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. it is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as huffman's algorithm, for another special model and a specific alphabet it gives golomb-rice codes.", "date_create": "2007-04-05", "area": "cs.ds", "authors": ["rodionov", "volkov"]}, {"idpaper": "0704.0834", "title": "p-adic arithmetic coding", "abstract": "a new incremental algorithm for data compression is presented. for a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. it is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as huffman's algorithm, for another special model and a specific alphabet it gives golomb-rice codes.", "date_create": "2007-04-05", "area": "cs.ds", "authors": ["rodionov", "volkov"]}, {"idpaper": "0704.0834", "title": "p-adic arithmetic coding", "abstract": "a new incremental algorithm for data compression is presented. for a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. it is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as huffman's algorithm, for another special model and a specific alphabet it gives golomb-rice codes.", "date_create": "2007-04-05", "area": "cs.ds", "authors": ["rodionov", "volkov"]}, {"idpaper": "0704.0834", "title": "p-adic arithmetic coding", "abstract": "a new incremental algorithm for data compression is presented. for a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. it is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as huffman's algorithm, for another special model and a specific alphabet it gives golomb-rice codes.", "date_create": "2007-04-05", "area": "cs.ds", "authors": ["rodionov", "volkov"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0860", "title": "availability assessment of sunos/solaris unix systems based on syslogd   and wtmpx logfiles : a case study", "abstract": "this paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 sunos/solaris unix workstations and servers interconnected through a local area network. we focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. it is widely recognized that the information contained in such event logs might be incomplete or imperfect. the solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the sunos/solaris unix operating system. the results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["simache", "kaaniche"]}, {"idpaper": "0704.0861", "title": "empirical analysis and statistical modeling of attack processes based on   honeypots", "abstract": "honeypots are more and more used to collect data on malicious activities on the internet and to better understand the strategies and techniques used by attackers to compromise target systems. analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. this paper presents some empirical analyses based on the data collected from the leurr{\\'e}.com honeypot platforms deployed on the internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", "date_create": "2007-04-06", "area": "cs.pf cs.cr", "authors": ["kaaniche", "deswarte", "alata", "dacier", "nicomette"]}, {"idpaper": "0704.0861", "title": "empirical analysis and statistical modeling of attack processes based on   honeypots", "abstract": "honeypots are more and more used to collect data on malicious activities on the internet and to better understand the strategies and techniques used by attackers to compromise target systems. analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. this paper presents some empirical analyses based on the data collected from the leurr{\\'e}.com honeypot platforms deployed on the internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", "date_create": "2007-04-06", "area": "cs.pf cs.cr", "authors": ["kaaniche", "deswarte", "alata", "dacier", "nicomette"]}, {"idpaper": "0704.0861", "title": "empirical analysis and statistical modeling of attack processes based on   honeypots", "abstract": "honeypots are more and more used to collect data on malicious activities on the internet and to better understand the strategies and techniques used by attackers to compromise target systems. analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. this paper presents some empirical analyses based on the data collected from the leurr{\\'e}.com honeypot platforms deployed on the internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", "date_create": "2007-04-06", "area": "cs.pf cs.cr", "authors": ["kaaniche", "deswarte", "alata", "dacier", "nicomette"]}, {"idpaper": "0704.0861", "title": "empirical analysis and statistical modeling of attack processes based on   honeypots", "abstract": "honeypots are more and more used to collect data on malicious activities on the internet and to better understand the strategies and techniques used by attackers to compromise target systems. analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. this paper presents some empirical analyses based on the data collected from the leurr{\\'e}.com honeypot platforms deployed on the internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", "date_create": "2007-04-06", "area": "cs.pf cs.cr", "authors": ["kaaniche", "deswarte", "alata", "dacier", "nicomette"]}, {"idpaper": "0704.0865", "title": "an architecture-based dependability modeling framework using aadl", "abstract": "for efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. aadl (architecture analysis and design language) has proved to be efficient for software architecture modeling. in addition, aadl was designed to accommodate several types of analyses. this paper presents an iterative dependency-driven approach for dependability modeling using aadl. it is illustrated on a small example. this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from aadl models to support the analysis of software and system architectures, in critical application domains.", "date_create": "2007-04-06", "area": "cs.pf cs.se", "authors": ["rugina", "kanoun", "kaaniche"]}, {"idpaper": "0704.0865", "title": "an architecture-based dependability modeling framework using aadl", "abstract": "for efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. aadl (architecture analysis and design language) has proved to be efficient for software architecture modeling. in addition, aadl was designed to accommodate several types of analyses. this paper presents an iterative dependency-driven approach for dependability modeling using aadl. it is illustrated on a small example. this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from aadl models to support the analysis of software and system architectures, in critical application domains.", "date_create": "2007-04-06", "area": "cs.pf cs.se", "authors": ["rugina", "kanoun", "kaaniche"]}, {"idpaper": "0704.0865", "title": "an architecture-based dependability modeling framework using aadl", "abstract": "for efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. aadl (architecture analysis and design language) has proved to be efficient for software architecture modeling. in addition, aadl was designed to accommodate several types of analyses. this paper presents an iterative dependency-driven approach for dependability modeling using aadl. it is illustrated on a small example. this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from aadl models to support the analysis of software and system architectures, in critical application domains.", "date_create": "2007-04-06", "area": "cs.pf cs.se", "authors": ["rugina", "kanoun", "kaaniche"]}, {"idpaper": "0704.0865", "title": "an architecture-based dependability modeling framework using aadl", "abstract": "for efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. aadl (architecture analysis and design language) has proved to be efficient for software architecture modeling. in addition, aadl was designed to accommodate several types of analyses. this paper presents an iterative dependency-driven approach for dependability modeling using aadl. it is illustrated on a small example. this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from aadl models to support the analysis of software and system architectures, in critical application domains.", "date_create": "2007-04-06", "area": "cs.pf cs.se", "authors": ["rugina", "kanoun", "kaaniche"]}, {"idpaper": "0704.0865", "title": "an architecture-based dependability modeling framework using aadl", "abstract": "for efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. aadl (architecture analysis and design language) has proved to be efficient for software architecture modeling. in addition, aadl was designed to accommodate several types of analyses. this paper presents an iterative dependency-driven approach for dependability modeling using aadl. it is illustrated on a small example. this approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from aadl models to support the analysis of software and system architectures, in critical application domains.", "date_create": "2007-04-06", "area": "cs.pf cs.se", "authors": ["rugina", "kanoun", "kaaniche"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0879", "title": "a hierarchical approach for dependability analysis of a commercial   cache-based raid storage architecture", "abstract": "we present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based raid storage system. the archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. the impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. a simulation submodel is associated with each abstraction level. the models have been devel-oped using depend, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", "date_create": "2007-04-06", "area": "cs.pf", "authors": ["kaaniche", "romano", "kalbarczyk", "iyer", "karcich"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0954", "title": "sensor networks with random links: topology design for distributed   consensus", "abstract": "in a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. the signal-to-noise ratio (snr) is usually a main factor in determining the probability of error (or of communication failure) in a link. these probabilities are then a proxy for the snr under which the links operate. the paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. to consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. with these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. we show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", "date_create": "2007-04-06", "area": "cs.it cs.lg math.it", "authors": ["kar", "moura"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0967", "title": "cross-layer optimization of mimo-based mesh networks with gaussian   vector broadcast channels", "abstract": "mimo technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. in a mimo-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a gaussian vector broadcast channel. recently, researchers showed that ``dirty paper coding'' (dpc) is the optimal transmission strategy for gaussian vector broadcast channels. so far, there has been little study on how this fundamental result will impact the cross-layer design for mimo-based mesh networks. to fill this gap, we consider the problem of jointly optimizing dpc power allocation in the link layer at each node and multihop/multipath routing in a mimo-based mesh networks. it turns out that this optimization problem is a very challenging non-convex problem. to address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. for the transformed problem, we develop an efficient solution procedure that integrates lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. in our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using dpc.", "date_create": "2007-04-06", "area": "cs.it cs.ar math.it", "authors": ["liu", "hou"]}, {"idpaper": "0704.0985", "title": "architecture for pseudo acausal evolvable embedded systems", "abstract": "advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. this paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. it is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. an embedded system that uses theoretical framework of acausality is proposed. our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. various aspects of this architecture are discussed in detail along with the limitations.", "date_create": "2007-04-07", "area": "cs.ne cs.ai", "authors": ["abubakr", "vinay"]}, {"idpaper": "0704.0985", "title": "architecture for pseudo acausal evolvable embedded systems", "abstract": "advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. this paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. it is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. an embedded system that uses theoretical framework of acausality is proposed. our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. various aspects of this architecture are discussed in detail along with the limitations.", "date_create": "2007-04-07", "area": "cs.ne cs.ai", "authors": ["abubakr", "vinay"]}, {"idpaper": "0704.0985", "title": "architecture for pseudo acausal evolvable embedded systems", "abstract": "advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. this paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. it is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. an embedded system that uses theoretical framework of acausality is proposed. our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. various aspects of this architecture are discussed in detail along with the limitations.", "date_create": "2007-04-07", "area": "cs.ne cs.ai", "authors": ["abubakr", "vinay"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1020", "title": "the on-line shortest path problem under partial monitoring", "abstract": "the on-line shortest path problem is considered under various models of partial monitoring. given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. in a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. for this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. the algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. an extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. a version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. applications to routing in packet switched networks along with simulation results are also presented.", "date_create": "2007-04-08", "area": "cs.lg cs.sc", "authors": ["gyorgy", "linder", "lugosi", "ottucsak"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1043", "title": "on the kolmogorov-chaitin complexity for short sequences", "abstract": "a drawback of kolmogorov-chaitin complexity (k) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. moreover, when strings are short, the dependence of k on a particular universal turing machine u can be arbitrary. in practice one can approximate it by computable compression methods. however, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. in this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the kolmogorov-chaitin complexity for short sequences. additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic turing machine.", "date_create": "2007-04-08", "area": "cs.cc cs.it math.it", "authors": ["delahaye", "zenil"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1068", "title": "fast paths in large-scale dynamic road networks", "abstract": "efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to gps terminal enabled vehicles. the heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", "date_create": "2007-04-09", "area": "cs.ni cs.ds", "authors": ["nannicini", "baptiste", "barbier", "krob", "liberti"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1070", "title": "differential diversity reception of mdpsk over independent rayleigh   channels with nonidentical branch statistics and asymmetric fading spectrum", "abstract": "this paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (dpsk) with differential detection over nonselective, independent, nonidentically distributed, rayleigh fading channels. the fading process in each branch is assumed to have an arbitrary doppler spectrum with arbitrary doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. using 8-dpsk as an example, the average bit error probability (bep) of the optimum diversity receiver is obtained by calculating the bep for each of the three individual bits. the bep results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", "date_create": "2007-04-09", "area": "cs.it cs.pf math.it", "authors": ["fu", "kam"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1158", "title": "novelty and collective attention", "abstract": "the subject of collective attention is central to an information age where millions of people are inundated with daily messages. it is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. we have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. the observations can be described by a dynamical model characterized by a single novelty factor. our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", "date_create": "2007-04-09", "area": "cs.cy cs.ir physics.soc-ph", "authors": ["wu", "huberman"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1196", "title": "novel algorithm to calculate hypervolume indicator of pareto   approximation set", "abstract": "hypervolume indicator is a commonly accepted quality measure for comparing pareto approximation set generated by multi-objective optimizers. the best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $o(n^{d/2})$ with special data structures. this paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. it splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. in special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. the complexity analysis shows that the proposed algorithm achieves $o((\\frac{d}{2})^n)$ time and $o(dn^2)$ space complexity in the worst case.", "date_create": "2007-04-10", "area": "cs.cg cs.ne", "authors": ["yang", "ding"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1198", "title": "a doubly distributed genetic algorithm for network coding", "abstract": "we present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. this genotype distribution is shown to offer a significant gain in running time. then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. this temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", "date_create": "2007-04-10", "area": "cs.ne cs.ni", "authors": ["kim", "aggarwal", "o'reilly", "medard"]}, {"idpaper": "0704.1267", "title": "text line segmentation of historical documents: a survey", "abstract": "there is a huge amount of historical documents in libraries and in various national archives that have not been exploited electronically. although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are in use today. for all these tasks, a major step is document segmentation into text lines. because of the low quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),automatic text line segmentation remains an open research field. the objective of this paper is to present a survey of existing methods, developed during the last decade, and dedicated to documents of historical interest.", "date_create": "2007-04-10", "area": "cs.cv", "authors": ["likforman-sulem", "zahour", "taconet"]}, {"idpaper": "0704.1267", "title": "text line segmentation of historical documents: a survey", "abstract": "there is a huge amount of historical documents in libraries and in various national archives that have not been exploited electronically. although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are in use today. for all these tasks, a major step is document segmentation into text lines. because of the low quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),automatic text line segmentation remains an open research field. the objective of this paper is to present a survey of existing methods, developed during the last decade, and dedicated to documents of historical interest.", "date_create": "2007-04-10", "area": "cs.cv", "authors": ["likforman-sulem", "zahour", "taconet"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1269", "title": "phase transitions in the coloring of random graphs", "abstract": "we consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions).   we show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. first, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. eventually, above the coloring threshold, no more solutions are available. we compute all the critical connectivities for erdos-renyi and regular random graphs and determine their asymptotic values for large number of colors.   finally, we discuss the algorithmic consequences of our findings. we argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. we also discuss the performance of a simple local walk-col algorithm and of the belief propagation algorithm in the light of our results.", "date_create": "2007-04-10", "area": "cond-mat.dis-nn cond-mat.stat-mech cs.cc", "authors": ["zdeborov\u00e1", "krzakala"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1274", "title": "parametric learning and monte carlo optimization", "abstract": "this paper uncovers and explores the close relationship between monte carlo optimization of a parametrized integral (mco), parametric machine-learning (pl), and `blackbox' or `oracle'-based optimization (bo). we make four contributions. first, we prove that mco is mathematically identical to a broad class of pl problems. this identity potentially provides a new application domain for all broadly applicable pl techniques: mco. second, we introduce immediate sampling, a new version of the probability collectives (pc) algorithm for blackbox optimization. immediate sampling transforms the original bo problem into an mco problem. accordingly, by combining these first two contributions, we can apply all pl techniques to bo. in our third contribution we validate this way of improving bo by demonstrating that cross-validation and bagging improve immediate sampling. finally, conventional mc and mco procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. we demonstrate that one can exploit the sample location information using pl techniques, for example by forming a fit of the sample locations to the associated values of the integrand. this provides an additional way to apply pl techniques to improve mco.", "date_create": "2007-04-10", "area": "cs.lg", "authors": ["wolpert", "rajnarayan"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1294", "title": "a disciplined approach to adopting agile practices: the agile adoption   framework", "abstract": "many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. to date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. to address this problem we present the agile adoption framework. the framework consists of two components: an agile measurement index, and a 4-stage process, that together guide and assist the agile adoption efforts of organizations. more specifically, the agile measurement index is used to identify the agile potential of projects and organizations. the 4-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", "date_create": "2007-04-10", "area": "cs.se", "authors": ["sidky", "arthur", "bohner"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1317", "title": "low density lattice codes", "abstract": "low density lattice codes (ldlc) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white gaussian noise (awgn) channel. in ldlc a codeword x is generated directly at the n-dimensional euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = gb, where h, the inverse of g, is restricted to be sparse. the fact that h is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5db from capacity at block length of n = 100,000 symbols. the paper also discusses convergence results and implementation considerations.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["sommer", "feder", "shalvi"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1358", "title": "distance preserving mappings from ternary vectors to permutations", "abstract": "distance-preserving mappings (dpms) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger hamming distance than that of the vectors. in this paper, we propose a construction of dpms from ternary vectors. the constructed dpms improve the lower bounds on the maximal size of permutation arrays.", "date_create": "2007-04-11", "area": "cs.dm cs.it math.it", "authors": ["lin", "chang", "chen", "kl\u00f8ve"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1373", "title": "a language-based approach for improving the robustness of network   application protocol implementations", "abstract": "the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.", "date_create": "2007-04-11", "area": "cs.pl", "authors": ["laurent", "r\u00e9veill\u00e8re", "lawall", "muller"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1394", "title": "calculating valid domains for bdd-based interactive configuration", "abstract": "in these notes we formally describe the functionality of calculating valid domains from the bdd representing the solution space of valid configurations. the formalization is largely based on the clab configuration framework.", "date_create": "2007-04-11", "area": "cs.ai", "authors": ["hadzic", "jensen", "andersen"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1455", "title": "a better good-turing estimator for sequence probabilities", "abstract": "we consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. the key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. in this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. to overcome this problem, the traditional approach to probability estimation is to use the classical good-turing estimator. we introduce a natural scaling model and use it to show that the good-turing sequence probability estimator is not consistent. we then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", "date_create": "2007-04-11", "area": "cs.it math.it", "authors": ["wagner", "viswanath", "kulkarni"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1524", "title": "glrt-optimal noncoherent lattice decoding", "abstract": "this paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of qam and pam signals over complex-valued fading channels. the algorithms are optimal in terms of the generalized likelihood ratio test (glrt). the computational complexity is polynomial in the block length; making glrt-optimal noncoherent detection feasible for implementation. we also provide even lower complexity suboptimal algorithms. simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (pat). the new technique is shown to outperform pat.", "date_create": "2007-04-12", "area": "cs.it math.it", "authors": ["ryan", "collings", "clarkson"]}, {"idpaper": "0704.1571", "title": "on restrictions of balanced 2-interval graphs", "abstract": "the class of 2-interval graphs has been introduced for modelling scheduling and allocation problems, and more recently for specific bioinformatic problems. some of those applications imply restrictions on the 2-interval graphs, and justify the introduction of a hierarchy of subclasses of 2-interval graphs that generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and (x,x)-interval graphs. we provide instances that show that all the inclusions are strict. we extend the np-completeness proof of recognizing 2-interval graphs to the recognition of balanced 2-interval graphs. finally we give hints on the complexity of unit 2-interval graphs recognition, by studying relationships with other graph classes: proper circular-arc, quasi-line graphs, k_{1,5}-free graphs, ...", "date_create": "2007-04-12", "area": "cs.dm q-bio.qm", "authors": ["gambette", "vialette"]}, {"idpaper": "0704.1571", "title": "on restrictions of balanced 2-interval graphs", "abstract": "the class of 2-interval graphs has been introduced for modelling scheduling and allocation problems, and more recently for specific bioinformatic problems. some of those applications imply restrictions on the 2-interval graphs, and justify the introduction of a hierarchy of subclasses of 2-interval graphs that generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and (x,x)-interval graphs. we provide instances that show that all the inclusions are strict. we extend the np-completeness proof of recognizing 2-interval graphs to the recognition of balanced 2-interval graphs. finally we give hints on the complexity of unit 2-interval graphs recognition, by studying relationships with other graph classes: proper circular-arc, quasi-line graphs, k_{1,5}-free graphs, ...", "date_create": "2007-04-12", "area": "cs.dm q-bio.qm", "authors": ["gambette", "vialette"]}, {"idpaper": "0704.1571", "title": "on restrictions of balanced 2-interval graphs", "abstract": "the class of 2-interval graphs has been introduced for modelling scheduling and allocation problems, and more recently for specific bioinformatic problems. some of those applications imply restrictions on the 2-interval graphs, and justify the introduction of a hierarchy of subclasses of 2-interval graphs that generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and (x,x)-interval graphs. we provide instances that show that all the inclusions are strict. we extend the np-completeness proof of recognizing 2-interval graphs to the recognition of balanced 2-interval graphs. finally we give hints on the complexity of unit 2-interval graphs recognition, by studying relationships with other graph classes: proper circular-arc, quasi-line graphs, k_{1,5}-free graphs, ...", "date_create": "2007-04-12", "area": "cs.dm q-bio.qm", "authors": ["gambette", "vialette"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1676", "title": "personalizing image search results on flickr", "abstract": "the social media site flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. flickr offers multiple ways of browsing or searching it. one option is tag search, which returns all images tagged with a specific keyword. if the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. we claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. we show how to exploit this metadata to personalize search results for the user, thereby improving search performance. first, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. the users' interests can similarly be described by the tags they used for annotating their images. the latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", "date_create": "2007-04-12", "area": "cs.ir cs.ai cs.cy cs.dl cs.hc", "authors": ["lerman", "plangprasopchok", "wong"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1678", "title": "settling the complexity of computing two-player nash equilibria", "abstract": "we settle a long-standing open question in algorithmic game theory. we prove that bimatrix, the problem of finding a nash equilibrium in a two-player game, is complete for the complexity class ppad polynomial parity argument, directed version) introduced by papadimitriou in 1991.   this is the first of a series of results concerning the complexity of nash equilibria. in particular, we prove the following theorems:   bimatrix does not have a fully polynomial-time approximation scheme unless every problem in ppad is solvable in polynomial time. the smoothed complexity of the classic lemke-howson algorithm and, in fact, of any algorithm for bimatrix is not polynomial unless every problem in ppad is solvable in randomized polynomial time. our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. our results also have two broad complexity implications in mathematical economics and operations research: arrow-debreu market equilibria are ppad-hard to compute. the p-matrix linear complementary problem is computationally harder than convex programming unless every problem in ppad is solvable in polynomial time.", "date_create": "2007-04-12", "area": "cs.gt cs.cc", "authors": ["chen", "deng", "teng"]}, {"idpaper": "0704.1707", "title": "a cut-free sequent calculus for bi-intuitionistic logic: extended   version", "abstract": "bi-intuitionistic logic is the extension of intuitionistic logic with a connective dual to implication. bi-intuitionistic logic was introduced by rauszer as a hilbert calculus with algebraic and kripke semantics. but her subsequent ``cut-free'' sequent calculus for biint has recently been shown by uustalu to fail cut-elimination. we present a new cut-free sequent calculus for biint, and prove it sound and complete with respect to its kripke semantics. ensuring completeness is complicated by the interaction between implication and its dual, similarly to future and past modalities in tense logic. our calculus handles this interaction using extended sequents which pass information from premises to conclusions using variables instantiated at the leaves of failed derivation trees. our simple termination argument allows our calculus to be used for automated deduction, although this is not its main purpose.", "date_create": "2007-04-13", "area": "cs.lo", "authors": ["buisman", "gor\u00e9"]}, {"idpaper": "0704.1709", "title": "traitement des donnees manquantes au moyen de l'algorithme de kohonen", "abstract": "nous montrons comment il est possible d'utiliser l'algorithme d'auto organisation de kohonen pour traiter des donn\\'ees avec valeurs manquantes et estimer ces derni\\`eres. apr\\`es un rappel m\\'ethodologique, nous illustrons notre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.   -----   we show how it is possible to use the kohonen self-organizing algorithm to deal with data which contain missing values and to estimate them. after a methodological recall, we illustrate our purpose from three real databases applications.", "date_create": "2007-04-13", "area": "stat.ap cs.ne", "authors": ["cottrell", "ibbou", "letr\u00e9my"]}, {"idpaper": "0704.1709", "title": "traitement des donnees manquantes au moyen de l'algorithme de kohonen", "abstract": "nous montrons comment il est possible d'utiliser l'algorithme d'auto organisation de kohonen pour traiter des donn\\'ees avec valeurs manquantes et estimer ces derni\\`eres. apr\\`es un rappel m\\'ethodologique, nous illustrons notre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.   -----   we show how it is possible to use the kohonen self-organizing algorithm to deal with data which contain missing values and to estimate them. after a methodological recall, we illustrate our purpose from three real databases applications.", "date_create": "2007-04-13", "area": "stat.ap cs.ne", "authors": ["cottrell", "ibbou", "letr\u00e9my"]}, {"idpaper": "0704.1768", "title": "assessment and propagation of input uncertainty in tree-based option   pricing models", "abstract": "this paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. we provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. we finally compare our methods with classical calibration-based results assuming that there is no options market established. these methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", "date_create": "2007-04-13", "area": "cs.ce cs.gt", "authors": ["gzyl", "molina", "ter horst"]}, {"idpaper": "0704.1768", "title": "assessment and propagation of input uncertainty in tree-based option   pricing models", "abstract": "this paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. we provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. we finally compare our methods with classical calibration-based results assuming that there is no options market established. these methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", "date_create": "2007-04-13", "area": "cs.ce cs.gt", "authors": ["gzyl", "molina", "ter horst"]}, {"idpaper": "0704.1768", "title": "assessment and propagation of input uncertainty in tree-based option   pricing models", "abstract": "this paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. we provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. we finally compare our methods with classical calibration-based results assuming that there is no options market established. these methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", "date_create": "2007-04-13", "area": "cs.ce cs.gt", "authors": ["gzyl", "molina", "ter horst"]}, {"idpaper": "0704.1768", "title": "assessment and propagation of input uncertainty in tree-based option   pricing models", "abstract": "this paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. we provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. we finally compare our methods with classical calibration-based results assuming that there is no options market established. these methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", "date_create": "2007-04-13", "area": "cs.ce cs.gt", "authors": ["gzyl", "molina", "ter horst"]}, {"idpaper": "0704.1768", "title": "assessment and propagation of input uncertainty in tree-based option   pricing models", "abstract": "this paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. we provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. we finally compare our methods with classical calibration-based results assuming that there is no options market established. these methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", "date_create": "2007-04-13", "area": "cs.ce cs.gt", "authors": ["gzyl", "molina", "ter horst"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1783", "title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service (qos) requirements. to attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different qos metric value (e.g. bandwidth, cost, delay, packet loss). the second step consists in writing this graph as a program in soft constraint logic programming (sclp): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to qos routing problems. moreover, c-semiring structures are a convenient tool to model qos metrics. at last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", "date_create": "2007-04-13", "area": "cs.lo cs.ai cs.ni", "authors": ["bistarelli", "montanari", "rossi", "santini"]}, {"idpaper": "0704.1818", "title": "low-density graph codes that are optimal for source/channel coding and   binning", "abstract": "we describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (ldgm) code with a low-density parity check (ldpc) code. our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. more precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(r, d)$ on the rate-distortion curve of the binary symmetric source. in the context of channel coding, we prove that finite degree codes can achieve any pair $(c, p)$ on the capacity-noise curve of the binary symmetric channel. next, we show that our compound construction has a nested structure that can be exploited to achieve the wyner-ziv bound for source coding with side information (scsi), as well as the gelfand-pinsker bound for channel coding with side information (ccsi). although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", "date_create": "2007-04-13", "area": "cs.it math.it", "authors": ["wainwright", "martinian"]}, {"idpaper": "0704.1818", "title": "low-density graph codes that are optimal for source/channel coding and   binning", "abstract": "we describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (ldgm) code with a low-density parity check (ldpc) code. our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. more precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(r, d)$ on the rate-distortion curve of the binary symmetric source. in the context of channel coding, we prove that finite degree codes can achieve any pair $(c, p)$ on the capacity-noise curve of the binary symmetric channel. next, we show that our compound construction has a nested structure that can be exploited to achieve the wyner-ziv bound for source coding with side information (scsi), as well as the gelfand-pinsker bound for channel coding with side information (ccsi). although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", "date_create": "2007-04-13", "area": "cs.it math.it", "authors": ["wainwright", "martinian"]}, {"idpaper": "0704.1818", "title": "low-density graph codes that are optimal for source/channel coding and   binning", "abstract": "we describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (ldgm) code with a low-density parity check (ldpc) code. our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. more precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(r, d)$ on the rate-distortion curve of the binary symmetric source. in the context of channel coding, we prove that finite degree codes can achieve any pair $(c, p)$ on the capacity-noise curve of the binary symmetric channel. next, we show that our compound construction has a nested structure that can be exploited to achieve the wyner-ziv bound for source coding with side information (scsi), as well as the gelfand-pinsker bound for channel coding with side information (ccsi). although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", "date_create": "2007-04-13", "area": "cs.it math.it", "authors": ["wainwright", "martinian"]}, {"idpaper": "0704.1818", "title": "low-density graph codes that are optimal for source/channel coding and   binning", "abstract": "we describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (ldgm) code with a low-density parity check (ldpc) code. our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. more precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(r, d)$ on the rate-distortion curve of the binary symmetric source. in the context of channel coding, we prove that finite degree codes can achieve any pair $(c, p)$ on the capacity-noise curve of the binary symmetric channel. next, we show that our compound construction has a nested structure that can be exploited to achieve the wyner-ziv bound for source coding with side information (scsi), as well as the gelfand-pinsker bound for channel coding with side information (ccsi). although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", "date_create": "2007-04-13", "area": "cs.it math.it", "authors": ["wainwright", "martinian"]}, {"idpaper": "0704.1818", "title": "low-density graph codes that are optimal for source/channel coding and   binning", "abstract": "we describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (ldgm) code with a low-density parity check (ldpc) code. our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. more precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(r, d)$ on the rate-distortion curve of the binary symmetric source. in the context of channel coding, we prove that finite degree codes can achieve any pair $(c, p)$ on the capacity-noise curve of the binary symmetric channel. next, we show that our compound construction has a nested structure that can be exploited to achieve the wyner-ziv bound for source coding with side information (scsi), as well as the gelfand-pinsker bound for channel coding with side information (ccsi). although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", "date_create": "2007-04-13", "area": "cs.it math.it", "authors": ["wainwright", "martinian"]}, {"idpaper": "0704.1829", "title": "on-line chain partitions of up-growing semi-orders", "abstract": "on-line chain partition is a two-player game between spoiler and algorithm. spoiler presents a partially ordered set, point by point. algorithm assigns incoming points (immediately and irrevocably) to the chains which constitute a chain partition of the order. the value of the game for orders of width $w$ is a minimum number $\\fval(w)$ such that algorithm has a strategy using at most $\\fval(w)$ chains on orders of width at most $w$. we analyze the chain partition game for up-growing semi-orders. surprisingly, the golden ratio comes into play and the value of the game is $\\lfloor\\frac{1+\\sqrt{5}}{2}\\; w \\rfloor$.", "date_create": "2007-04-13", "area": "cs.dm", "authors": ["felsner", "kloch", "matecki", "micek"]}, {"idpaper": "0704.1833", "title": "analysis of the 802.11e enhanced distributed channel access function", "abstract": "the ieee 802.11e standard revises the medium access control (mac) layer of the former ieee 802.11 standard for quality-of-service (qos) provision in the wireless local area networks (wlans). the enhanced distributed channel access (edca) function of 802.11e defines multiple access categories (ac) with ac-specific contention window (cw) sizes, arbitration interframe space (aifs) values, and transmit opportunity (txop) limits to support mac-level qos and prioritization. we propose an analytical model for the edca function which incorporates an accurate cw, aifs, and txop differentiation at any traffic load. the proposed model is also shown to capture the effect of mac layer buffer size on the performance. analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, edca parameters, and mac layer buffer space.", "date_create": "2007-04-13", "area": "cs.ni", "authors": ["inan", "keceli", "ayanoglu"]}, {"idpaper": "0704.1833", "title": "analysis of the 802.11e enhanced distributed channel access function", "abstract": "the ieee 802.11e standard revises the medium access control (mac) layer of the former ieee 802.11 standard for quality-of-service (qos) provision in the wireless local area networks (wlans). the enhanced distributed channel access (edca) function of 802.11e defines multiple access categories (ac) with ac-specific contention window (cw) sizes, arbitration interframe space (aifs) values, and transmit opportunity (txop) limits to support mac-level qos and prioritization. we propose an analytical model for the edca function which incorporates an accurate cw, aifs, and txop differentiation at any traffic load. the proposed model is also shown to capture the effect of mac layer buffer size on the performance. analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, edca parameters, and mac layer buffer space.", "date_create": "2007-04-13", "area": "cs.ni", "authors": ["inan", "keceli", "ayanoglu"]}, {"idpaper": "0704.1833", "title": "analysis of the 802.11e enhanced distributed channel access function", "abstract": "the ieee 802.11e standard revises the medium access control (mac) layer of the former ieee 802.11 standard for quality-of-service (qos) provision in the wireless local area networks (wlans). the enhanced distributed channel access (edca) function of 802.11e defines multiple access categories (ac) with ac-specific contention window (cw) sizes, arbitration interframe space (aifs) values, and transmit opportunity (txop) limits to support mac-level qos and prioritization. we propose an analytical model for the edca function which incorporates an accurate cw, aifs, and txop differentiation at any traffic load. the proposed model is also shown to capture the effect of mac layer buffer size on the performance. analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, edca parameters, and mac layer buffer space.", "date_create": "2007-04-13", "area": "cs.ni", "authors": ["inan", "keceli", "ayanoglu"]}, {"idpaper": "0704.1833", "title": "analysis of the 802.11e enhanced distributed channel access function", "abstract": "the ieee 802.11e standard revises the medium access control (mac) layer of the former ieee 802.11 standard for quality-of-service (qos) provision in the wireless local area networks (wlans). the enhanced distributed channel access (edca) function of 802.11e defines multiple access categories (ac) with ac-specific contention window (cw) sizes, arbitration interframe space (aifs) values, and transmit opportunity (txop) limits to support mac-level qos and prioritization. we propose an analytical model for the edca function which incorporates an accurate cw, aifs, and txop differentiation at any traffic load. the proposed model is also shown to capture the effect of mac layer buffer size on the performance. analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, edca parameters, and mac layer buffer space.", "date_create": "2007-04-13", "area": "cs.ni", "authors": ["inan", "keceli", "ayanoglu"]}, {"idpaper": "0704.1833", "title": "analysis of the 802.11e enhanced distributed channel access function", "abstract": "the ieee 802.11e standard revises the medium access control (mac) layer of the former ieee 802.11 standard for quality-of-service (qos) provision in the wireless local area networks (wlans). the enhanced distributed channel access (edca) function of 802.11e defines multiple access categories (ac) with ac-specific contention window (cw) sizes, arbitration interframe space (aifs) values, and transmit opportunity (txop) limits to support mac-level qos and prioritization. we propose an analytical model for the edca function which incorporates an accurate cw, aifs, and txop differentiation at any traffic load. the proposed model is also shown to capture the effect of mac layer buffer size on the performance. analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, edca parameters, and mac layer buffer space.", "date_create": "2007-04-13", "area": "cs.ni", "authors": ["inan", "keceli", "ayanoglu"]}, {"idpaper": "0704.1873", "title": "an achievable rate region for interference channels with conferencing", "abstract": "in this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. we employ superposition block markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. we show that, under respective conditions, the proposed achievable region reduces to han and kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the gaussian vector broadcast channel. numerical examples for the gaussian case are given.", "date_create": "2007-04-14", "area": "cs.it math.it", "authors": ["cao", "chen"]}, {"idpaper": "0704.1873", "title": "an achievable rate region for interference channels with conferencing", "abstract": "in this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. we employ superposition block markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. we show that, under respective conditions, the proposed achievable region reduces to han and kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the gaussian vector broadcast channel. numerical examples for the gaussian case are given.", "date_create": "2007-04-14", "area": "cs.it math.it", "authors": ["cao", "chen"]}, {"idpaper": "0704.1873", "title": "an achievable rate region for interference channels with conferencing", "abstract": "in this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. we employ superposition block markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. we show that, under respective conditions, the proposed achievable region reduces to han and kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the gaussian vector broadcast channel. numerical examples for the gaussian case are given.", "date_create": "2007-04-14", "area": "cs.it math.it", "authors": ["cao", "chen"]}, {"idpaper": "0704.1873", "title": "an achievable rate region for interference channels with conferencing", "abstract": "in this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. we employ superposition block markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. we show that, under respective conditions, the proposed achievable region reduces to han and kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the gaussian vector broadcast channel. numerical examples for the gaussian case are given.", "date_create": "2007-04-14", "area": "cs.it math.it", "authors": ["cao", "chen"]}, {"idpaper": "0704.1873", "title": "an achievable rate region for interference channels with conferencing", "abstract": "in this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. we employ superposition block markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. we show that, under respective conditions, the proposed achievable region reduces to han and kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the gaussian vector broadcast channel. numerical examples for the gaussian case are given.", "date_create": "2007-04-14", "area": "cs.it math.it", "authors": ["cao", "chen"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.1925", "title": "blind identification of distributed antenna systems with multiple   carrier frequency offsets", "abstract": "in spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (cfos) arising from mismatch between the oscillators of transmitters and receivers. this results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. in this paper, a new approach for blind cfo estimation and symbol recovery is proposed. the received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual multiple-input multiple-output (mimo) problem. by applying blind mimo system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct cfo. by applying a decision feedback phase lock loop (pll), the cfo can be mitigated and the transmitted symbols can be recovered. the estimated mimo system response provides information about the cfos that can be used to initialize the pll, speed up its convergence, and avoid ambiguities usually linked with pll.", "date_create": "2007-04-15", "area": "cs.it math.it", "authors": ["yu", "petropulu", "poor"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2010", "title": "a study of structural properties on profiles hmms", "abstract": "motivation: profile hidden markov models (phmms) are a popular and very useful tool in the detection of the remote homologue protein families. unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. we present hmmer-struct, a model construction algorithm and tool that tries to improve phmm performance by using structural information while training phmms. as a first step, hmmer-struct constructs a set of phmms. each phmm is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. properties used were primary, secondary and tertiary structures, accessibility and packing. hmmer-struct then prioritizes the results by voting. results: we used the scop database to perform our experiments. throughout, we apply leave-one-family-out cross-validation over protein superfamilies. first, we used the mammoth-mult structural aligner to align the training set proteins. then, we performed two sets of experiments. in a first experiment, we compared structure weighted models against standard phmms and against each other. in a second experiment, we compared the voting model against individual phmms. we compare method performance through roc curves and through precision/recall curves, and assess significance through the paired two tailed t-test. our results show significant performance improvements of all structurally weighted models over default hmmer, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", "date_create": "2007-04-16", "area": "cs.ai", "authors": ["bernardes", "davila", "costa", "zaverucha"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2014", "title": "extensive games with possibly unaware players", "abstract": "standard game theory assumes that the structure of the game is common knowledge among players. we relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. in particular, they may not be aware of moves that they and other agents can make. we show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. we provide a generalization of nash equilibrium and show that every game with awareness has a generalized nash equilibrium. finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", "date_create": "2007-04-16", "area": "cs.gt cs.ma", "authors": ["halpern", "r\u00eago"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2258", "title": "on the hardness of approximating stopping and trapping sets in ldpc   codes", "abstract": "we prove that approximating the size of stopping and trapping sets in tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (ldpc) codes, is np-hard. the ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length ldpc codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", "date_create": "2007-04-17", "area": "cs.it math.it", "authors": ["mcgregor", "milenkovic"]}, {"idpaper": "0704.2282", "title": "kekul\\'e cells for molecular computation", "abstract": "the configurations of single and double bonds in polycyclic hydrocarbons are abstracted as kekul\\'e states of graphs. sending a so-called soliton over an open channel between ports (external nodes) of the graph changes the kekul\\'e state and therewith the set of open channels in the graph. this switching behaviour is proposed as a basis for molecular computation. the proposal is highly speculative but may have tremendous impact.   kekul\\'e states with the same boundary behaviour (port assignment) can be regarded as equivalent. this gives rise to the abstraction of kekul\\'e cells. the basic theory of kekul\\'e states and kekul\\'e cells is developed here, up to the classification of kekul\\'e cells with $\\leq 4$ ports. to put the theory in context, we generalize kekul\\'e states to semi-kekul\\'e states, which form the solutions of a linear system of equations over the field of the bits 0 and 1. we briefly study so-called omniconjugated graphs, in which every port assignment of the right signature has a kekul\\'e state. omniconjugated graphs may be useful as connectors between computational elements. we finally investigate some examples with potentially useful switching behaviour.", "date_create": "2007-04-18", "area": "cs.oh cs.dm", "authors": ["hesselink", "hummelen", "jonkman", "reker", "de lavalette", "van der veen"]}, {"idpaper": "0704.2282", "title": "kekul\\'e cells for molecular computation", "abstract": "the configurations of single and double bonds in polycyclic hydrocarbons are abstracted as kekul\\'e states of graphs. sending a so-called soliton over an open channel between ports (external nodes) of the graph changes the kekul\\'e state and therewith the set of open channels in the graph. this switching behaviour is proposed as a basis for molecular computation. the proposal is highly speculative but may have tremendous impact.   kekul\\'e states with the same boundary behaviour (port assignment) can be regarded as equivalent. this gives rise to the abstraction of kekul\\'e cells. the basic theory of kekul\\'e states and kekul\\'e cells is developed here, up to the classification of kekul\\'e cells with $\\leq 4$ ports. to put the theory in context, we generalize kekul\\'e states to semi-kekul\\'e states, which form the solutions of a linear system of equations over the field of the bits 0 and 1. we briefly study so-called omniconjugated graphs, in which every port assignment of the right signature has a kekul\\'e state. omniconjugated graphs may be useful as connectors between computational elements. we finally investigate some examples with potentially useful switching behaviour.", "date_create": "2007-04-18", "area": "cs.oh cs.dm", "authors": ["hesselink", "hummelen", "jonkman", "reker", "de lavalette", "van der veen"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2295", "title": "using image attributes for human identification protocols", "abstract": "a secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. recently, the authors proposed a human identification protocol in the rsa conference 2007, which is loosely based on the ability of humans to efficiently process an image. the advantage being that an automated adversary is not effective in attacking the protocol without human assistance. this paper extends that work by trying to solve some of the open problems. first, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. secondly, we propose a new construction based on textual captchas (reverse turing tests) in order to make the generation of automated challenges easier. we also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", "date_create": "2007-04-18", "area": "cs.cr", "authors": ["jameel", "lee", "lee"]}, {"idpaper": "0704.2344", "title": "parallel computing for the finite element method", "abstract": "a finite element method is presented to compute time harmonic microwave fields in three dimensional configurations. nodal-based finite elements have been coupled with an absorbing boundary condition to solve open boundary problems. this paper describes how the modeling of large devices has been made possible using parallel computation, new algorithms are then proposed to implement this formulation on a cluster of workstations (10 dec alpha 300x) and on a cray c98. analysis of the computation efficiency is performed using simple problems. the electromagnetic scattering of a plane wave by a perfect electric conducting airplane is finally given as example.", "date_create": "2007-04-18", "area": "cs.dc", "authors": ["vollaire", "nicolas", "nicolas"]}, {"idpaper": "0704.2344", "title": "parallel computing for the finite element method", "abstract": "a finite element method is presented to compute time harmonic microwave fields in three dimensional configurations. nodal-based finite elements have been coupled with an absorbing boundary condition to solve open boundary problems. this paper describes how the modeling of large devices has been made possible using parallel computation, new algorithms are then proposed to implement this formulation on a cluster of workstations (10 dec alpha 300x) and on a cray c98. analysis of the computation efficiency is performed using simple problems. the electromagnetic scattering of a plane wave by a perfect electric conducting airplane is finally given as example.", "date_create": "2007-04-18", "area": "cs.dc", "authors": ["vollaire", "nicolas", "nicolas"]}, {"idpaper": "0704.2344", "title": "parallel computing for the finite element method", "abstract": "a finite element method is presented to compute time harmonic microwave fields in three dimensional configurations. nodal-based finite elements have been coupled with an absorbing boundary condition to solve open boundary problems. this paper describes how the modeling of large devices has been made possible using parallel computation, new algorithms are then proposed to implement this formulation on a cluster of workstations (10 dec alpha 300x) and on a cray c98. analysis of the computation efficiency is performed using simple problems. the electromagnetic scattering of a plane wave by a perfect electric conducting airplane is finally given as example.", "date_create": "2007-04-18", "area": "cs.dc", "authors": ["vollaire", "nicolas", "nicolas"]}, {"idpaper": "0704.2351", "title": "parallel computation of the rank of large sparse matrices from algebraic   k-theory", "abstract": "this paper deals with the computation of the rank and of some integer smith forms of a series of sparse matrices arising in algebraic k-theory. the number of non zero entries in the considered matrices ranges from 8 to 37 millions. the largest rank computation took more than 35 days on 50 processors. we report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. in particular, these results are part of the first computation of the cohomology of the linear group gl_7(z).", "date_create": "2007-04-18", "area": "math.kt cs.dc cs.sc math.nt", "authors": ["dumas", "elbaz-vincent", "giorgi", "urbanska"]}, {"idpaper": "0704.2351", "title": "parallel computation of the rank of large sparse matrices from algebraic   k-theory", "abstract": "this paper deals with the computation of the rank and of some integer smith forms of a series of sparse matrices arising in algebraic k-theory. the number of non zero entries in the considered matrices ranges from 8 to 37 millions. the largest rank computation took more than 35 days on 50 processors. we report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. in particular, these results are part of the first computation of the cohomology of the linear group gl_7(z).", "date_create": "2007-04-18", "area": "math.kt cs.dc cs.sc math.nt", "authors": ["dumas", "elbaz-vincent", "giorgi", "urbanska"]}, {"idpaper": "0704.2351", "title": "parallel computation of the rank of large sparse matrices from algebraic   k-theory", "abstract": "this paper deals with the computation of the rank and of some integer smith forms of a series of sparse matrices arising in algebraic k-theory. the number of non zero entries in the considered matrices ranges from 8 to 37 millions. the largest rank computation took more than 35 days on 50 processors. we report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. in particular, these results are part of the first computation of the cohomology of the linear group gl_7(z).", "date_create": "2007-04-18", "area": "math.kt cs.dc cs.sc math.nt", "authors": ["dumas", "elbaz-vincent", "giorgi", "urbanska"]}, {"idpaper": "0704.2351", "title": "parallel computation of the rank of large sparse matrices from algebraic   k-theory", "abstract": "this paper deals with the computation of the rank and of some integer smith forms of a series of sparse matrices arising in algebraic k-theory. the number of non zero entries in the considered matrices ranges from 8 to 37 millions. the largest rank computation took more than 35 days on 50 processors. we report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. in particular, these results are part of the first computation of the cohomology of the linear group gl_7(z).", "date_create": "2007-04-18", "area": "math.kt cs.dc cs.sc math.nt", "authors": ["dumas", "elbaz-vincent", "giorgi", "urbanska"]}, {"idpaper": "0704.2386", "title": "bounded pushdown dimension vs lempel ziv information density", "abstract": "in this paper we introduce a variant of pushdown dimension called bounded pushdown (bpd) dimension, that measures the density of information contained in a sequence, relative to a bpd automata, i.e. a finite state machine equipped with an extra infinite memory stack, with the additional requirement that every input symbol only allows a bounded number of stack movements. bpd automata are a natural real-time restriction of pushdown automata. we show that bpd dimension is a robust notion by giving an equivalent characterization of bpd dimension in terms of bpd compressors. we then study the relationships between bpd compression, and the standard lempel-ziv (lz) compression algorithm, and show that in contrast to the finite-state compressor case, lz is not universal for bounded pushdown compressors in a strong sense: we construct a sequence that lz fails to compress signicantly, but that is compressed by at least a factor 2 by a bpd compressor. as a corollary we obtain a strong separation between finite-state and bpd dimension.", "date_create": "2007-04-18", "area": "cs.cc cs.it math.it", "authors": ["albert", "mayordomo", "moser"]}, {"idpaper": "0704.2386", "title": "bounded pushdown dimension vs lempel ziv information density", "abstract": "in this paper we introduce a variant of pushdown dimension called bounded pushdown (bpd) dimension, that measures the density of information contained in a sequence, relative to a bpd automata, i.e. a finite state machine equipped with an extra infinite memory stack, with the additional requirement that every input symbol only allows a bounded number of stack movements. bpd automata are a natural real-time restriction of pushdown automata. we show that bpd dimension is a robust notion by giving an equivalent characterization of bpd dimension in terms of bpd compressors. we then study the relationships between bpd compression, and the standard lempel-ziv (lz) compression algorithm, and show that in contrast to the finite-state compressor case, lz is not universal for bounded pushdown compressors in a strong sense: we construct a sequence that lz fails to compress signicantly, but that is compressed by at least a factor 2 by a bpd compressor. as a corollary we obtain a strong separation between finite-state and bpd dimension.", "date_create": "2007-04-18", "area": "cs.cc cs.it math.it", "authors": ["albert", "mayordomo", "moser"]}, {"idpaper": "0704.2386", "title": "bounded pushdown dimension vs lempel ziv information density", "abstract": "in this paper we introduce a variant of pushdown dimension called bounded pushdown (bpd) dimension, that measures the density of information contained in a sequence, relative to a bpd automata, i.e. a finite state machine equipped with an extra infinite memory stack, with the additional requirement that every input symbol only allows a bounded number of stack movements. bpd automata are a natural real-time restriction of pushdown automata. we show that bpd dimension is a robust notion by giving an equivalent characterization of bpd dimension in terms of bpd compressors. we then study the relationships between bpd compression, and the standard lempel-ziv (lz) compression algorithm, and show that in contrast to the finite-state compressor case, lz is not universal for bounded pushdown compressors in a strong sense: we construct a sequence that lz fails to compress signicantly, but that is compressed by at least a factor 2 by a bpd compressor. as a corollary we obtain a strong separation between finite-state and bpd dimension.", "date_create": "2007-04-18", "area": "cs.cc cs.it math.it", "authors": ["albert", "mayordomo", "moser"]}, {"idpaper": "0704.2448", "title": "light logics and optimal reduction: completeness and complexity", "abstract": "typing of lambda-terms in elementary and light affine logic (eal, lal, resp.) has been studied for two different reasons: on the one hand the evaluation of typed terms using lal (eal, resp.) proof-nets admits a guaranteed polynomial (elementary, resp.) bound; on the other hand these terms can also be evaluated by optimal reduction using the abstract version of lamping's algorithm. the first reduction is global while the second one is local and asynchronous. we prove that for lal (eal, resp.) typed terms, lamping's abstract algorithm also admits a polynomial (elementary, resp.) bound. we also show its soundness and completeness (for eal and lal with type fixpoints), by using a simple geometry of interaction model (context semantics).", "date_create": "2007-04-18", "area": "cs.lo cs.pl", "authors": ["baillot", "coppola", "lago"]}, {"idpaper": "0704.2475", "title": "physical layer network coding", "abstract": "a main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., ieee 802.11). this paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. specifically, we propose a physical-layer network coding (pnc) scheme to coordinate transmissions among nodes. in contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, pnc makes use of the additive nature of simultaneously arriving electromagnetic (em) waves for equivalent coding operation. and in doing so, pnc can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. more specifically, the information-theoretic capacity of pnc is almost double that of traditional transmission in the snr region of practical interest (higher than 0db). we believe this is a first paper that ventures into em-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["shengli", "liew", "lam"]}, {"idpaper": "0704.2475", "title": "physical layer network coding", "abstract": "a main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., ieee 802.11). this paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. specifically, we propose a physical-layer network coding (pnc) scheme to coordinate transmissions among nodes. in contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, pnc makes use of the additive nature of simultaneously arriving electromagnetic (em) waves for equivalent coding operation. and in doing so, pnc can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. more specifically, the information-theoretic capacity of pnc is almost double that of traditional transmission in the snr region of practical interest (higher than 0db). we believe this is a first paper that ventures into em-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["shengli", "liew", "lam"]}, {"idpaper": "0704.2475", "title": "physical layer network coding", "abstract": "a main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., ieee 802.11). this paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. specifically, we propose a physical-layer network coding (pnc) scheme to coordinate transmissions among nodes. in contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, pnc makes use of the additive nature of simultaneously arriving electromagnetic (em) waves for equivalent coding operation. and in doing so, pnc can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. more specifically, the information-theoretic capacity of pnc is almost double that of traditional transmission in the snr region of practical interest (higher than 0db). we believe this is a first paper that ventures into em-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["shengli", "liew", "lam"]}, {"idpaper": "0704.2475", "title": "physical layer network coding", "abstract": "a main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., ieee 802.11). this paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. specifically, we propose a physical-layer network coding (pnc) scheme to coordinate transmissions among nodes. in contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, pnc makes use of the additive nature of simultaneously arriving electromagnetic (em) waves for equivalent coding operation. and in doing so, pnc can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. more specifically, the information-theoretic capacity of pnc is almost double that of traditional transmission in the snr region of practical interest (higher than 0db). we believe this is a first paper that ventures into em-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["shengli", "liew", "lam"]}, {"idpaper": "0704.2475", "title": "physical layer network coding", "abstract": "a main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., ieee 802.11). this paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. specifically, we propose a physical-layer network coding (pnc) scheme to coordinate transmissions among nodes. in contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, pnc makes use of the additive nature of simultaneously arriving electromagnetic (em) waves for equivalent coding operation. and in doing so, pnc can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. more specifically, the information-theoretic capacity of pnc is almost double that of traditional transmission in the snr region of practical interest (higher than 0db). we believe this is a first paper that ventures into em-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["shengli", "liew", "lam"]}, {"idpaper": "0704.2544", "title": "existence proofs of some exit like functions", "abstract": "the extended bp (ebp) generalized exit (gexit) function introduced in \\cite{mmru05} plays a fundamental role in the asymptotic analysis of sparse graph codes. for transmission over the binary erasure channel (bec) the analytic properties of the ebp gexit function are relatively simple and well understood. the general case is much harder and even the existence of the curve is not known in general. we introduce some tools from non-linear analysis which can be useful to prove the existence of exit like curves in some cases. the main tool is the krasnoselskii-rabinowitz (kr) bifurcation theorem.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["rathi", "urbanke"]}, {"idpaper": "0704.2544", "title": "existence proofs of some exit like functions", "abstract": "the extended bp (ebp) generalized exit (gexit) function introduced in \\cite{mmru05} plays a fundamental role in the asymptotic analysis of sparse graph codes. for transmission over the binary erasure channel (bec) the analytic properties of the ebp gexit function are relatively simple and well understood. the general case is much harder and even the existence of the curve is not known in general. we introduce some tools from non-linear analysis which can be useful to prove the existence of exit like curves in some cases. the main tool is the krasnoselskii-rabinowitz (kr) bifurcation theorem.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["rathi", "urbanke"]}, {"idpaper": "0704.2544", "title": "existence proofs of some exit like functions", "abstract": "the extended bp (ebp) generalized exit (gexit) function introduced in \\cite{mmru05} plays a fundamental role in the asymptotic analysis of sparse graph codes. for transmission over the binary erasure channel (bec) the analytic properties of the ebp gexit function are relatively simple and well understood. the general case is much harder and even the existence of the curve is not known in general. we introduce some tools from non-linear analysis which can be useful to prove the existence of exit like curves in some cases. the main tool is the krasnoselskii-rabinowitz (kr) bifurcation theorem.", "date_create": "2007-04-19", "area": "cs.it math.it", "authors": ["rathi", "urbanke"]}, {"idpaper": "0704.2609", "title": "a-infinity structure on simplicial complexes", "abstract": "a discrete (finite-difference) analogue of differential forms is considered, defined on simplicial complexes, including triangulations of continuous manifolds. various operations are explicitly defined on these forms, including exterior derivative and exterior product. the latter one is non-associative. instead, as anticipated, it is a part of non-trivial a-infinity structure, involving a chain of poly-linear operations, constrained by nilpotency relation: (d + \\wedge + m + ...)^n = 0 with n=2.", "date_create": "2007-04-19", "area": "math.gt cs.dm hep-th", "authors": ["dolotin", "morozov", "shakirov"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2659", "title": "minimum expected distortion in gaussian layered broadcast coding with   successive refinement", "abstract": "a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. in the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. in the optimal power distribution, as snr increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. on the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["ng", "gunduz", "goldsmith", "erkip"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2668", "title": "supervised feature selection via dependence estimation", "abstract": "we introduce a framework for filtering features that employs the hilbert-schmidt independence criterion (hsic) as a measure of dependence between the features and the labels. the key idea is that good features should maximise such dependence. feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. we demonstrate the usefulness of our method on both artificial and real world datasets.", "date_create": "2007-04-20", "area": "cs.lg", "authors": ["song", "smola", "gretton", "borgwardt", "bedo"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2680", "title": "a channel that heats up", "abstract": "motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. for this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", "date_create": "2007-04-20", "area": "cs.it math.it", "authors": ["koch", "lapidoth", "sotiriadis"]}, {"idpaper": "0704.2725", "title": "exploiting heavy tails in training times of multilayer perceptrons: a   case study with the uci thyroid disease database", "abstract": "the random initialization of weights of a multilayer perceptron makes it possible to model its training process as a las vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. this modeling is used to perform a case study on a well-known pattern recognition benchmark: the uci thyroid disease database. empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. this fact is exploited to reduce the training time cost by applying two simple restart strategies. the first assumes full knowledge of the distribution yielding a 40% cut down in expected time with respect to the training without restarts. the second, assumes null knowledge, yielding a reduction ranging from 9% to 23%.", "date_create": "2007-04-20", "area": "cs.ne", "authors": ["cebrian", "cantador"]}, {"idpaper": "0704.2725", "title": "exploiting heavy tails in training times of multilayer perceptrons: a   case study with the uci thyroid disease database", "abstract": "the random initialization of weights of a multilayer perceptron makes it possible to model its training process as a las vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. this modeling is used to perform a case study on a well-known pattern recognition benchmark: the uci thyroid disease database. empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. this fact is exploited to reduce the training time cost by applying two simple restart strategies. the first assumes full knowledge of the distribution yielding a 40% cut down in expected time with respect to the training without restarts. the second, assumes null knowledge, yielding a reduction ranging from 9% to 23%.", "date_create": "2007-04-20", "area": "cs.ne", "authors": ["cebrian", "cantador"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2841", "title": "a high-throughput cross-layer scheme for distributed wireless ad hoc   networks", "abstract": "in wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. in recent work, ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. however, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. in this paper, a technique that significantly reduces the collaboration overhead is proposed. it consists of two phases. in the first phase, nodes transmit locally in a random access fashion. collisions, when they occur, are viewed as linear mixtures of the collided packets. in the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. this step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. the scheme requires that each node has global knowledge of the network coordinates. the proposed scheme can achieve high throughput, which in certain cases exceeds one.", "date_create": "2007-04-21", "area": "cs.it math.it", "authors": ["petropulu", "dong", "poor"]}, {"idpaper": "0704.2902", "title": "recommending related papers based on digital library access records", "abstract": "an important goal for digital libraries is to enable researchers to more easily explore related work. while citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. in particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.", "date_create": "2007-04-23", "area": "cs.dl cs.ir", "authors": ["pohl", "radlinski", "joachims"]}, {"idpaper": "0704.2902", "title": "recommending related papers based on digital library access records", "abstract": "an important goal for digital libraries is to enable researchers to more easily explore related work. while citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. in particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.", "date_create": "2007-04-23", "area": "cs.dl cs.ir", "authors": ["pohl", "radlinski", "joachims"]}, {"idpaper": "0704.2902", "title": "recommending related papers based on digital library access records", "abstract": "an important goal for digital libraries is to enable researchers to more easily explore related work. while citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. in particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.", "date_create": "2007-04-23", "area": "cs.dl cs.ir", "authors": ["pohl", "radlinski", "joachims"]}, {"idpaper": "0704.2919", "title": "on verifying and engineering the well-gradedness of a union-closed   family", "abstract": "current techniques for generating a knowledge space, such as query, guarantees that the resulting structure is closed under union, but not that it satisfies wellgradedness, which is one of the defining conditions for a learning space. we give necessary and sufficient conditions on the base of a union-closed set family that ensures that the family is well-graded. we consider two cases, depending on whether or not the family contains the empty set. we also provide algorithms for efficiently testing these conditions, and for augmenting a set family in a minimal way to one that satisfies these conditions.", "date_create": "2007-04-23", "area": "math.co cs.dm cs.ds", "authors": ["eppstein", "falmagne", "uzun"]}, {"idpaper": "0704.2919", "title": "on verifying and engineering the well-gradedness of a union-closed   family", "abstract": "current techniques for generating a knowledge space, such as query, guarantees that the resulting structure is closed under union, but not that it satisfies wellgradedness, which is one of the defining conditions for a learning space. we give necessary and sufficient conditions on the base of a union-closed set family that ensures that the family is well-graded. we consider two cases, depending on whether or not the family contains the empty set. we also provide algorithms for efficiently testing these conditions, and for augmenting a set family in a minimal way to one that satisfies these conditions.", "date_create": "2007-04-23", "area": "math.co cs.dm cs.ds", "authors": ["eppstein", "falmagne", "uzun"]}, {"idpaper": "0704.2919", "title": "on verifying and engineering the well-gradedness of a union-closed   family", "abstract": "current techniques for generating a knowledge space, such as query, guarantees that the resulting structure is closed under union, but not that it satisfies wellgradedness, which is one of the defining conditions for a learning space. we give necessary and sufficient conditions on the base of a union-closed set family that ensures that the family is well-graded. we consider two cases, depending on whether or not the family contains the empty set. we also provide algorithms for efficiently testing these conditions, and for augmenting a set family in a minimal way to one that satisfies these conditions.", "date_create": "2007-04-23", "area": "math.co cs.dm cs.ds", "authors": ["eppstein", "falmagne", "uzun"]}, {"idpaper": "0704.3035", "title": "achievable rates for two-way wire-tap channels", "abstract": "we consider two-way wire-tap channels, where two users are communicating with each other in the presence of an eavesdropper, who has access to the communications through a multiple-access channel. we find achievable rates for two different scenarios, the gaussian two-way wire-tap channel, (gtw-wt), and the binary additive two-way wire-tap channel, (batw-wt). it is shown that the two-way channels inherently provide a unique advantage for wire-tapped scenarios, as the users know their own transmitted signals and in effect help encrypt the other user's messages, similar to a one-time pad. we compare the achievable rates to that of the gaussian multiple-access wire-tap channel (gmac-wt) to illustrate this advantage.", "date_create": "2007-04-23", "area": "cs.it cs.cr math.it", "authors": ["tekin", "yener"]}, {"idpaper": "0704.3035", "title": "achievable rates for two-way wire-tap channels", "abstract": "we consider two-way wire-tap channels, where two users are communicating with each other in the presence of an eavesdropper, who has access to the communications through a multiple-access channel. we find achievable rates for two different scenarios, the gaussian two-way wire-tap channel, (gtw-wt), and the binary additive two-way wire-tap channel, (batw-wt). it is shown that the two-way channels inherently provide a unique advantage for wire-tapped scenarios, as the users know their own transmitted signals and in effect help encrypt the other user's messages, similar to a one-time pad. we compare the achievable rates to that of the gaussian multiple-access wire-tap channel (gmac-wt) to illustrate this advantage.", "date_create": "2007-04-23", "area": "cs.it cs.cr math.it", "authors": ["tekin", "yener"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3157", "title": "experimenting with recursive queries in database and logic programming   systems", "abstract": "this paper considers the problem of reasoning on massive amounts of (possibly distributed) data. presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) dbmss is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. this paper provides a contribution in this setting; it presents a new system, called dlv$^{db}$, which aims to solve these problems. moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: ldl++, xsb, smodels and three top-level commercial dbmss. dlv$^{db}$ significantly outperforms even the commercial database systems on recursive queries. to appear in theory and practice of logic programming (tplp)", "date_create": "2007-04-24", "area": "cs.ai cs.db", "authors": ["terracina", "leone", "lio", "panetta"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3197", "title": "euclidean shortest paths in simple cube curves at a glance", "abstract": "this paper reports about the development of two provably correct approximate algorithms which calculate the euclidean shortest path (esp) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal o}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. a run-time diagram also illustrates this linear-time behavior of the implemented esp algorithm.", "date_create": "2007-04-23", "area": "cs.cg cs.dm", "authors": ["li", "klette"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3287", "title": "sample size cognizant detection of signals in white noise", "abstract": "the detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. we present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals.   this motivates our heuristic definition of the 'effective number of identifiable signals.' numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to wax and kailath's \"asymptotically consistent\" mdl based estimator.", "date_create": "2007-04-24", "area": "cs.it math.it", "authors": ["rao", "edelman"]}, {"idpaper": "0704.3292", "title": "coalition games with cooperative transmission: a cure for the curse of   boundary nodes in selfish packet-forwarding wireless networks", "abstract": "in wireless packet-forwarding networks with selfish nodes, applications of a repeated game can induce the nodes to forward each others' packets, so that the network performance can be improved. however, the nodes on the boundary of such networks cannot benefit from this strategy, as the other nodes do not depend on them. this problem is sometimes known as the curse of the boundary nodes. to overcome this problem, an approach based on coalition games is proposed, in which the boundary nodes can use cooperative transmission to help the backbone nodes in the middle of the network. in return, the backbone nodes are willing to forward the boundary nodes' packets. the stability of the coalitions is studied using the concept of a core. then two types of fairness, namely, the min-max fairness using nucleolus and the average fairness using the shapley function are investigated. finally, a protocol is designed using both repeated games and coalition games. simulation results show how boundary nodes and backbone nodes form coalitions together according to different fairness criteria. the proposed protocol can improve the network connectivity by about 50%, compared with pure repeated game schemes.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["han", "poor"]}, {"idpaper": "0704.3292", "title": "coalition games with cooperative transmission: a cure for the curse of   boundary nodes in selfish packet-forwarding wireless networks", "abstract": "in wireless packet-forwarding networks with selfish nodes, applications of a repeated game can induce the nodes to forward each others' packets, so that the network performance can be improved. however, the nodes on the boundary of such networks cannot benefit from this strategy, as the other nodes do not depend on them. this problem is sometimes known as the curse of the boundary nodes. to overcome this problem, an approach based on coalition games is proposed, in which the boundary nodes can use cooperative transmission to help the backbone nodes in the middle of the network. in return, the backbone nodes are willing to forward the boundary nodes' packets. the stability of the coalitions is studied using the concept of a core. then two types of fairness, namely, the min-max fairness using nucleolus and the average fairness using the shapley function are investigated. finally, a protocol is designed using both repeated games and coalition games. simulation results show how boundary nodes and backbone nodes form coalitions together according to different fairness criteria. the proposed protocol can improve the network connectivity by about 50%, compared with pure repeated game schemes.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["han", "poor"]}, {"idpaper": "0704.3292", "title": "coalition games with cooperative transmission: a cure for the curse of   boundary nodes in selfish packet-forwarding wireless networks", "abstract": "in wireless packet-forwarding networks with selfish nodes, applications of a repeated game can induce the nodes to forward each others' packets, so that the network performance can be improved. however, the nodes on the boundary of such networks cannot benefit from this strategy, as the other nodes do not depend on them. this problem is sometimes known as the curse of the boundary nodes. to overcome this problem, an approach based on coalition games is proposed, in which the boundary nodes can use cooperative transmission to help the backbone nodes in the middle of the network. in return, the backbone nodes are willing to forward the boundary nodes' packets. the stability of the coalitions is studied using the concept of a core. then two types of fairness, namely, the min-max fairness using nucleolus and the average fairness using the shapley function are investigated. finally, a protocol is designed using both repeated games and coalition games. simulation results show how boundary nodes and backbone nodes form coalitions together according to different fairness criteria. the proposed protocol can improve the network connectivity by about 50%, compared with pure repeated game schemes.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["han", "poor"]}, {"idpaper": "0704.3313", "title": "straggler identification in round-trip data streams via newton's   identities and invertible bloom filters", "abstract": "we introduce the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. the goal is to do this in o(n) space, where n is the total number of identities. the straggler identification problem has applications, for example, in determining the set of unacknowledged packets in a high-bandwidth multicast data stream. we provide a deterministic solution to the straggler identification problem that uses only o(d log n) bits and is based on a novel application of newton's identities for symmetric polynomials. this solution can identify any subset of d stragglers from a set of n o(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. nevertheless, we show that there is a simple randomized solution using o(d log n log(1/epsilon)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where epsilon>0 is a user-defined parameter bounding the probability of an incorrect response. this randomized solution is based on a new type of bloom filter, which we call the invertible bloom filter.", "date_create": "2007-04-25", "area": "cs.ds", "authors": ["eppstein", "goodrich"]}, {"idpaper": "0704.3313", "title": "straggler identification in round-trip data streams via newton's   identities and invertible bloom filters", "abstract": "we introduce the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. the goal is to do this in o(n) space, where n is the total number of identities. the straggler identification problem has applications, for example, in determining the set of unacknowledged packets in a high-bandwidth multicast data stream. we provide a deterministic solution to the straggler identification problem that uses only o(d log n) bits and is based on a novel application of newton's identities for symmetric polynomials. this solution can identify any subset of d stragglers from a set of n o(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. nevertheless, we show that there is a simple randomized solution using o(d log n log(1/epsilon)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where epsilon>0 is a user-defined parameter bounding the probability of an incorrect response. this randomized solution is based on a new type of bloom filter, which we call the invertible bloom filter.", "date_create": "2007-04-25", "area": "cs.ds", "authors": ["eppstein", "goodrich"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3316", "title": "vocabulary growth in collaborative tagging systems", "abstract": "we analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. we study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. in both cases, we find power-law behaviors with exponents smaller than one. surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", "date_create": "2007-04-25", "area": "cs.ir cond-mat.stat-mech cs.cy physics.data-an", "authors": ["cattuto", "baldassarri", "servedio", "loreto"]}, {"idpaper": "0704.3359", "title": "direct optimization of ranking measures", "abstract": "web page ranking and collaborative filtering require the optimization of sophisticated performance measures. current support vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. we present a new approach which allows direct optimization of the relevant loss functions. this is achieved via structured estimation in hilbert spaces. it is most related to max-margin-markov networks optimization of multivariate performance measures. key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the hungarian marriage algorithm. at test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. experiments show that the our algorithm is fast and that it works very well.", "date_create": "2007-04-25", "area": "cs.ir cs.ai", "authors": ["le", "smola"]}, {"idpaper": "0704.3359", "title": "direct optimization of ranking measures", "abstract": "web page ranking and collaborative filtering require the optimization of sophisticated performance measures. current support vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. we present a new approach which allows direct optimization of the relevant loss functions. this is achieved via structured estimation in hilbert spaces. it is most related to max-margin-markov networks optimization of multivariate performance measures. key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the hungarian marriage algorithm. at test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. experiments show that the our algorithm is fast and that it works very well.", "date_create": "2007-04-25", "area": "cs.ir cs.ai", "authors": ["le", "smola"]}, {"idpaper": "0704.3402", "title": "diversity-multiplexing tradeoff in selective-fading mimo channels", "abstract": "we establish the optimal diversity-multiplexing (dm) tradeoff of coherent time, frequency and time-frequency selective-fading mimo channels and provide a code design criterion for dm-tradeoff optimality. our results are based on the analysis of the \"jensen channel\" associated to a given selective-fading mimo channel. while the original problem seems analytically intractable due to the mutual information being a sum of correlated random variables, the jensen channel is equivalent to the original channel in the sense of the dm-tradeoff and lends itself nicely to analytical treatment. finally, as a consequence of our results, we find that the classical rank criterion for space-time code design (in selective-fading mimo channels) ensures optimality in the sense of the dm-tradeoff.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["coronel", "b\u00f6lcskei"]}, {"idpaper": "0704.3402", "title": "diversity-multiplexing tradeoff in selective-fading mimo channels", "abstract": "we establish the optimal diversity-multiplexing (dm) tradeoff of coherent time, frequency and time-frequency selective-fading mimo channels and provide a code design criterion for dm-tradeoff optimality. our results are based on the analysis of the \"jensen channel\" associated to a given selective-fading mimo channel. while the original problem seems analytically intractable due to the mutual information being a sum of correlated random variables, the jensen channel is equivalent to the original channel in the sense of the dm-tradeoff and lends itself nicely to analytical treatment. finally, as a consequence of our results, we find that the classical rank criterion for space-time code design (in selective-fading mimo channels) ensures optimality in the sense of the dm-tradeoff.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["coronel", "b\u00f6lcskei"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3405", "title": "estimation diversity and energy efficiency in distributed sensing", "abstract": "distributed estimation based on measurements from multiple wireless sensors is investigated. it is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. the observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. assuming that the best linear unbiased estimator (blue) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. the optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. in the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. here, the adaptive power gain is similar to the array gain achieved in multiple-input single-output (miso) multi-antenna systems when channel conditions are known to the transmitter. in the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["cui", "xiao", "goldsmith", "luo", "poor"]}, {"idpaper": "0704.3408", "title": "the trade-off between processing gains of an impulse radio uwb system in   the presence of timing jitter", "abstract": "in time hopping impulse radio, $n_f$ pulses of duration $t_c$ are transmitted for each information symbol. this gives rise to two types of processing gain: (i) pulse combining gain, which is a factor $n_f$, and (ii) pulse spreading gain, which is $n_c=t_f/t_c$, where $t_f$ is the mean interval between two subsequent pulses. this paper investigates the trade-off between these two types of processing gain in the presence of timing jitter. first, an additive white gaussian noise (awgn) channel is considered and approximate closed form expressions for bit error probability are derived for impulse radio systems with and without pulse-based polarity randomization. both symbol-synchronous and chip-synchronous scenarios are considered. the effects of multiple-access interference and timing jitter on the selection of optimal system parameters are explained through theoretical analysis. finally, a multipath scenario is considered and the trade-off between processing gains of a synchronous impulse radio system with pulse-based polarity randomization is analyzed. the effects of the timing jitter, multiple-access interference and inter-frame interference are investigated. simulation studies support the theoretical results.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["gezici", "molisch", "poor", "kobayashi"]}, {"idpaper": "0704.3408", "title": "the trade-off between processing gains of an impulse radio uwb system in   the presence of timing jitter", "abstract": "in time hopping impulse radio, $n_f$ pulses of duration $t_c$ are transmitted for each information symbol. this gives rise to two types of processing gain: (i) pulse combining gain, which is a factor $n_f$, and (ii) pulse spreading gain, which is $n_c=t_f/t_c$, where $t_f$ is the mean interval between two subsequent pulses. this paper investigates the trade-off between these two types of processing gain in the presence of timing jitter. first, an additive white gaussian noise (awgn) channel is considered and approximate closed form expressions for bit error probability are derived for impulse radio systems with and without pulse-based polarity randomization. both symbol-synchronous and chip-synchronous scenarios are considered. the effects of multiple-access interference and timing jitter on the selection of optimal system parameters are explained through theoretical analysis. finally, a multipath scenario is considered and the trade-off between processing gains of a synchronous impulse radio system with pulse-based polarity randomization is analyzed. the effects of the timing jitter, multiple-access interference and inter-frame interference are investigated. simulation studies support the theoretical results.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["gezici", "molisch", "poor", "kobayashi"]}, {"idpaper": "0704.3408", "title": "the trade-off between processing gains of an impulse radio uwb system in   the presence of timing jitter", "abstract": "in time hopping impulse radio, $n_f$ pulses of duration $t_c$ are transmitted for each information symbol. this gives rise to two types of processing gain: (i) pulse combining gain, which is a factor $n_f$, and (ii) pulse spreading gain, which is $n_c=t_f/t_c$, where $t_f$ is the mean interval between two subsequent pulses. this paper investigates the trade-off between these two types of processing gain in the presence of timing jitter. first, an additive white gaussian noise (awgn) channel is considered and approximate closed form expressions for bit error probability are derived for impulse radio systems with and without pulse-based polarity randomization. both symbol-synchronous and chip-synchronous scenarios are considered. the effects of multiple-access interference and timing jitter on the selection of optimal system parameters are explained through theoretical analysis. finally, a multipath scenario is considered and the trade-off between processing gains of a synchronous impulse radio system with pulse-based polarity randomization is analyzed. the effects of the timing jitter, multiple-access interference and inter-frame interference are investigated. simulation studies support the theoretical results.", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["gezici", "molisch", "poor", "kobayashi"]}, {"idpaper": "0704.3433", "title": "bayesian approach to rough set", "abstract": "this paper proposes an approach to training rough set models using bayesian framework trained using markov chain monte carlo (mcmc) method. the prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. markov chain monte carlo sampling is conducted through sampling in the rough set granule space and metropolis algorithm is used as an acceptance criteria. the proposed method is tested to estimate the risk of hiv given demographic data. the results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. in addition the bayesian rough set give the probabilities of the estimated hiv status as well as the linguistic rules describing how the demographic parameters drive the risk of hiv.", "date_create": "2007-04-25", "area": "cs.ai", "authors": ["marwala", "crossingham"]}, {"idpaper": "0704.3433", "title": "bayesian approach to rough set", "abstract": "this paper proposes an approach to training rough set models using bayesian framework trained using markov chain monte carlo (mcmc) method. the prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. markov chain monte carlo sampling is conducted through sampling in the rough set granule space and metropolis algorithm is used as an acceptance criteria. the proposed method is tested to estimate the risk of hiv given demographic data. the results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. in addition the bayesian rough set give the probabilities of the estimated hiv status as well as the linguistic rules describing how the demographic parameters drive the risk of hiv.", "date_create": "2007-04-25", "area": "cs.ai", "authors": ["marwala", "crossingham"]}, {"idpaper": "0704.3433", "title": "bayesian approach to rough set", "abstract": "this paper proposes an approach to training rough set models using bayesian framework trained using markov chain monte carlo (mcmc) method. the prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. markov chain monte carlo sampling is conducted through sampling in the rough set granule space and metropolis algorithm is used as an acceptance criteria. the proposed method is tested to estimate the risk of hiv given demographic data. the results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. in addition the bayesian rough set give the probabilities of the estimated hiv status as well as the linguistic rules describing how the demographic parameters drive the risk of hiv.", "date_create": "2007-04-25", "area": "cs.ai", "authors": ["marwala", "crossingham"]}, {"idpaper": "0704.3433", "title": "bayesian approach to rough set", "abstract": "this paper proposes an approach to training rough set models using bayesian framework trained using markov chain monte carlo (mcmc) method. the prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. markov chain monte carlo sampling is conducted through sampling in the rough set granule space and metropolis algorithm is used as an acceptance criteria. the proposed method is tested to estimate the risk of hiv given demographic data. the results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. in addition the bayesian rough set give the probabilities of the estimated hiv status as well as the linguistic rules describing how the demographic parameters drive the risk of hiv.", "date_create": "2007-04-25", "area": "cs.ai", "authors": ["marwala", "crossingham"]}, {"idpaper": "0704.3433", "title": "bayesian approach to rough set", "abstract": "this paper proposes an approach to training rough set models using bayesian framework trained using markov chain monte carlo (mcmc) method. the prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. markov chain monte carlo sampling is conducted through sampling in the rough set granule space and metropolis algorithm is used as an acceptance criteria. the proposed method is tested to estimate the risk of hiv given demographic data. the results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. in addition the bayesian rough set give the probabilities of the estimated hiv status as well as the linguistic rules describing how the demographic parameters drive the risk of hiv.", "date_create": "2007-04-25", "area": "cs.ai", "authors": ["marwala", "crossingham"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3434", "title": "on sensing capacity of sensor networks for the class of linear   observation, fixed snr models", "abstract": "in this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed snr regime. sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. in this context sparsity of the phenomena is a key feature that determines sensing capacity. precluding the snr of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. nevertheless the development there was motivated from an algorithmic perspective. in this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. in this direction we first generalize the fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed snr regime sensing capacity goes down to zero as sparsity goes down to zero. this means that disproportionately more sensors are required to monitor very sparse events. our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. in this direction we show the following results (a) sensing capacity goes down as sensing diversity per sensor goes down; (b) random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", "date_create": "2007-04-25", "area": "cs.it math.it", "authors": ["aeron", "zhao", "saligrama"]}, {"idpaper": "0704.3453", "title": "an adaptive strategy for the classification of g-protein coupled   receptors", "abstract": "one of the major problems in computational biology is the inability of existing classification models to incorporate expanding and new domain knowledge. this problem of static classification models is addressed in this paper by the introduction of incremental learning for problems in bioinformatics. many machine learning tools have been applied to this problem using static machine learning structures such as neural networks or support vector machines that are unable to accommodate new information into their existing models. we utilize the fuzzy artmap as an alternate machine learning system that has the ability of incrementally learning new data as it becomes available. the fuzzy artmap is found to be comparable to many of the widespread machine learning systems. the use of an evolutionary strategy in the selection and combination of individual classifiers into an ensemble system, coupled with the incremental learning ability of the fuzzy artmap is proven to be suitable as a pattern classifier. the algorithm presented is tested using data from the g-coupled protein receptors database and shows good accuracy of 83%. the system presented is also generally applicable, and can be used in problems in genomics and proteomics.", "date_create": "2007-04-25", "area": "cs.ai q-bio.qm", "authors": ["mohamed", "rubin", "marwala"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3500", "title": "une plate-forme dynamique pour l'\\'evaluation des performances des bases   de donn\\'ees \\`a objets", "abstract": "in object-oriented or object-relational databases such as multimedia databases or most xml databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. however, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. this paper opens up research regarding this issue by proposing a dynamic object evaluation framework (doef). doef accomplishes access pattern change by defining configurable styles of change. it is a preliminary prototype that has been designed to be open and fully extensible. though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. furthermore, new access pattern change models can be added too. to illustrate the capabilities of doef, we conducted two different sets of experiments. in the first set of experiments, we used doef to compare the performances of four state of the art dynamic clustering algorithms. the results show that doef is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. they also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. in the second set of experiments, we used doef to compare the performance of two different object stores: platypus and shore. the use of doef exposed the poor swapping performance of platypus.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["he", "darmont"]}, {"idpaper": "0704.3501", "title": "conception d'un banc d'essais d\\'ecisionnel", "abstract": "we present in this paper a new benchmark for evaluating the performances of data warehouses. benchmarking is useful either to system users for comparing the performances of different systems, or to system engineers for testing the effect of various design choices. while the tpc (transaction processing performance council) standard benchmarks address the first point, they are not tuneable enough to address the second one. our data warehouse engineering benchmark (dweb) allows to generate various ad-hoc synthetic data warehouses and workloads. dweb is fully parameterized. however, two levels of parameterization keep it easy to tune. since dweb mainly meets engineering benchmarking needs, it is complimentary to the tpc standard benchmarks, and not a competitor. finally, dweb is implemented as a java free software that can be interfaced with most existing relational database management systems.", "date_create": "2007-04-26", "area": "cs.db", "authors": ["darmont", "bentayeb", "boussa\u00efd"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3573", "title": "simulating spin systems on ianus, an fpga-based computer", "abstract": "we describe the hardwired implementation of algorithms for monte carlo simulations of a large class of spin models. we have implemented these algorithms as vhdl codes and we have mapped them onto a dedicated processor based on a large fpga device. the measured performance on one such processor is comparable to o(100) carefully programmed high-end pcs: it turns out to be even better for some selected spin models. we describe here codes that we are currently executing on the ianus massively parallel fpga-based system.", "date_create": "2007-04-26", "area": "cond-mat.dis-nn cs.ar", "authors": ["belletti", "cotallo", "cruz", "fern\u00e1ndez", "gordillo", "maiorano", "mantovani", "marinari", "mart\u00edn-mayor", "mu\u00f1oz-sudupe", "navarro", "p\u00e9rez-gaviro", "ruiz-lorenzo", "schifano", "sciretti", "taranc\u00f3n", "tripiccione", "velasco"]}, {"idpaper": "0704.3635", "title": "rough sets computations to impute missing data", "abstract": "many techniques for handling missing data have been proposed in the literature. most of these techniques are overly complex. this paper explores an imputation technique based on rough set computations. in this paper, characteristic relations are introduced to describe incompletely specified decision tables.it is shown that the basic rough set idea of lower and upper approximations for incompletely specified decision tables may be defined in a variety of different ways. empirical results obtained using real data are given and they provide a valuable and promising insight to the problem of missing data. missing data were predicted with an accuracy of up to 99%.", "date_create": "2007-04-26", "area": "cs.cv cs.ir", "authors": ["nelwamondo", "marwala"]}, {"idpaper": "0704.3643", "title": "sabbath day home automation: \"it's like mixing technology and religion\"", "abstract": "we present a qualitative study of 20 american orthodox jewish families' use of home automation for religious purposes. these lead users offer insight into real-life, long-term experience with home automation technologies. we discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. we also discuss the relationship of home automation to family life. we draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", "date_create": "2007-04-26", "area": "cs.hc", "authors": ["woodruff", "augustin", "foucault"]}, {"idpaper": "0704.3643", "title": "sabbath day home automation: \"it's like mixing technology and religion\"", "abstract": "we present a qualitative study of 20 american orthodox jewish families' use of home automation for religious purposes. these lead users offer insight into real-life, long-term experience with home automation technologies. we discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. we also discuss the relationship of home automation to family life. we draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", "date_create": "2007-04-26", "area": "cs.hc", "authors": ["woodruff", "augustin", "foucault"]}, {"idpaper": "0704.3643", "title": "sabbath day home automation: \"it's like mixing technology and religion\"", "abstract": "we present a qualitative study of 20 american orthodox jewish families' use of home automation for religious purposes. these lead users offer insight into real-life, long-term experience with home automation technologies. we discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. we also discuss the relationship of home automation to family life. we draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", "date_create": "2007-04-26", "area": "cs.hc", "authors": ["woodruff", "augustin", "foucault"]}, {"idpaper": "0704.3643", "title": "sabbath day home automation: \"it's like mixing technology and religion\"", "abstract": "we present a qualitative study of 20 american orthodox jewish families' use of home automation for religious purposes. these lead users offer insight into real-life, long-term experience with home automation technologies. we discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. we also discuss the relationship of home automation to family life. we draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", "date_create": "2007-04-26", "area": "cs.hc", "authors": ["woodruff", "augustin", "foucault"]}, {"idpaper": "0704.3643", "title": "sabbath day home automation: \"it's like mixing technology and religion\"", "abstract": "we present a qualitative study of 20 american orthodox jewish families' use of home automation for religious purposes. these lead users offer insight into real-life, long-term experience with home automation technologies. we discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. we also discuss the relationship of home automation to family life. we draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", "date_create": "2007-04-26", "area": "cs.hc", "authors": ["woodruff", "augustin", "foucault"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3646", "title": "lower bounds on implementing robust and resilient mediators", "abstract": "we consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. we prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). the bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", "date_create": "2007-04-26", "area": "cs.gt cs.cr cs.dc", "authors": ["abraham", "dolev", "halpern"]}, {"idpaper": "0704.3647", "title": "evaluating personal archiving strategies for internet-based information", "abstract": "internet-based personal digital belongings present different vulnerabilities than locally stored materials. we use responses to a survey of people who have recovered lost websites, in combination with supplementary interviews, to paint a fuller picture of current curatorial strategies and practices. we examine the types of personal, topical, and commercial websites that respondents have lost and the reasons they have lost this potentially valuable material. we further explore what they have tried to recover and how the loss influences their subsequent practices. we found that curation of personal digital materials in online stores bears some striking similarities to the curation of similar materials stored locally in that study participants continue to archive personal assets by relying on a combination of benign neglect, sporadic backups, and unsystematic file replication. however, we have also identified issues specific to internet-based material: how risk is spread by distributing the files among multiple servers and services; the circular reasoning participants use when they discuss the safety of their digital assets; and the types of online material that are particularly vulnerable to loss. the study reveals ways in which expectations of permanence and notification are violated and situations in which benign neglect has far greater consequences for the long-term fate of important digital assets.", "date_create": "2007-04-26", "area": "cs.dl cs.cy cs.hc", "authors": ["marshall", "mccown", "nelson"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3665", "title": "on the development of text input method - lessons learned", "abstract": "intelligent input methods (im) are essential for making text entries in many east asian scripts, but their application to other languages has not been fully explored. this paper discusses how such tools can contribute to the development of computer processing of other oriental languages. we propose a design philosophy that regards im as a text service platform, and treats the study of im as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (hci), and natural language processing (nlp). we discuss these three perspectives and indicate a number of possible future research directions.", "date_create": "2007-04-27", "area": "cs.cl cs.hc", "authors": ["jiang", "liu", "hsieh", "hsu"]}, {"idpaper": "0704.3683", "title": "the complexity of weighted boolean #csp", "abstract": "this paper gives a dichotomy theorem for the complexity of computing the partition function of an instance of a weighted boolean constraint satisfaction problem. the problem is parameterised by a finite set f of non-negative functions that may be used to assign weights to the configurations (feasible solutions) of a problem instance. classical constraint satisfaction problems correspond to the special case of 0,1-valued functions. we show that the partition function, i.e. the sum of the weights of all configurations, can be computed in polynomial time if either (1) every function in f is of ``product type'', or (2) every function in f is ``pure affine''. for every other fixed set f, computing the partition function is fp^{#p}-complete.", "date_create": "2007-04-27", "area": "cs.cc math.co", "authors": ["dyer", "goldberg", "jerrum"]}, {"idpaper": "0704.3683", "title": "the complexity of weighted boolean #csp", "abstract": "this paper gives a dichotomy theorem for the complexity of computing the partition function of an instance of a weighted boolean constraint satisfaction problem. the problem is parameterised by a finite set f of non-negative functions that may be used to assign weights to the configurations (feasible solutions) of a problem instance. classical constraint satisfaction problems correspond to the special case of 0,1-valued functions. we show that the partition function, i.e. the sum of the weights of all configurations, can be computed in polynomial time if either (1) every function in f is of ``product type'', or (2) every function in f is ``pure affine''. for every other fixed set f, computing the partition function is fp^{#p}-complete.", "date_create": "2007-04-27", "area": "cs.cc math.co", "authors": ["dyer", "goldberg", "jerrum"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3881", "title": "a unified approach to energy-efficient power control in large cdma   systems", "abstract": "a unified approach to energy-efficient power control is proposed for code-division multiple access (cdma) networks. the approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (mmse) receiver, and the (nonlinear) optimal detectors. it exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (sir) in the large-system limit. it is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the nash equilibrium is sir-balanced. in addition, a unified power control (upc) algorithm for reaching the nash equilibrium is proposed. the algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. the convergence of the algorithm is proved for the matched filter, the decorrelator, and the mmse receiver, and is demonstrated by means of simulation for an optimal detector. moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["meshkati", "guo", "poor", "schwartz"]}, {"idpaper": "0704.3890", "title": "an algorithm for clock synchronization with the gradient property in   sensor networks", "abstract": "we introduce a distributed algorithm for clock synchronization in sensor networks. our algorithm assumes that nodes in the network only know their immediate neighborhoods and an upper bound on the network's diameter. clock-synchronization messages are only sent as part of the communication, assumed reasonably frequent, that already takes place among nodes. the algorithm has the gradient property of [2], achieving an o(1) worst-case skew between the logical clocks of neighbors. as in the case of [3,8], the algorithm's actions are such that no constant lower bound exists on the rate at which logical clocks progress in time, and for this reason the lower bound of [2,5] that forbids constant skew between neighbors does not apply.", "date_create": "2007-04-30", "area": "cs.dc", "authors": ["pussente", "barbosa"]}, {"idpaper": "0704.3890", "title": "an algorithm for clock synchronization with the gradient property in   sensor networks", "abstract": "we introduce a distributed algorithm for clock synchronization in sensor networks. our algorithm assumes that nodes in the network only know their immediate neighborhoods and an upper bound on the network's diameter. clock-synchronization messages are only sent as part of the communication, assumed reasonably frequent, that already takes place among nodes. the algorithm has the gradient property of [2], achieving an o(1) worst-case skew between the logical clocks of neighbors. as in the case of [3,8], the algorithm's actions are such that no constant lower bound exists on the rate at which logical clocks progress in time, and for this reason the lower bound of [2,5] that forbids constant skew between neighbors does not apply.", "date_create": "2007-04-30", "area": "cs.dc", "authors": ["pussente", "barbosa"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3904", "title": "acyclic preference systems in p2p networks", "abstract": "in this work we study preference systems natural for the peer-to-peer paradigm. most of them fall in three categories: global, symmetric and complementary. all these systems share an acyclicity property. as a consequence, they admit a stable (or pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. we analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. this gives a method to merge acyclic preference systems and retain the acyclicity. we also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. in particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", "date_create": "2007-04-30", "area": "cs.ds cs.gt", "authors": ["gai", "lebedev", "mathieu", "de montgolfier", "reynier", "viennot"]}, {"idpaper": "0704.3905", "title": "ensemble learning for free with evolutionary algorithms ?", "abstract": "evolutionary learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. in the meanwhile, ensemble learning, one of the most efficient approaches in supervised machine learning for the last decade, proceeds by building a population of diverse classifiers. ensemble learning with evolutionary computation thus receives increasing attention. the evolutionary ensemble learning (eel) approach presented in this paper features two contributions. first, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. further, a new selection criterion based on the classification margin is proposed. this criterion is used to extract the classifier ensemble from the final population only (off-line) or incrementally along evolution (on-line). experiments on a set of benchmark problems show that off-line outperforms single-hypothesis evolutionary learning and state-of-art boosting and generates smaller classifier ensembles.", "date_create": "2007-04-30", "area": "cs.ai", "authors": ["gagn\u00e9", "sebag", "schoenauer", "tomassini"]}, {"idpaper": "0704.3905", "title": "ensemble learning for free with evolutionary algorithms ?", "abstract": "evolutionary learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. in the meanwhile, ensemble learning, one of the most efficient approaches in supervised machine learning for the last decade, proceeds by building a population of diverse classifiers. ensemble learning with evolutionary computation thus receives increasing attention. the evolutionary ensemble learning (eel) approach presented in this paper features two contributions. first, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. further, a new selection criterion based on the classification margin is proposed. this criterion is used to extract the classifier ensemble from the final population only (off-line) or incrementally along evolution (on-line). experiments on a set of benchmark problems show that off-line outperforms single-hypothesis evolutionary learning and state-of-art boosting and generates smaller classifier ensembles.", "date_create": "2007-04-30", "area": "cs.ai", "authors": ["gagn\u00e9", "sebag", "schoenauer", "tomassini"]}, {"idpaper": "0704.3905", "title": "ensemble learning for free with evolutionary algorithms ?", "abstract": "evolutionary learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. in the meanwhile, ensemble learning, one of the most efficient approaches in supervised machine learning for the last decade, proceeds by building a population of diverse classifiers. ensemble learning with evolutionary computation thus receives increasing attention. the evolutionary ensemble learning (eel) approach presented in this paper features two contributions. first, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. further, a new selection criterion based on the classification margin is proposed. this criterion is used to extract the classifier ensemble from the final population only (off-line) or incrementally along evolution (on-line). experiments on a set of benchmark problems show that off-line outperforms single-hypothesis evolutionary learning and state-of-art boosting and generates smaller classifier ensembles.", "date_create": "2007-04-30", "area": "cs.ai", "authors": ["gagn\u00e9", "sebag", "schoenauer", "tomassini"]}, {"idpaper": "0704.3905", "title": "ensemble learning for free with evolutionary algorithms ?", "abstract": "evolutionary learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. in the meanwhile, ensemble learning, one of the most efficient approaches in supervised machine learning for the last decade, proceeds by building a population of diverse classifiers. ensemble learning with evolutionary computation thus receives increasing attention. the evolutionary ensemble learning (eel) approach presented in this paper features two contributions. first, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. further, a new selection criterion based on the classification margin is proposed. this criterion is used to extract the classifier ensemble from the final population only (off-line) or incrementally along evolution (on-line). experiments on a set of benchmark problems show that off-line outperforms single-hypothesis evolutionary learning and state-of-art boosting and generates smaller classifier ensembles.", "date_create": "2007-04-30", "area": "cs.ai", "authors": ["gagn\u00e9", "sebag", "schoenauer", "tomassini"]}, {"idpaper": "0705.0010", "title": "critical phenomena in complex networks", "abstract": "the combination of the compactness of networks, featuring small diameters, and their complex architectures results in a variety of critical effects dramatically different from those in cooperative systems on lattices. in the last few years, researchers have made important steps toward understanding the qualitatively new critical phenomena in complex networks. we review the results, concepts, and methods of this rapidly developing field. here we mostly consider two closely related classes of these critical phenomena, namely structural phase transitions in the network architectures and transitions in cooperative models on networks as substrates. we also discuss systems where a network and interacting agents on it influence each other. we overview a wide range of critical phenomena in equilibrium and growing networks including the birth of the giant connected component, percolation, k-core percolation, phenomena near epidemic thresholds, condensation transitions, critical phenomena in spin models placed on networks, synchronization, and self-organized criticality effects in interacting systems on networks. we also discuss strong finite size effects in these systems and highlight open problems and perspectives.", "date_create": "2007-04-30", "area": "cond-mat.stat-mech cs.ni math-ph math.mp physics.soc-ph", "authors": ["dorogovtsev", "goltsev", "mendes"]}, {"idpaper": "0705.0010", "title": "critical phenomena in complex networks", "abstract": "the combination of the compactness of networks, featuring small diameters, and their complex architectures results in a variety of critical effects dramatically different from those in cooperative systems on lattices. in the last few years, researchers have made important steps toward understanding the qualitatively new critical phenomena in complex networks. we review the results, concepts, and methods of this rapidly developing field. here we mostly consider two closely related classes of these critical phenomena, namely structural phase transitions in the network architectures and transitions in cooperative models on networks as substrates. we also discuss systems where a network and interacting agents on it influence each other. we overview a wide range of critical phenomena in equilibrium and growing networks including the birth of the giant connected component, percolation, k-core percolation, phenomena near epidemic thresholds, condensation transitions, critical phenomena in spin models placed on networks, synchronization, and self-organized criticality effects in interacting systems on networks. we also discuss strong finite size effects in these systems and highlight open problems and perspectives.", "date_create": "2007-04-30", "area": "cond-mat.stat-mech cs.ni math-ph math.mp physics.soc-ph", "authors": ["dorogovtsev", "goltsev", "mendes"]}, {"idpaper": "0705.0044", "title": "reliable memories built from unreliable components based on expander   graphs", "abstract": "in this paper, memories built from components subject to transient faults are considered. a fault-tolerant memory architecture based on low-density parity-check codes is proposed and the existence of reliable memories for the adversarial failure model is proved. the proof relies on the expansion property of the underlying tanner graph of the code. an equivalence between the taylor-kuznetsov (tk) scheme and gallager b algorithm is established and the results are extended to the independent failure model. it is also shown that the proposed memory architecture has lower redundancy compared to the tk scheme. the results are illustrated with specific numerical examples.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["chilappagari", "vasic"]}, {"idpaper": "0705.0044", "title": "reliable memories built from unreliable components based on expander   graphs", "abstract": "in this paper, memories built from components subject to transient faults are considered. a fault-tolerant memory architecture based on low-density parity-check codes is proposed and the existence of reliable memories for the adversarial failure model is proved. the proof relies on the expansion property of the underlying tanner graph of the code. an equivalence between the taylor-kuznetsov (tk) scheme and gallager b algorithm is established and the results are extended to the independent failure model. it is also shown that the proposed memory architecture has lower redundancy compared to the tk scheme. the results are illustrated with specific numerical examples.", "date_create": "2007-04-30", "area": "cs.it math.it", "authors": ["chilappagari", "vasic"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0081", "title": "constructions of q-ary constant-weight codes", "abstract": "this paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. the construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", "date_create": "2007-05-01", "area": "cs.it math.it", "authors": ["chee", "ling"]}, {"idpaper": "0705.0252", "title": "power allocation for discrete-input non-ergodic block-fading channels", "abstract": "we consider power allocation algorithms for fixed-rate transmission over nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. optimal power allocation schemes are shown to be direct applications of previous results in the literature. we show that the snr exponent of the optimal short-term scheme is given by the singleton bound. we also illustrate the significant gains available by employing long-term power constraints. due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. we propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["nguyen", "fabregas", "rasmussen"]}, {"idpaper": "0705.0252", "title": "power allocation for discrete-input non-ergodic block-fading channels", "abstract": "we consider power allocation algorithms for fixed-rate transmission over nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. optimal power allocation schemes are shown to be direct applications of previous results in the literature. we show that the snr exponent of the optimal short-term scheme is given by the singleton bound. we also illustrate the significant gains available by employing long-term power constraints. due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. we propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["nguyen", "fabregas", "rasmussen"]}, {"idpaper": "0705.0252", "title": "power allocation for discrete-input non-ergodic block-fading channels", "abstract": "we consider power allocation algorithms for fixed-rate transmission over nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. optimal power allocation schemes are shown to be direct applications of previous results in the literature. we show that the snr exponent of the optimal short-term scheme is given by the singleton bound. we also illustrate the significant gains available by employing long-term power constraints. due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. we propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["nguyen", "fabregas", "rasmussen"]}, {"idpaper": "0705.0252", "title": "power allocation for discrete-input non-ergodic block-fading channels", "abstract": "we consider power allocation algorithms for fixed-rate transmission over nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. optimal power allocation schemes are shown to be direct applications of previous results in the literature. we show that the snr exponent of the optimal short-term scheme is given by the singleton bound. we also illustrate the significant gains available by employing long-term power constraints. due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. we propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["nguyen", "fabregas", "rasmussen"]}, {"idpaper": "0705.0252", "title": "power allocation for discrete-input non-ergodic block-fading channels", "abstract": "we consider power allocation algorithms for fixed-rate transmission over nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. optimal power allocation schemes are shown to be direct applications of previous results in the literature. we show that the snr exponent of the optimal short-term scheme is given by the singleton bound. we also illustrate the significant gains available by employing long-term power constraints. due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. we propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["nguyen", "fabregas", "rasmussen"]}, {"idpaper": "0705.0281", "title": "dynamic clustering in object-oriented databases: an advocacy for   simplicity", "abstract": "we present in this paper three dynamic clustering techniques for object-oriented databases (oodbs). the first two, dynamic, statistical & tunable clustering (dstc) and statclust, exploit both comprehensive usage statistics and the inter-object reference graph. they are quite elaborate. however, they are also complex to implement and induce a high overhead. the third clustering technique, called detection & reclustering of objects (dro), is based on the same principles, but is much simpler to implement. these three clustering algorithm have been implemented in the texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the object clustering benchmark (ocb). the results obtained showed that dro induced a lighter overhead while still achieving better overall performance.", "date_create": "2007-05-02", "area": "cs.db", "authors": ["darmont", "fromantin", "r\u00e9gnier", "gruenwald", "schneider"]}, {"idpaper": "0705.0281", "title": "dynamic clustering in object-oriented databases: an advocacy for   simplicity", "abstract": "we present in this paper three dynamic clustering techniques for object-oriented databases (oodbs). the first two, dynamic, statistical & tunable clustering (dstc) and statclust, exploit both comprehensive usage statistics and the inter-object reference graph. they are quite elaborate. however, they are also complex to implement and induce a high overhead. the third clustering technique, called detection & reclustering of objects (dro), is based on the same principles, but is much simpler to implement. these three clustering algorithm have been implemented in the texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the object clustering benchmark (ocb). the results obtained showed that dro induced a lighter overhead while still achieving better overall performance.", "date_create": "2007-05-02", "area": "cs.db", "authors": ["darmont", "fromantin", "r\u00e9gnier", "gruenwald", "schneider"]}, {"idpaper": "0705.0281", "title": "dynamic clustering in object-oriented databases: an advocacy for   simplicity", "abstract": "we present in this paper three dynamic clustering techniques for object-oriented databases (oodbs). the first two, dynamic, statistical & tunable clustering (dstc) and statclust, exploit both comprehensive usage statistics and the inter-object reference graph. they are quite elaborate. however, they are also complex to implement and induce a high overhead. the third clustering technique, called detection & reclustering of objects (dro), is based on the same principles, but is much simpler to implement. these three clustering algorithm have been implemented in the texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the object clustering benchmark (ocb). the results obtained showed that dro induced a lighter overhead while still achieving better overall performance.", "date_create": "2007-05-02", "area": "cs.db", "authors": ["darmont", "fromantin", "r\u00e9gnier", "gruenwald", "schneider"]}, {"idpaper": "0705.0281", "title": "dynamic clustering in object-oriented databases: an advocacy for   simplicity", "abstract": "we present in this paper three dynamic clustering techniques for object-oriented databases (oodbs). the first two, dynamic, statistical & tunable clustering (dstc) and statclust, exploit both comprehensive usage statistics and the inter-object reference graph. they are quite elaborate. however, they are also complex to implement and induce a high overhead. the third clustering technique, called detection & reclustering of objects (dro), is based on the same principles, but is much simpler to implement. these three clustering algorithm have been implemented in the texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the object clustering benchmark (ocb). the results obtained showed that dro induced a lighter overhead while still achieving better overall performance.", "date_create": "2007-05-02", "area": "cs.db", "authors": ["darmont", "fromantin", "r\u00e9gnier", "gruenwald", "schneider"]}, {"idpaper": "0705.0286", "title": "inverse-free berlekamp-massey-sakata algorithm and small decoders for   algebraic-geometric codes", "abstract": "this paper proposes a novel algorithm for finding error-locators of algebraic-geometric codes that can eliminate the division-calculations of finite fields from the berlekamp-massey-sakata algorithm. this inverse-free algorithm provides full performance in correcting a certain class of errors, generic errors, which includes most errors, and can decode codes on algebraic curves without the determination of unknown syndromes. moreover, we propose three different kinds of architectures that our algorithm can be applied to, and we represent the control operation of shift-registers and switches at each clock-timing with numerical simulations. we estimate the performance in comparison of the total running time and the numbers of multipliers and shift-registers in three architectures with those of the conventional ones for codes on algebraic curves.", "date_create": "2007-05-02", "area": "cs.it math.it", "authors": ["matsui", "mita"]}, {"idpaper": "0705.0315", "title": "wdm and directed star arboricity", "abstract": "a digraph is $m$-labelled if every arc is labelled by an integer in $\\{1, \\dots,m\\}$. motivated by wavelength assignment for multicasts in optical networks, we introduce and study $n$-fibre colourings of labelled digraphs. these are colourings of the arcs of $d$ such that at each vertex $v$, and for each colour $\\alpha$, $in(v,\\alpha)+out(v,\\alpha)\\leq n$ with $in(v,\\alpha)$ the number of arcs coloured $\\alpha$ entering $v$ and $out(v,\\alpha)$ the number of labels $l$ such that there is at least one arc of label $l$ leaving $v$ and coloured with $\\alpha$. the problem is to find the minimum number of colours $\\lambda_n(d)$ such that the $m$-labelled digraph $d$ has an $n$-fibre colouring. in the particular case when $d$ is $1$-labelled, $\\lambda_1(d)$ is called the directed star arboricity of $d$, and is denoted by $dst(d)$. we first show that $dst(d)\\leq 2\\delta^-(d)+1$, and conjecture that if $\\delta^-(d)\\geq 2$, then $dst(d)\\leq 2\\delta^-(d)$. we also prove that for a subcubic digraph $d$, then $dst(d)\\leq 3$, and that if $\\delta^+(d), \\delta^-(d)\\leq 2$, then $dst(d)\\leq 4$. finally, we study $\\lambda_n(m,k)=\\max\\{\\lambda_n(d) \\tq d \\mbox{is $m$-labelled} \\et \\delta^-(d)\\leq k\\}$. we show that if $m\\geq n$, then $\\ds \\left\\lceil\\frac{m}{n}\\left\\lceil \\frac{k}{n}\\right\\rceil + \\frac{k}{n} \\right\\rceil\\leq \\lambda_n(m,k) \\leq\\left\\lceil\\frac{m}{n}\\left\\lceil \\frac{k}{n}\\right\\rceil + \\frac{k}{n} \\right\\rceil + c \\frac{m^2\\log k}{n}$ for some constant $c$. we conjecture that the lower bound should be the right value of $\\lambda_n(m,k)$.", "date_create": "2007-05-02", "area": "cs.ni math.co", "authors": ["amini", "havet", "huc", "thomasse"]}, {"idpaper": "0705.0326", "title": "optimal delay-throughput trade-offs in mobile ad-hoc networks: hybrid   random walk and one-dimensional mobility models", "abstract": "optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models have been established in [23], where we showed that the optimal trade-offs can be achieved using rate-less codes when the required delay guarantees are sufficient large. in this paper, we extend the results to other mobility models including two-dimensional hybrid random walk model, one-dimensional i.i.d. mobility model and one-dimensional hybrid random walk model. we consider both fast mobiles and slow mobiles, and establish the optimal delay-throughput trade-offs under some conditions. joint coding-scheduling algorithms are also proposed to achieve the optimal trade-offs.", "date_create": "2007-05-02", "area": "cs.ni cs.it math.it", "authors": ["ying", "srikant"]}, {"idpaper": "0705.0326", "title": "optimal delay-throughput trade-offs in mobile ad-hoc networks: hybrid   random walk and one-dimensional mobility models", "abstract": "optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models have been established in [23], where we showed that the optimal trade-offs can be achieved using rate-less codes when the required delay guarantees are sufficient large. in this paper, we extend the results to other mobility models including two-dimensional hybrid random walk model, one-dimensional i.i.d. mobility model and one-dimensional hybrid random walk model. we consider both fast mobiles and slow mobiles, and establish the optimal delay-throughput trade-offs under some conditions. joint coding-scheduling algorithms are also proposed to achieve the optimal trade-offs.", "date_create": "2007-05-02", "area": "cs.ni cs.it math.it", "authors": ["ying", "srikant"]}, {"idpaper": "0705.0326", "title": "optimal delay-throughput trade-offs in mobile ad-hoc networks: hybrid   random walk and one-dimensional mobility models", "abstract": "optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models have been established in [23], where we showed that the optimal trade-offs can be achieved using rate-less codes when the required delay guarantees are sufficient large. in this paper, we extend the results to other mobility models including two-dimensional hybrid random walk model, one-dimensional i.i.d. mobility model and one-dimensional hybrid random walk model. we consider both fast mobiles and slow mobiles, and establish the optimal delay-throughput trade-offs under some conditions. joint coding-scheduling algorithms are also proposed to achieve the optimal trade-offs.", "date_create": "2007-05-02", "area": "cs.ni cs.it math.it", "authors": ["ying", "srikant"]}, {"idpaper": "0705.0422", "title": "frugal colouring of graphs", "abstract": "a $k$-frugal colouring of a graph $g$ is a proper colouring of the vertices of $g$ such that no colour appears more than $k$ times in the neighbourhood of a vertex. this type of colouring was introduced by hind, molloy and reed in 1997. in this paper, we study the frugal chromatic number of planar graphs, planar graphs with large girth, and outerplanar graphs, and relate this parameter with several well-studied colourings, such as colouring of the square, cyclic colouring, and $l(p,q)$-labelling. we also study frugal edge-colourings of multigraphs.", "date_create": "2007-05-03", "area": "cs.dm cs.ni", "authors": ["amini", "esperet", "heuvel"]}, {"idpaper": "0705.0453", "title": "ocb: a generic benchmark to evaluate the performances of object-oriented   database systems", "abstract": "we present in this paper a generic object-oriented benchmark (the object clustering benchmark) that has been designed to evaluate the performances of clustering policies in object-oriented databases. ocb is generic because its sample database may be customized to fit the databases introduced by the main existing benchmarks (e.g., oo1). ocb's current form is clustering-oriented because of its clustering-oriented workload, but it can be easily adapted to other purposes. lastly, ocb's code is compact and easily portable. ocb has been implemented in a real system (texas, running on a sun workstation), in order to test a specific clustering policy called dstc. a few results concerning this test are presented.", "date_create": "2007-05-03", "area": "cs.db", "authors": ["darmont", "petit", "schneider"]}, {"idpaper": "0705.0453", "title": "ocb: a generic benchmark to evaluate the performances of object-oriented   database systems", "abstract": "we present in this paper a generic object-oriented benchmark (the object clustering benchmark) that has been designed to evaluate the performances of clustering policies in object-oriented databases. ocb is generic because its sample database may be customized to fit the databases introduced by the main existing benchmarks (e.g., oo1). ocb's current form is clustering-oriented because of its clustering-oriented workload, but it can be easily adapted to other purposes. lastly, ocb's code is compact and easily portable. ocb has been implemented in a real system (texas, running on a sun workstation), in order to test a specific clustering policy called dstc. a few results concerning this test are presented.", "date_create": "2007-05-03", "area": "cs.db", "authors": ["darmont", "petit", "schneider"]}, {"idpaper": "0705.0552", "title": "succinct indexable dictionaries with applications to encoding $k$-ary   trees, prefix sums and multisets", "abstract": "we consider the {\\it indexable dictionary} problem, which consists of storing a set $s \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the operations of $\\rank(x)$, which returns the number of elements in $s$ that are less than $x$ if $x \\in s$, and -1 otherwise; and $\\select(i)$ which returns the $i$-th smallest element in $s$. we give a data structure that supports both operations in o(1) time on the ram model and requires ${\\cal b}(n,m) + o(n) + o(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal b}(n,m) = \\ceil{\\lg {m \\choose n}}$ is the minimum number of bits required to store any $n$-element subset from a universe of size $m$. previous dictionaries taking this space only supported (yes/no) membership queries in o(1) time. in the cell probe model we can remove the $o(\\lg \\lg m)$ additive term in the space bound, answering a question raised by fich and miltersen, and pagh.   we present extensions and applications of our indexable dictionary data structure, including:   an information-theoretically optimal representation of a $k$-ary cardinal tree that supports standard operations in constant time,   a representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal b}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\rank$ and $\\select$ operations in constant time, and   a representation of a sequence of $n$ non-negative integers summing up to $m$ in ${\\cal b}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant time.", "date_create": "2007-05-04", "area": "cs.ds cs.dm cs.it math.it", "authors": ["raman", "raman", "satti"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0564", "title": "rate bounds for mimo relay channels", "abstract": "this paper considers the multi-input multi-output (mimo) relay channel where multiple antennas are employed by each terminal. compared to single-input single-output (siso) relay channels, mimo relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. in this paper, a partial cooperation strategy that combines transmit-side message splitting and block-markov encoding is presented. lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for gaussian mimo relay channels.", "date_create": "2007-05-04", "area": "cs.it math.it", "authors": ["lo", "vishwanath", "heath"]}, {"idpaper": "0705.0599", "title": "nodetrix: hybrid representation for analyzing social networks", "abstract": "the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. to address this problem, we present nodetrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. a key contribution is a set of interaction techniques. these allow analysts to create a nodetrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the nodetrix representation to explore the dataset, and create meaningful summary visualizations of their findings. finally, we present a case study applying nodetrix to the analysis of the infovis 2004 coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results.", "date_create": "2007-05-04", "area": "cs.hc", "authors": ["henry", "fekete", "mcguffin"]}, {"idpaper": "0705.0599", "title": "nodetrix: hybrid representation for analyzing social networks", "abstract": "the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. to address this problem, we present nodetrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. a key contribution is a set of interaction techniques. these allow analysts to create a nodetrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the nodetrix representation to explore the dataset, and create meaningful summary visualizations of their findings. finally, we present a case study applying nodetrix to the analysis of the infovis 2004 coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results.", "date_create": "2007-05-04", "area": "cs.hc", "authors": ["henry", "fekete", "mcguffin"]}, {"idpaper": "0705.0599", "title": "nodetrix: hybrid representation for analyzing social networks", "abstract": "the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. to address this problem, we present nodetrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. a key contribution is a set of interaction techniques. these allow analysts to create a nodetrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the nodetrix representation to explore the dataset, and create meaningful summary visualizations of their findings. finally, we present a case study applying nodetrix to the analysis of the infovis 2004 coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results.", "date_create": "2007-05-04", "area": "cs.hc", "authors": ["henry", "fekete", "mcguffin"]}, {"idpaper": "0705.0599", "title": "nodetrix: hybrid representation for analyzing social networks", "abstract": "the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. to address this problem, we present nodetrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. a key contribution is a set of interaction techniques. these allow analysts to create a nodetrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the nodetrix representation to explore the dataset, and create meaningful summary visualizations of their findings. finally, we present a case study applying nodetrix to the analysis of the infovis 2004 coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results.", "date_create": "2007-05-04", "area": "cs.hc", "authors": ["henry", "fekete", "mcguffin"]}, {"idpaper": "0705.0602", "title": "risk assessment algorithms based on recursive neural networks", "abstract": "the assessment of highly-risky situations at road intersections have been recently revealed as an important research topic within the context of the automotive industry. in this paper we shall introduce a novel approach to compute risk functions by using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure. specifically, the elements of information either static or dynamic that appear in a road intersection scene are encoded by using directed positional acyclic labeled graphs. the risk assessment problem is then reformulated in terms of an inductive learning task carried out by a recursive neural network. recursive neural networks are connectionist models capable of solving supervised and non-supervised learning problems represented by directed ordered acyclic graphs. the potential of this novel approach is demonstrated through well predefined scenarios. the major difference of our approach compared to others is expressed by the fact of learning the structure of the risk. furthermore, the combination of a rich information encoding procedure with a generalized model of dynamical recurrent networks permit us, as we shall demonstrate, a sophisticated processing of information that we believe as being a first step for building future advanced intersection safety systems", "date_create": "2007-05-04", "area": "cs.ne", "authors": ["de lara", "parent"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0635", "title": "moving walkways, escalators, and elevators", "abstract": "we study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. this elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. the travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   we give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", "date_create": "2007-05-04", "area": "cs.cg", "authors": ["cardinal", "collette", "hurtado", "langerman", "palop"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0738", "title": "the optimization of a novel prismatic drive", "abstract": "the design of a mechanical transmission taking into account the transmitted forces is reported in this paper. this transmission is based on slide-o-cam, a cam mechanism with multiple rollers mounted on a common translating follower. the design of slide-o-cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. the pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. to assess the transmission capability of the mechanism, the hertz formula is introduced to calculate the stresses on the rollers and on the cams. the final transmission is intended to replace the current ball-screws in the orthoglide, a three-dof parallel robot for the production of translational motions, currently under development for machining applications at ecole centrale de nantes.", "date_create": "2007-05-05", "area": "cs.ro", "authors": ["chablat", "caro", "bouyer"]}, {"idpaper": "0705.0742", "title": "mimo detection employing markov chain monte carlo", "abstract": "we propose a soft-output detection scheme for multiple-input-multiple-output (mimo) systems. the detector employs markov chain monte carlo method to compute bit reliabilities from the signals received and is thus suited for coded mimo systems. it offers a good trade-off between achievable performance and algorithmic complexity.", "date_create": "2007-05-05", "area": "cs.gl", "authors": ["sundaram", "murthy"]}, {"idpaper": "0705.0856", "title": "the multiobjective optimization of a prismatic drive", "abstract": "the multiobjective optimization of slide-o-cam is reported in this paper. slide-o-cam is a cam mechanism with multiple rollers mounted on a common translating follower. this transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. a pareto frontier is obtained by means of multiobjective optimization. this optimization is based on three objective functions: (i) the pressure angle, which is a suitable performance index for the transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame; (ii) the hertz pressure used to evaluate the stresses produced on the contact surface between cam and roller; and (iii) the size of the mechanism, characterized by the number of cams and their width.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["bouyer", "caro", "chablat", "angeles"]}, {"idpaper": "0705.0915", "title": "satisfiability parsimoniously reduces to the tantrix(tm) rotation puzzle   problem", "abstract": "holzer and holzer (discrete applied mathematics 144(3):345--358, 2004) proved that the tantrix(tm) rotation puzzle problem is np-complete. they also showed that for infinite rotation puzzles, this problem becomes undecidable. we study the counting version and the unique version of this problem. we prove that the satisfiability problem parsimoniously reduces to the tantrix(tm) rotation puzzle problem. in particular, this reduction preserves the uniqueness of the solution, which implies that the unique tantrix(tm) rotation puzzle problem is as hard as the unique satisfiability problem, and so is dp-complete under polynomial-time randomized reductions, where dp is the second level of the boolean hierarchy over np.", "date_create": "2007-05-07", "area": "cs.cc", "authors": ["baumeister", "rothe"]}, {"idpaper": "0705.0915", "title": "satisfiability parsimoniously reduces to the tantrix(tm) rotation puzzle   problem", "abstract": "holzer and holzer (discrete applied mathematics 144(3):345--358, 2004) proved that the tantrix(tm) rotation puzzle problem is np-complete. they also showed that for infinite rotation puzzles, this problem becomes undecidable. we study the counting version and the unique version of this problem. we prove that the satisfiability problem parsimoniously reduces to the tantrix(tm) rotation puzzle problem. in particular, this reduction preserves the uniqueness of the solution, which implies that the unique tantrix(tm) rotation puzzle problem is as hard as the unique satisfiability problem, and so is dp-complete under polynomial-time randomized reductions, where dp is the second level of the boolean hierarchy over np.", "date_create": "2007-05-07", "area": "cs.cc", "authors": ["baumeister", "rothe"]}, {"idpaper": "0705.0915", "title": "satisfiability parsimoniously reduces to the tantrix(tm) rotation puzzle   problem", "abstract": "holzer and holzer (discrete applied mathematics 144(3):345--358, 2004) proved that the tantrix(tm) rotation puzzle problem is np-complete. they also showed that for infinite rotation puzzles, this problem becomes undecidable. we study the counting version and the unique version of this problem. we prove that the satisfiability problem parsimoniously reduces to the tantrix(tm) rotation puzzle problem. in particular, this reduction preserves the uniqueness of the solution, which implies that the unique tantrix(tm) rotation puzzle problem is as hard as the unique satisfiability problem, and so is dp-complete under polynomial-time randomized reductions, where dp is the second level of the boolean hierarchy over np.", "date_create": "2007-05-07", "area": "cs.cc", "authors": ["baumeister", "rothe"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.0959", "title": "the kinematic analysis of a symmetrical three-degree-of-freedom planar   parallel manipulator", "abstract": "presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. in opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. this property produces more complicated kinematic models but allows more flexibility in trajectory planning. to take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. the aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", "date_create": "2007-05-07", "area": "cs.ro", "authors": ["chablat", "wenger"]}, {"idpaper": "0705.1013", "title": "tracking user attention in collaborative tagging communities", "abstract": "collaborative tagging has recently attracted the attention of both industry and academia due to the popularity of content-sharing systems such as citeulike, del.icio.us, and flickr. these systems give users the opportunity to add data items and to attach their own metadata (or tags) to stored data. the result is an effective content management tool for individual users. recent studies, however, suggest that, as tagging communities grow, the added content and the metadata become harder to manage due to an ease in content diversity. thus, mechanisms that cope with increase of diversity are fundamental to improve the scalability and usability of collaborative tagging systems. this paper analyzes whether usage patterns can be harnessed to improve navigability in a growing knowledge space. to this end, it presents a characterization of two collaborative tagging communities that target scientific literature: citeulike and bibsonomy. we explore three main directions: first, we analyze the tagging activity distribution across the user population. second, we define new metrics for similarity in user interest and use these metrics to uncover the structure of the tagging communities we study. the structure we uncover suggests a clear segmentation of interests into a large number of individuals with unique preferences and a core set of users with interspersed interests. finally, we offer preliminary results that demonstrate that the interest-based structure of the tagging community can be used to facilitate content usage as communities scale.", "date_create": "2007-05-07", "area": "cs.dl cs.cy", "authors": ["santos-neto", "ripeanu", "iamnitchi"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1033", "title": "optimal cache-oblivious mesh layouts", "abstract": "a mesh is a graph that divides physical space into regularly-shaped regions. meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. in one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. the performance of a mesh update depends on the layout of the mesh in memory.   this paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. such a memory layout is called cache-oblivious. formally, for a $d$-dimensional mesh $g$, block size $b$, and cache size $m$ (where $m=\\omega(b^d)$), the mesh update of $g$ uses $o(1+|g|/b)$ memory transfers. the paper also shows how the mesh-update performance degrades for smaller caches, where $m=o(b^d)$.   the paper then gives two algorithms for finding cache-oblivious mesh layouts. the first layout algorithm runs in time $o(|g|\\log^2|g|)$ both in expectation and with high probability on a ram. it uses $o(1+|g|\\log^2(|g|/m)/b)$ memory transfers in expectation and $o(1+(|g|/b)(\\log^2(|g|/m) + \\log|g|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (dam) models. the layout is obtained by finding a fully balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree. the second algorithm runs faster by almost a $\\log|g|/\\log\\log|g|$ factor in all three memory models, both in expectation and with high probability. the layout obtained by finding a relax-balanced decomposition tree of $g$ and then performing an in-order traversal of the leaves of the tree.", "date_create": "2007-05-08", "area": "cs.ds cs.ce cs.ms cs.na", "authors": ["bender", "kuszmaul", "teng", "wang"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1151", "title": "achievable rates and optimal resource allocation for imperfectly-known   fading relay channels", "abstract": "in this paper, achievable rates of imperfectly-known fading relay channels are studied. it is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. in the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. the achievable rate expressions are then employed to identify the optimal resource allocation strategies.", "date_create": "2007-05-08", "area": "cs.it math.it", "authors": ["zhang", "gursoy"]}, {"idpaper": "0705.1218", "title": "calibration of quasi-isotropic parallel kinematic machines: orthoglide", "abstract": "the paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the orthoglide family. it is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. they are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the tcp along the cartesian axes. using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. validity of the proposed calibration technique is confirmed by the experimental results.", "date_create": "2007-05-09", "area": "cs.ro", "authors": ["pashkevich", "gomolitsky", "wenger", "chablat"]}, {"idpaper": "0705.1227", "title": "rate adaptation for cognitive radio under interference from primary   spectrum user", "abstract": "a cognitive radio can operate as a secondary system in a given spectrum. this operation should use limited power in order not to disturb the communication by primary spectrum user. under such conditions, in this paper we investigate how to maximize the spectral efficiency in the secondary system. a secondary receiver observes a multiple access channel of two users, the secondary and the primary transmitter, respectively. we show that, for spectrally-efficient operation, the secondary system should apply opportunistic interference cancellation (oic). with oic, the secondary system decodes the primary signal when such an opportunity is created by the primary rate and the power received from the primary system. for such an operation, we derive the achievable data rate in the secondary system. when the primary signal is decodable, we devise a method, based on superposition coding, by which the secondary system can achieve the maximal possible rate. finally, we investigate the power allocation in the secondary system when multiple channels are used. we show that the optimal power allocation with oic can be achieved through intercepted water-filling instead of the conventional water-filling. the results show a significant gain for the rate achieved through an opportunistic interference cancellation.", "date_create": "2007-05-09", "area": "cs.it cs.ni math.it", "authors": ["popovski", "yomo", "nishimori", "di taranto"]}, {"idpaper": "0705.1284", "title": "workspace analysis of the orthoglide using interval analysis", "abstract": "this paper addresses the workspace analysis of the orthoglide, a 3-dof parallel mechanism designed for machining applications. this machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the cartesian x-y-z space with fixed orientation. the workspace analysis is conducted on the bases of prescribed kinetostatic performances. the interesting features of the orthoglide are a regular cartesian workspace shape, uniform performances in all directions and good compactness. interval analysis based methods for computing the dextrous workspace and the largest cube enclosed in this workspace are presented.", "date_create": "2007-05-09", "area": "cs.ro", "authors": ["chablat", "wenger", "merlet"]}, {"idpaper": "0705.1364", "title": "an approximation algorithm for shortest descending paths", "abstract": "a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. no efficient algorithm is known to find a shortest descending path (sdp) from s to t in a polyhedral terrain. we give a simple approximation algorithm that solves the sdp problem on general terrains. our algorithm discretizes the terrain with o(n^2 x / e) steiner points so that after an o(n^2 x / e * log(n x /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate sdp from s to any point v in o(n) time if v is either a vertex of the terrain or a steiner point, and in o(n x /e) time otherwise. here n is the size of the terrain, and x is a parameter of the geometry of the terrain.", "date_create": "2007-05-09", "area": "cs.cg cs.ds", "authors": ["ahmed", "lubiw"]}, {"idpaper": "0705.1364", "title": "an approximation algorithm for shortest descending paths", "abstract": "a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. no efficient algorithm is known to find a shortest descending path (sdp) from s to t in a polyhedral terrain. we give a simple approximation algorithm that solves the sdp problem on general terrains. our algorithm discretizes the terrain with o(n^2 x / e) steiner points so that after an o(n^2 x / e * log(n x /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate sdp from s to any point v in o(n) time if v is either a vertex of the terrain or a steiner point, and in o(n x /e) time otherwise. here n is the size of the terrain, and x is a parameter of the geometry of the terrain.", "date_create": "2007-05-09", "area": "cs.cg cs.ds", "authors": ["ahmed", "lubiw"]}, {"idpaper": "0705.1364", "title": "an approximation algorithm for shortest descending paths", "abstract": "a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. no efficient algorithm is known to find a shortest descending path (sdp) from s to t in a polyhedral terrain. we give a simple approximation algorithm that solves the sdp problem on general terrains. our algorithm discretizes the terrain with o(n^2 x / e) steiner points so that after an o(n^2 x / e * log(n x /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate sdp from s to any point v in o(n) time if v is either a vertex of the terrain or a steiner point, and in o(n x /e) time otherwise. here n is the size of the terrain, and x is a parameter of the geometry of the terrain.", "date_create": "2007-05-09", "area": "cs.cg cs.ds", "authors": ["ahmed", "lubiw"]}, {"idpaper": "0705.1364", "title": "an approximation algorithm for shortest descending paths", "abstract": "a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. no efficient algorithm is known to find a shortest descending path (sdp) from s to t in a polyhedral terrain. we give a simple approximation algorithm that solves the sdp problem on general terrains. our algorithm discretizes the terrain with o(n^2 x / e) steiner points so that after an o(n^2 x / e * log(n x /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate sdp from s to any point v in o(n) time if v is either a vertex of the terrain or a steiner point, and in o(n x /e) time otherwise. here n is the size of the terrain, and x is a parameter of the geometry of the terrain.", "date_create": "2007-05-09", "area": "cs.cg cs.ds", "authors": ["ahmed", "lubiw"]}, {"idpaper": "0705.1364", "title": "an approximation algorithm for shortest descending paths", "abstract": "a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. no efficient algorithm is known to find a shortest descending path (sdp) from s to t in a polyhedral terrain. we give a simple approximation algorithm that solves the sdp problem on general terrains. our algorithm discretizes the terrain with o(n^2 x / e) steiner points so that after an o(n^2 x / e * log(n x /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate sdp from s to any point v in o(n) time if v is either a vertex of the terrain or a steiner point, and in o(n x /e) time otherwise. here n is the size of the terrain, and x is a parameter of the geometry of the terrain.", "date_create": "2007-05-09", "area": "cs.cg cs.ds", "authors": ["ahmed", "lubiw"]}, {"idpaper": "0705.1409", "title": "singularity surfaces and maximal singularity-free boxes in the joint   space of planar 3-rpr parallel manipulators", "abstract": "in this paper, a method to compute joint space singularity surfaces of 3-rpr planar parallel manipulators is first presented. then, a procedure to determine maximal joint space singularity-free boxes is introduced. numerical examples are given in order to illustrate graphically the results. this study is of high interest for planning trajectories in the joint space of 3-rpr parallel manipulators and for manipulators design as it may constitute a tool for choosing appropriate joint limits and thus for sizing the link lengths of the manipulator.", "date_create": "2007-05-10", "area": "cs.ro", "authors": ["zein", "wenger", "chablat"]}, {"idpaper": "0705.1409", "title": "singularity surfaces and maximal singularity-free boxes in the joint   space of planar 3-rpr parallel manipulators", "abstract": "in this paper, a method to compute joint space singularity surfaces of 3-rpr planar parallel manipulators is first presented. then, a procedure to determine maximal joint space singularity-free boxes is introduced. numerical examples are given in order to illustrate graphically the results. this study is of high interest for planning trajectories in the joint space of 3-rpr parallel manipulators and for manipulators design as it may constitute a tool for choosing appropriate joint limits and thus for sizing the link lengths of the manipulator.", "date_create": "2007-05-10", "area": "cs.ro", "authors": ["zein", "wenger", "chablat"]}, {"idpaper": "0705.1541", "title": "unfolding manhattan towers", "abstract": "we provide an algorithm for unfolding the surface of any orthogonal polyhedron that falls into a particular shape class we call manhattan towers, to a nonoverlapping planar orthogonal polygon. the algorithm cuts along edges of a 4x5x1 refinement of the vertex grid.", "date_create": "2007-05-10", "area": "cs.cg cs.dm", "authors": ["damian", "flatland", "o'rourke"]}, {"idpaper": "0705.1541", "title": "unfolding manhattan towers", "abstract": "we provide an algorithm for unfolding the surface of any orthogonal polyhedron that falls into a particular shape class we call manhattan towers, to a nonoverlapping planar orthogonal polygon. the algorithm cuts along edges of a 4x5x1 refinement of the vertex grid.", "date_create": "2007-05-10", "area": "cs.cg cs.dm", "authors": ["damian", "flatland", "o'rourke"]}, {"idpaper": "0705.1541", "title": "unfolding manhattan towers", "abstract": "we provide an algorithm for unfolding the surface of any orthogonal polyhedron that falls into a particular shape class we call manhattan towers, to a nonoverlapping planar orthogonal polygon. the algorithm cuts along edges of a 4x5x1 refinement of the vertex grid.", "date_create": "2007-05-10", "area": "cs.cg cs.dm", "authors": ["damian", "flatland", "o'rourke"]}, {"idpaper": "0705.1583", "title": "wireless networking to support data and voice communication using spread   spectrum technology in the physical layer", "abstract": "wireless networking is rapidly growing and becomes an inexpensive technology which allows multiple users to simultaneously access the network and the internet while roaming about the campus. in the present work, the software development of a wireless lan(wlan) is highlighted. this wlan utilizes direct sequence spread spectrum (dsss) technology at 902mhz rf carrier frequency in its physical layer. cost effective installation and antijaming property of spread spectrum technology are the major advantages of this work.", "date_create": "2007-05-11", "area": "cs.ni", "authors": ["dhar", "bera"]}, {"idpaper": "0705.1583", "title": "wireless networking to support data and voice communication using spread   spectrum technology in the physical layer", "abstract": "wireless networking is rapidly growing and becomes an inexpensive technology which allows multiple users to simultaneously access the network and the internet while roaming about the campus. in the present work, the software development of a wireless lan(wlan) is highlighted. this wlan utilizes direct sequence spread spectrum (dsss) technology at 902mhz rf carrier frequency in its physical layer. cost effective installation and antijaming property of spread spectrum technology are the major advantages of this work.", "date_create": "2007-05-11", "area": "cs.ni", "authors": ["dhar", "bera"]}, {"idpaper": "0705.1583", "title": "wireless networking to support data and voice communication using spread   spectrum technology in the physical layer", "abstract": "wireless networking is rapidly growing and becomes an inexpensive technology which allows multiple users to simultaneously access the network and the internet while roaming about the campus. in the present work, the software development of a wireless lan(wlan) is highlighted. this wlan utilizes direct sequence spread spectrum (dsss) technology at 902mhz rf carrier frequency in its physical layer. cost effective installation and antijaming property of spread spectrum technology are the major advantages of this work.", "date_create": "2007-05-11", "area": "cs.ni", "authors": ["dhar", "bera"]}, {"idpaper": "0705.1583", "title": "wireless networking to support data and voice communication using spread   spectrum technology in the physical layer", "abstract": "wireless networking is rapidly growing and becomes an inexpensive technology which allows multiple users to simultaneously access the network and the internet while roaming about the campus. in the present work, the software development of a wireless lan(wlan) is highlighted. this wlan utilizes direct sequence spread spectrum (dsss) technology at 902mhz rf carrier frequency in its physical layer. cost effective installation and antijaming property of spread spectrum technology are the major advantages of this work.", "date_create": "2007-05-11", "area": "cs.ni", "authors": ["dhar", "bera"]}, {"idpaper": "0705.1612", "title": "a class of ldpc erasure distributions with closed-form threshold   expression", "abstract": "in this paper, a family of low-density parity-check (ldpc) degree distributions, whose decoding threshold on the binary erasure channel (bec) admits a simple closed form, is presented. these degree distributions are a subset of the check regular distributions (i.e. all the check nodes have the same degree), and are referred to as $p$-positive distributions. it is given proof that the threshold for a $p$-positive distribution is simply expressed by $[\\lambda'(0)\\rho'(1)]^{-1}$. besides this closed form threshold expression, the $p$-positive distributions exhibit three additional properties. first, for given code rate, check degree and maximum variable degree, they are in some cases characterized by a threshold which is extremely close to that of the best known check regular distributions, under the same set of constraints. second, the threshold optimization problem within the $p$-positive class can be solved in some cases with analytic methods, without using any numerical optimization tool. third, these distributions can achieve the bec capacity. the last property is shown by proving that the well-known binomial degree distributions belong to the $p$-positive family.", "date_create": "2007-05-11", "area": "cs.it math.it", "authors": ["paolini", "chiani"]}, {"idpaper": "0705.1672", "title": "principal component analysis and automatic relevance determination in   damage identification", "abstract": "this paper compares two neural network input selection schemes, the principal component analysis (pca) and the automatic relevance determination (ard) based on mac-kay's evidence framework. the pca takes all the input data and projects it onto a lower dimension space, thereby reduc-ing the dimension of the input space. this input reduction method often results with parameters that have significant influence on the dynamics of the data being diluted by those that do not influence the dynamics of the data. the ard selects the most relevant input parameters and discards those that do not contribute significantly to the dynamics of the data being modelled. the ard sometimes results with important input parameters being discarded thereby compromising the dynamics of the data. the pca and ard methods are implemented together with a multi-layer-perceptron (mlp) network for fault identification in structures and the performance of the two methods is as-sessed. it is observed that ard and pca give similar accu-racy levels when used as input-selection schemes. there-fore, the choice of input-selection scheme is dependent on the nature of the data being processed.", "date_create": "2007-05-11", "area": "cs.ce", "authors": ["mdlazi", "marwala", "stander", "scheffer", "heyns"]}, {"idpaper": "0705.1789", "title": "random linear network coding: a free cipher?", "abstract": "we consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). for this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. a preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", "date_create": "2007-05-12", "area": "cs.it cs.cr math.it", "authors": ["lima", "m\u00e9dard", "barros"]}, {"idpaper": "0705.1789", "title": "random linear network coding: a free cipher?", "abstract": "we consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). for this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. a preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", "date_create": "2007-05-12", "area": "cs.it cs.cr math.it", "authors": ["lima", "m\u00e9dard", "barros"]}, {"idpaper": "0705.1789", "title": "random linear network coding: a free cipher?", "abstract": "we consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). for this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. a preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", "date_create": "2007-05-12", "area": "cs.it cs.cr math.it", "authors": ["lima", "m\u00e9dard", "barros"]}, {"idpaper": "0705.1789", "title": "random linear network coding: a free cipher?", "abstract": "we consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). for this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. a preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", "date_create": "2007-05-12", "area": "cs.it cs.cr math.it", "authors": ["lima", "m\u00e9dard", "barros"]}, {"idpaper": "0705.1789", "title": "random linear network coding: a free cipher?", "abstract": "we consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). for this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. a preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", "date_create": "2007-05-12", "area": "cs.it cs.cr math.it", "authors": ["lima", "m\u00e9dard", "barros"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1925", "title": "double sided watermark embedding and detection with perceptual analysis", "abstract": "in our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", "date_create": "2007-05-14", "area": "cs.mm cs.cr", "authors": ["zhong", "huang"]}, {"idpaper": "0705.1939", "title": "towards informative statistical flow inversion", "abstract": "a problem which has recently attracted research attention is that of estimating the distribution of flow sizes in internet traffic. on high traffic links it is sometimes impossible to record every packet. researchers have approached the problem of estimating flow lengths from sampled packet data in two separate ways. firstly, different sampling methodologies can be tried to more accurately measure the desired system parameters. one such method is the sample-and-hold method where, if a packet is sampled, all subsequent packets in that flow are sampled. secondly, statistical methods can be used to ``invert'' the sampled data and produce an estimate of flow lengths from a sample.   in this paper we propose, implement and test two variants on the sample-and-hold method. in addition we show how the sample-and-hold method can be inverted to get an estimation of the genuine distribution of flow sizes. experiments are carried out on real network traces to compare standard packet sampling with three variants of sample-and-hold. the methods are compared for their ability to reconstruct the genuine distribution of flow sizes in the traffic.", "date_create": "2007-05-14", "area": "cs.ni cs.pf", "authors": ["clegg", "haddadi", "landa", "rio"]}, {"idpaper": "0705.1939", "title": "towards informative statistical flow inversion", "abstract": "a problem which has recently attracted research attention is that of estimating the distribution of flow sizes in internet traffic. on high traffic links it is sometimes impossible to record every packet. researchers have approached the problem of estimating flow lengths from sampled packet data in two separate ways. firstly, different sampling methodologies can be tried to more accurately measure the desired system parameters. one such method is the sample-and-hold method where, if a packet is sampled, all subsequent packets in that flow are sampled. secondly, statistical methods can be used to ``invert'' the sampled data and produce an estimate of flow lengths from a sample.   in this paper we propose, implement and test two variants on the sample-and-hold method. in addition we show how the sample-and-hold method can be inverted to get an estimation of the genuine distribution of flow sizes. experiments are carried out on real network traces to compare standard packet sampling with three variants of sample-and-hold. the methods are compared for their ability to reconstruct the genuine distribution of flow sizes in the traffic.", "date_create": "2007-05-14", "area": "cs.ni cs.pf", "authors": ["clegg", "haddadi", "landa", "rio"]}, {"idpaper": "0705.2009", "title": "bit-interleaved coded multiple beamforming with imperfect csit", "abstract": "this paper addresses the performance of bit-interleaved coded multiple beamforming (bicmb) [1], [2] with imperfect knowledge of beamforming vectors. most studies for limited-rate channel state information at the transmitter (csit) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. in bicmb, this property does not hold. on the other hand, the optimum precoder and detector for bicmb are invariant under a diagonal unitary transform. in order to design a limited-rate csit system for bicmb, we propose a new distortion measure optimum under this invariance. based on this new distortion measure, we introduce a new set of centroids and employ the generalized lloyd algorithm for codebook design. we provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. we show that although these receivers have the same performance for perfect csit, their performance varies under imperfect csit.", "date_create": "2007-05-14", "area": "cs.it math.it", "authors": ["sengul", "park", "ayanoglu"]}, {"idpaper": "0705.2009", "title": "bit-interleaved coded multiple beamforming with imperfect csit", "abstract": "this paper addresses the performance of bit-interleaved coded multiple beamforming (bicmb) [1], [2] with imperfect knowledge of beamforming vectors. most studies for limited-rate channel state information at the transmitter (csit) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. in bicmb, this property does not hold. on the other hand, the optimum precoder and detector for bicmb are invariant under a diagonal unitary transform. in order to design a limited-rate csit system for bicmb, we propose a new distortion measure optimum under this invariance. based on this new distortion measure, we introduce a new set of centroids and employ the generalized lloyd algorithm for codebook design. we provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. we show that although these receivers have the same performance for perfect csit, their performance varies under imperfect csit.", "date_create": "2007-05-14", "area": "cs.it math.it", "authors": ["sengul", "park", "ayanoglu"]}, {"idpaper": "0705.2009", "title": "bit-interleaved coded multiple beamforming with imperfect csit", "abstract": "this paper addresses the performance of bit-interleaved coded multiple beamforming (bicmb) [1], [2] with imperfect knowledge of beamforming vectors. most studies for limited-rate channel state information at the transmitter (csit) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. in bicmb, this property does not hold. on the other hand, the optimum precoder and detector for bicmb are invariant under a diagonal unitary transform. in order to design a limited-rate csit system for bicmb, we propose a new distortion measure optimum under this invariance. based on this new distortion measure, we introduce a new set of centroids and employ the generalized lloyd algorithm for codebook design. we provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. we show that although these receivers have the same performance for perfect csit, their performance varies under imperfect csit.", "date_create": "2007-05-14", "area": "cs.it math.it", "authors": ["sengul", "park", "ayanoglu"]}, {"idpaper": "0705.2009", "title": "bit-interleaved coded multiple beamforming with imperfect csit", "abstract": "this paper addresses the performance of bit-interleaved coded multiple beamforming (bicmb) [1], [2] with imperfect knowledge of beamforming vectors. most studies for limited-rate channel state information at the transmitter (csit) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. in bicmb, this property does not hold. on the other hand, the optimum precoder and detector for bicmb are invariant under a diagonal unitary transform. in order to design a limited-rate csit system for bicmb, we propose a new distortion measure optimum under this invariance. based on this new distortion measure, we introduce a new set of centroids and employ the generalized lloyd algorithm for codebook design. we provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. we show that although these receivers have the same performance for perfect csit, their performance varies under imperfect csit.", "date_create": "2007-05-14", "area": "cs.it math.it", "authors": ["sengul", "park", "ayanoglu"]}, {"idpaper": "0705.2011", "title": "multi-dimensional recurrent neural networks", "abstract": "recurrent neural networks (rnns) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. some of the properties that make rnns suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. however, there has so far been no direct way of applying rnns to data with more than one spatio-temporal dimension. this paper introduces multi-dimensional recurrent neural networks (mdrnns), thereby extending the potential applicability of rnns to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. experimental results are provided for two image segmentation tasks.", "date_create": "2007-05-14", "area": "cs.ai cs.cv", "authors": ["graves", "fernandez", "schmidhuber"]}, {"idpaper": "0705.2011", "title": "multi-dimensional recurrent neural networks", "abstract": "recurrent neural networks (rnns) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. some of the properties that make rnns suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. however, there has so far been no direct way of applying rnns to data with more than one spatio-temporal dimension. this paper introduces multi-dimensional recurrent neural networks (mdrnns), thereby extending the potential applicability of rnns to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. experimental results are provided for two image segmentation tasks.", "date_create": "2007-05-14", "area": "cs.ai cs.cv", "authors": ["graves", "fernandez", "schmidhuber"]}, {"idpaper": "0705.2011", "title": "multi-dimensional recurrent neural networks", "abstract": "recurrent neural networks (rnns) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. some of the properties that make rnns suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. however, there has so far been no direct way of applying rnns to data with more than one spatio-temporal dimension. this paper introduces multi-dimensional recurrent neural networks (mdrnns), thereby extending the potential applicability of rnns to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. experimental results are provided for two image segmentation tasks.", "date_create": "2007-05-14", "area": "cs.ai cs.cv", "authors": ["graves", "fernandez", "schmidhuber"]}, {"idpaper": "0705.2011", "title": "multi-dimensional recurrent neural networks", "abstract": "recurrent neural networks (rnns) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. some of the properties that make rnns suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. however, there has so far been no direct way of applying rnns to data with more than one spatio-temporal dimension. this paper introduces multi-dimensional recurrent neural networks (mdrnns), thereby extending the potential applicability of rnns to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. experimental results are provided for two image segmentation tasks.", "date_create": "2007-05-14", "area": "cs.ai cs.cv", "authors": ["graves", "fernandez", "schmidhuber"]}, {"idpaper": "0705.2235", "title": "response prediction of structural system subject to earthquake motions   using artificial neural network", "abstract": "this paper uses artificial neural network (ann) models to compute response of structural system subject to indian earthquakes at chamoli and uttarkashi ground motion data. the system is first trained for a single real earthquake data. the trained ann architecture is then used to simulate earthquakes with various intensities and it was found that the predicted responses given by ann model are accurate for practical purposes. when the ann is trained by a part of the ground motion data, it can also identify the responses of the structural system well. in this way the safeness of the structural systems may be predicted in case of future earthquakes without waiting for the earthquake to occur for the lessons. time period and the corresponding maximum response of the building for an earthquake has been evaluated, which is again trained to predict the maximum response of the building at different time periods. the trained time period versus maximum response ann model is also tested for real earthquake data of other place, which was not used in the training and was found to be in good agreement.", "date_create": "2007-05-15", "area": "cs.ai", "authors": ["chakraverty", "marwala", "gupta", "tettey"]}, {"idpaper": "0705.2235", "title": "response prediction of structural system subject to earthquake motions   using artificial neural network", "abstract": "this paper uses artificial neural network (ann) models to compute response of structural system subject to indian earthquakes at chamoli and uttarkashi ground motion data. the system is first trained for a single real earthquake data. the trained ann architecture is then used to simulate earthquakes with various intensities and it was found that the predicted responses given by ann model are accurate for practical purposes. when the ann is trained by a part of the ground motion data, it can also identify the responses of the structural system well. in this way the safeness of the structural systems may be predicted in case of future earthquakes without waiting for the earthquake to occur for the lessons. time period and the corresponding maximum response of the building for an earthquake has been evaluated, which is again trained to predict the maximum response of the building at different time periods. the trained time period versus maximum response ann model is also tested for real earthquake data of other place, which was not used in the training and was found to be in good agreement.", "date_create": "2007-05-15", "area": "cs.ai", "authors": ["chakraverty", "marwala", "gupta", "tettey"]}, {"idpaper": "0705.2273", "title": "on the information rate of mimo systems with finite rate channel state   feedback and power on/off strategy", "abstract": "this paper quantifies the information rate of multiple-input multiple-output (mimo) systems with finite rate channel state feedback and power on/off strategy. in power on/off strategy, a beamforming vector (beam) is either turned on (denoted by on-beam) with a constant power or turned off. we prove that the ratio of the optimal number of on-beams and the number of antennas converges to a constant for a given signal-to-noise ratio (snr) when the number of transmit and receive antennas approaches infinity simultaneously and when beamforming is perfect. based on this result, a near optimal strategy, i.e., power on/off strategy with a constant number of on-beams, is discussed. for such a strategy, we propose the power efficiency factor to quantify the effect of imperfect beamforming. a formula is proposed to compute the maximum power efficiency factor achievable given a feedback rate. the information rate of the overall mimo system can be approximated by combining the asymptotic results and the formula for power efficiency factor. simulations show that this approximation is accurate for all snr regimes.", "date_create": "2007-05-15", "area": "cs.it math.it", "authors": ["dai", "liu", "rider", "lau"]}, {"idpaper": "0705.2787", "title": "worst-case background knowledge for privacy-preserving data publishing", "abstract": "recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. however, in practice, the data publisher does not know what background knowledge the attacker possesses. thus, it is important to consider the worst-case. in this paper, we initiate a formal study of worst-case background knowledge. we propose a language that can express any background knowledge about the data. we provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. we also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", "date_create": "2007-05-18", "area": "cs.db", "authors": ["martin", "kifer", "machanavajjhala", "gehrke", "halpern"]}, {"idpaper": "0705.2787", "title": "worst-case background knowledge for privacy-preserving data publishing", "abstract": "recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. however, in practice, the data publisher does not know what background knowledge the attacker possesses. thus, it is important to consider the worst-case. in this paper, we initiate a formal study of worst-case background knowledge. we propose a language that can express any background knowledge about the data. we provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. we also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", "date_create": "2007-05-18", "area": "cs.db", "authors": ["martin", "kifer", "machanavajjhala", "gehrke", "halpern"]}, {"idpaper": "0705.2787", "title": "worst-case background knowledge for privacy-preserving data publishing", "abstract": "recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. however, in practice, the data publisher does not know what background knowledge the attacker possesses. thus, it is important to consider the worst-case. in this paper, we initiate a formal study of worst-case background knowledge. we propose a language that can express any background knowledge about the data. we provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. we also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", "date_create": "2007-05-18", "area": "cs.db", "authors": ["martin", "kifer", "machanavajjhala", "gehrke", "halpern"]}, {"idpaper": "0705.2787", "title": "worst-case background knowledge for privacy-preserving data publishing", "abstract": "recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. however, in practice, the data publisher does not know what background knowledge the attacker possesses. thus, it is important to consider the worst-case. in this paper, we initiate a formal study of worst-case background knowledge. we propose a language that can express any background knowledge about the data. we provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. we also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", "date_create": "2007-05-18", "area": "cs.db", "authors": ["martin", "kifer", "machanavajjhala", "gehrke", "halpern"]}, {"idpaper": "0705.2787", "title": "worst-case background knowledge for privacy-preserving data publishing", "abstract": "recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. however, in practice, the data publisher does not know what background knowledge the attacker possesses. thus, it is important to consider the worst-case. in this paper, we initiate a formal study of worst-case background knowledge. we propose a language that can express any background knowledge about the data. we provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. we also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", "date_create": "2007-05-18", "area": "cs.db", "authors": ["martin", "kifer", "machanavajjhala", "gehrke", "halpern"]}, {"idpaper": "0705.2835", "title": "voronoi diagram of polygonal chains under the discrete fr\\'echet   distance", "abstract": "polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. a well-known measure to characterize the similarity of two polygonal chains is the famous fr\\`{e}chet distance. in this paper, for the first time, we consider the voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete fr\\`{e}chet distance. given $n$ polygonal chains ${\\cal c}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a voronoi diagram {\\em vd}$_f({\\cal c})$ by presenting the first known upper and lower bounds for {\\em vd}$_f({\\cal c})$.", "date_create": "2007-05-19", "area": "cs.cg cs.cc", "authors": ["bereg", "gavrilova", "zhu"]}, {"idpaper": "0705.2835", "title": "voronoi diagram of polygonal chains under the discrete fr\\'echet   distance", "abstract": "polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. a well-known measure to characterize the similarity of two polygonal chains is the famous fr\\`{e}chet distance. in this paper, for the first time, we consider the voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete fr\\`{e}chet distance. given $n$ polygonal chains ${\\cal c}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a voronoi diagram {\\em vd}$_f({\\cal c})$ by presenting the first known upper and lower bounds for {\\em vd}$_f({\\cal c})$.", "date_create": "2007-05-19", "area": "cs.cg cs.cc", "authors": ["bereg", "gavrilova", "zhu"]}, {"idpaper": "0705.2835", "title": "voronoi diagram of polygonal chains under the discrete fr\\'echet   distance", "abstract": "polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. a well-known measure to characterize the similarity of two polygonal chains is the famous fr\\`{e}chet distance. in this paper, for the first time, we consider the voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete fr\\`{e}chet distance. given $n$ polygonal chains ${\\cal c}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a voronoi diagram {\\em vd}$_f({\\cal c})$ by presenting the first known upper and lower bounds for {\\em vd}$_f({\\cal c})$.", "date_create": "2007-05-19", "area": "cs.cg cs.cc", "authors": ["bereg", "gavrilova", "zhu"]}, {"idpaper": "0705.2835", "title": "voronoi diagram of polygonal chains under the discrete fr\\'echet   distance", "abstract": "polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. a well-known measure to characterize the similarity of two polygonal chains is the famous fr\\`{e}chet distance. in this paper, for the first time, we consider the voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete fr\\`{e}chet distance. given $n$ polygonal chains ${\\cal c}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a voronoi diagram {\\em vd}$_f({\\cal c})$ by presenting the first known upper and lower bounds for {\\em vd}$_f({\\cal c})$.", "date_create": "2007-05-19", "area": "cs.cg cs.cc", "authors": ["bereg", "gavrilova", "zhu"]}, {"idpaper": "0705.2847", "title": "capacity of sparse multipath channels in the ultra-wideband regime", "abstract": "this paper studies the ergodic capacity of time- and frequency-selective multipath fading channels in the ultrawideband (uwb) regime when training signals are used for channel estimation at the receiver. motivated by recent measurement results on uwb channels, we propose a model for sparse multipath channels. a key implication of sparsity is that the independent degrees of freedom (dof) in the channel scale sub-linearly with the signal space dimension (product of signaling duration and bandwidth). sparsity is captured by the number of resolvable paths in delay and doppler. our analysis is based on a training and communication scheme that employs signaling over orthogonal short-time fourier (stf) basis functions. stf signaling naturally relates sparsity in delay-doppler to coherence in time-frequency. we study the impact of multipath sparsity on two fundamental metrics of spectral efficiency in the wideband/low-snr limit introduced by verdu: first- and second-order optimality conditions. recent results by zheng et. al. have underscored the large gap in spectral efficiency between coherent and non-coherent extremes and the importance of channel learning in bridging the gap. building on these results, our results lead to the following implications of multipath sparsity: 1) the coherence requirements are shared in both time and frequency, thereby significantly relaxing the required scaling in coherence time with snr; 2) sparse multipath channels are asymptotically coherent -- for a given but large bandwidth, the channel can be learned perfectly and the coherence requirements for first- and second-order optimality met through sufficiently large signaling duration; and 3) the requirement of peaky signals in attaining capacity is eliminated or relaxed in sparse environments.", "date_create": "2007-05-19", "area": "cs.it math.it", "authors": ["raghavan", "hariharan", "sayeed"]}, {"idpaper": "0705.2847", "title": "capacity of sparse multipath channels in the ultra-wideband regime", "abstract": "this paper studies the ergodic capacity of time- and frequency-selective multipath fading channels in the ultrawideband (uwb) regime when training signals are used for channel estimation at the receiver. motivated by recent measurement results on uwb channels, we propose a model for sparse multipath channels. a key implication of sparsity is that the independent degrees of freedom (dof) in the channel scale sub-linearly with the signal space dimension (product of signaling duration and bandwidth). sparsity is captured by the number of resolvable paths in delay and doppler. our analysis is based on a training and communication scheme that employs signaling over orthogonal short-time fourier (stf) basis functions. stf signaling naturally relates sparsity in delay-doppler to coherence in time-frequency. we study the impact of multipath sparsity on two fundamental metrics of spectral efficiency in the wideband/low-snr limit introduced by verdu: first- and second-order optimality conditions. recent results by zheng et. al. have underscored the large gap in spectral efficiency between coherent and non-coherent extremes and the importance of channel learning in bridging the gap. building on these results, our results lead to the following implications of multipath sparsity: 1) the coherence requirements are shared in both time and frequency, thereby significantly relaxing the required scaling in coherence time with snr; 2) sparse multipath channels are asymptotically coherent -- for a given but large bandwidth, the channel can be learned perfectly and the coherence requirements for first- and second-order optimality met through sufficiently large signaling duration; and 3) the requirement of peaky signals in attaining capacity is eliminated or relaxed in sparse environments.", "date_create": "2007-05-19", "area": "cs.it math.it", "authors": ["raghavan", "hariharan", "sayeed"]}, {"idpaper": "0705.2854", "title": "scanning and sequential decision making for multi-dimensional data -   part ii: the noisy case", "abstract": "we consider the problem of sequential decision making on random fields corrupted by noise. in this scenario, the decision maker observes a noisy version of the data, yet judged with respect to the clean data. in particular, we first consider the problem of sequentially scanning and filtering noisy random fields. in this case, the sequential filter is given the freedom to choose the path over which it traverses the random field (e.g., noisy image or video sequence), thus it is natural to ask what is the best achievable performance and how sensitive this performance is to the choice of the scan. we formally define the problem of scanning and filtering, derive a bound on the best achievable performance and quantify the excess loss occurring when non-optimal scanners are used, compared to optimal scanning and filtering.   we then discuss the problem of sequential scanning and prediction of noisy random fields. this setting is a natural model for applications such as restoration and coding of noisy images. we formally define the problem of scanning and prediction of a noisy multidimensional array and relate the optimal performance to the clean scandictability defined by merhav and weissman. moreover, bounds on the excess loss due to sub-optimal scans are derived, and a universal prediction algorithm is suggested.   this paper is the second part of a two-part paper. the first paper dealt with sequential decision making on noiseless data arrays, namely, when the decision maker is judged with respect to the same data array it observes.", "date_create": "2007-05-20", "area": "cs.it cs.cv math.it", "authors": ["cohen", "weissman", "merhav"]}, {"idpaper": "0705.2854", "title": "scanning and sequential decision making for multi-dimensional data -   part ii: the noisy case", "abstract": "we consider the problem of sequential decision making on random fields corrupted by noise. in this scenario, the decision maker observes a noisy version of the data, yet judged with respect to the clean data. in particular, we first consider the problem of sequentially scanning and filtering noisy random fields. in this case, the sequential filter is given the freedom to choose the path over which it traverses the random field (e.g., noisy image or video sequence), thus it is natural to ask what is the best achievable performance and how sensitive this performance is to the choice of the scan. we formally define the problem of scanning and filtering, derive a bound on the best achievable performance and quantify the excess loss occurring when non-optimal scanners are used, compared to optimal scanning and filtering.   we then discuss the problem of sequential scanning and prediction of noisy random fields. this setting is a natural model for applications such as restoration and coding of noisy images. we formally define the problem of scanning and prediction of a noisy multidimensional array and relate the optimal performance to the clean scandictability defined by merhav and weissman. moreover, bounds on the excess loss due to sub-optimal scans are derived, and a universal prediction algorithm is suggested.   this paper is the second part of a two-part paper. the first paper dealt with sequential decision making on noiseless data arrays, namely, when the decision maker is judged with respect to the same data array it observes.", "date_create": "2007-05-20", "area": "cs.it cs.cv math.it", "authors": ["cohen", "weissman", "merhav"]}, {"idpaper": "0705.2862", "title": "cryptanalysis of group-based key agreement protocols using subgroup   distance functions", "abstract": "we introduce a new approach for cryptanalysis of key agreement protocols based on noncommutative groups. this approach uses functions that estimate the distance of a group element to a given subgroup. we test it against the shpilrain-ushakov protocol, which is based on thompson's group f.", "date_create": "2007-05-20", "area": "cs.cr", "authors": ["ruinskiy", "shamir", "tsaban"]}, {"idpaper": "0705.3015", "title": "an extensible timing infrastructure for adaptive large-scale   applications", "abstract": "real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. the cactus framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing api. applications built with cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as papi. we describe the cactus timer interface, its motivation, and its implementation. we then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", "date_create": "2007-05-21", "area": "cs.pf cs.dc", "authors": ["stark", "allen", "goodale", "radke", "schnetter"]}, {"idpaper": "0705.3015", "title": "an extensible timing infrastructure for adaptive large-scale   applications", "abstract": "real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. the cactus framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing api. applications built with cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as papi. we describe the cactus timer interface, its motivation, and its implementation. we then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", "date_create": "2007-05-21", "area": "cs.pf cs.dc", "authors": ["stark", "allen", "goodale", "radke", "schnetter"]}, {"idpaper": "0705.3015", "title": "an extensible timing infrastructure for adaptive large-scale   applications", "abstract": "real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. the cactus framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing api. applications built with cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as papi. we describe the cactus timer interface, its motivation, and its implementation. we then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", "date_create": "2007-05-21", "area": "cs.pf cs.dc", "authors": ["stark", "allen", "goodale", "radke", "schnetter"]}, {"idpaper": "0705.3015", "title": "an extensible timing infrastructure for adaptive large-scale   applications", "abstract": "real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. the cactus framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing api. applications built with cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as papi. we describe the cactus timer interface, its motivation, and its implementation. we then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", "date_create": "2007-05-21", "area": "cs.pf cs.dc", "authors": ["stark", "allen", "goodale", "radke", "schnetter"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3468", "title": "linear tabling strategies and optimizations", "abstract": "recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. one decision concerns when answers are consumed and returned. this paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. the results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. in this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. we give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. benchmarking in b-prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of slg.", "date_create": "2007-05-23", "area": "cs.pl", "authors": ["zhou", "sato", "shen"]}, {"idpaper": "0705.3487", "title": "linearly bounded infinite graphs", "abstract": "linearly bounded turing machines have been mainly studied as acceptors for context-sensitive languages. we define a natural class of infinite automata representing their observable computational behavior, called linearly bounded graphs. these automata naturally accept the same languages as the linearly bounded machines defining them. we present some of their structural properties as well as alternative characterizations in terms of rewriting systems and context-sensitive transductions. finally, we compare these graphs to rational graphs, which are another class of automata accepting the context-sensitive languages, and prove that in the bounded-degree case, rational graphs are a strict sub-class of linearly bounded graphs.", "date_create": "2007-05-24", "area": "cs.lo", "authors": ["carayol", "meyer"]}, {"idpaper": "0705.3593", "title": "mi image registration using prior knowledge", "abstract": "subtraction of aligned images is a means to assess changes in a wide variety of clinical applications. in this paper we explore the information theoretical origin of mutual information (mi), which is based on shannon's entropy.however, the interpretation of standard mi registration as a communication channel suggests that mi is too restrictive a criterion. in this paper the concept of mutual information (mi) is extended to (normalized) focussed mutual information (fmi) to incorporate prior knowledge to overcome some shortcomings of mi. we use this to develop new methodologies to successfully address specific registration problems, the follow-up of dental restorations, cephalometry, and the monitoring of implants.", "date_create": "2007-05-24", "area": "cs.cv", "authors": ["jacquet", "de groen"]}, {"idpaper": "0705.3610", "title": "a logic of reachable patterns in linked data-structures", "abstract": "we define a new decidable logic for expressing and checking invariants of programs that manipulate dynamically-allocated objects via pointers and destructive pointer updates. the main feature of this logic is the ability to limit the neighborhood of a node that is reachable via a regular expression from a designated node. the logic is closed under boolean operations (entailment, negation) and has a finite model property. the key technical result is the proof of decidability. we show how to express precondition, postconditions, and loop invariants for some interesting programs. it is also possible to express properties such as disjointness of data-structures, and low-level heap mutations. moreover, our logic can express properties of arbitrary data-structures and of an arbitrary number of pointer fields. the latter provides a way to naturally specify postconditions that relate the fields on entry to a procedure to the fields on exit. therefore, it is possible to use the logic to automatically prove partial correctness of programs performing low-level heap mutations.", "date_create": "2007-05-24", "area": "cs.lo", "authors": ["yorsh", "rabinovich", "sagiv", "meyer", "bouajjani"]}, {"idpaper": "0705.3631", "title": "triple-loop networks with arbitrarily many minimum distance diagrams", "abstract": "minimum distance diagrams are a way to encode the diameter and routing information of multi-loop networks. for the widely studied case of double-loop networks, it is known that each network has at most two such diagrams and that they have a very definite form \"l-shape''.   in contrast, in this paper we show that there are triple-loop networks with an arbitrarily big number of associated minimum distance diagrams. for doing this, we build-up on the relations between minimum distance diagrams and monomial ideals.", "date_create": "2007-05-24", "area": "math.co cs.dm math.oc", "authors": ["sabariego", "santos"]}, {"idpaper": "0705.3820", "title": "maximizing maximal angles for plane straight-line graphs", "abstract": "let $g=(s, e)$ be a plane straight-line graph on a finite point set $s\\subset\\r^2$ in general position. the incident angles of a vertex $p \\in s$ of $g$ are the angles between any two edges of $g$ that appear consecutively in the circular order of the edges incident to $p$.   a plane straight-line graph is called $\\phi$-open if each vertex has an incident angle of size at least $\\phi$. in this paper we study the following type of question: what is the maximum angle $\\phi$ such that for any finite set $s\\subset\\r^2$ of points in general position we can find a graph from a certain class of graphs on $s$ that is $\\phi$-open? in particular, we consider the classes of triangulations, spanning trees, and paths on $s$ and give tight bounds in most cases.", "date_create": "2007-05-25", "area": "cs.cg cs.dm math.co", "authors": ["aichholzer", "hackl", "hoffmann", "huemer", "por", "santos", "speckmann", "vogtenhuber"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4085", "title": "the distance geometry of music", "abstract": "we demonstrate relationships between the classic euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. specifically, we show how the structure of the euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. we prove that these \\emph{euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the euclidean distances between all pairs of onsets, viewing onsets as points on a circle. indeed, euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. we also show that essentially all euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. all of our results for musical rhythms apply equally well to musical scales. in addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by erd\\h{o}s in the plane.", "date_create": "2007-05-28", "area": "cs.cg", "authors": ["demaine", "gomez-martin", "meijer", "rappaport", "taslakian", "toussaint", "winograd", "wood"]}, {"idpaper": "0705.4094", "title": "efficiency and nash equilibria in a scrip system for p2p networks", "abstract": "a model of providing service in a p2p network is analyzed. it is shown that by adding a scrip system, a mechanism that admits a reasonable nash equilibrium that reduces free riding can be obtained. the effect of varying the total amount of money (scrip) in the system on efficiency (i.e., social welfare) is analyzed, and it is shown that by maintaining the appropriate ratio between the total amount of money and the number of agents, efficiency is maximized. the work has implications for many online systems, not only p2p networks but also a wide variety of online forums for which scrip systems are popular, but formal analyses have been lacking.", "date_create": "2007-05-28", "area": "cs.gt", "authors": ["friedman", "halpern", "kash"]}, {"idpaper": "0705.4094", "title": "efficiency and nash equilibria in a scrip system for p2p networks", "abstract": "a model of providing service in a p2p network is analyzed. it is shown that by adding a scrip system, a mechanism that admits a reasonable nash equilibrium that reduces free riding can be obtained. the effect of varying the total amount of money (scrip) in the system on efficiency (i.e., social welfare) is analyzed, and it is shown that by maintaining the appropriate ratio between the total amount of money and the number of agents, efficiency is maximized. the work has implications for many online systems, not only p2p networks but also a wide variety of online forums for which scrip systems are popular, but formal analyses have been lacking.", "date_create": "2007-05-28", "area": "cs.gt", "authors": ["friedman", "halpern", "kash"]}, {"idpaper": "0705.4094", "title": "efficiency and nash equilibria in a scrip system for p2p networks", "abstract": "a model of providing service in a p2p network is analyzed. it is shown that by adding a scrip system, a mechanism that admits a reasonable nash equilibrium that reduces free riding can be obtained. the effect of varying the total amount of money (scrip) in the system on efficiency (i.e., social welfare) is analyzed, and it is shown that by maintaining the appropriate ratio between the total amount of money and the number of agents, efficiency is maximized. the work has implications for many online systems, not only p2p networks but also a wide variety of online forums for which scrip systems are popular, but formal analyses have been lacking.", "date_create": "2007-05-28", "area": "cs.gt", "authors": ["friedman", "halpern", "kash"]}, {"idpaper": "0705.4320", "title": "defect-tolerant cmol cell assignment via satisfiability", "abstract": "we present a cad framework for cmol, a hybrid cmos/ molecular circuit architecture. our framework first transforms any logically synthesized circuit based on and/or/not gates to a nor gate circuit, and then maps the nor gates to cmol. we encode the cmol cell assignment problem as boolean conditions. the boolean constraint is satisfiable if and only if there is a way to map all the nor gates to the cmol cells. we further investigate various types of static defects for the cmol architecture, and propose a reconfiguration technique that can deal with these defects through our cad framework. this is the first automated framework for cmol cell assignment, and the first to model several different cmol static defects. empirical results show that our approach is efficient and scalable.", "date_create": "2007-05-29", "area": "cs.dm cs.ds", "authors": ["hung", "gao", "song", "hammerstrom"]}, {"idpaper": "0705.4320", "title": "defect-tolerant cmol cell assignment via satisfiability", "abstract": "we present a cad framework for cmol, a hybrid cmos/ molecular circuit architecture. our framework first transforms any logically synthesized circuit based on and/or/not gates to a nor gate circuit, and then maps the nor gates to cmol. we encode the cmol cell assignment problem as boolean conditions. the boolean constraint is satisfiable if and only if there is a way to map all the nor gates to the cmol cells. we further investigate various types of static defects for the cmol architecture, and propose a reconfiguration technique that can deal with these defects through our cad framework. this is the first automated framework for cmol cell assignment, and the first to model several different cmol static defects. empirical results show that our approach is efficient and scalable.", "date_create": "2007-05-29", "area": "cs.dm cs.ds", "authors": ["hung", "gao", "song", "hammerstrom"]}, {"idpaper": "0705.4320", "title": "defect-tolerant cmol cell assignment via satisfiability", "abstract": "we present a cad framework for cmol, a hybrid cmos/ molecular circuit architecture. our framework first transforms any logically synthesized circuit based on and/or/not gates to a nor gate circuit, and then maps the nor gates to cmol. we encode the cmol cell assignment problem as boolean conditions. the boolean constraint is satisfiable if and only if there is a way to map all the nor gates to the cmol cells. we further investigate various types of static defects for the cmol architecture, and propose a reconfiguration technique that can deal with these defects through our cad framework. this is the first automated framework for cmol cell assignment, and the first to model several different cmol static defects. empirical results show that our approach is efficient and scalable.", "date_create": "2007-05-29", "area": "cs.dm cs.ds", "authors": ["hung", "gao", "song", "hammerstrom"]}, {"idpaper": "0705.4320", "title": "defect-tolerant cmol cell assignment via satisfiability", "abstract": "we present a cad framework for cmol, a hybrid cmos/ molecular circuit architecture. our framework first transforms any logically synthesized circuit based on and/or/not gates to a nor gate circuit, and then maps the nor gates to cmol. we encode the cmol cell assignment problem as boolean conditions. the boolean constraint is satisfiable if and only if there is a way to map all the nor gates to the cmol cells. we further investigate various types of static defects for the cmol architecture, and propose a reconfiguration technique that can deal with these defects through our cad framework. this is the first automated framework for cmol cell assignment, and the first to model several different cmol static defects. empirical results show that our approach is efficient and scalable.", "date_create": "2007-05-29", "area": "cs.dm cs.ds", "authors": ["hung", "gao", "song", "hammerstrom"]}, {"idpaper": "0705.4415", "title": "perceval: a computer-driven system for experimentation on auditory and   visual perception", "abstract": "since perception tests are highly time-consuming, there is a need to automate as many operations as possible, such as stimulus generation, procedure control, perception testing, and data analysis. the computer-driven system we are presenting here meets these objectives. to achieve large flexibility, the tests are controlled by scripts. the system's core software resembles that of a lexical-syntactic analyzer, which reads and interprets script files sent to it. the execution sequence (trial) is modified in accordance with the commands and data received. this type of operation provides a great deal of flexibility and supports a wide variety of tests such as auditory-lexical decision making, phoneme monitoring, gating, phonetic categorization, word identification, voice quality, etc. to achieve good performance, we were careful about timing accuracy, which is the greatest problem in computerized perception tests.", "date_create": "2007-05-30", "area": "cs.se", "authors": ["andr\u00e9", "ghio", "cav\u00e9", "teston"]}, {"idpaper": "0705.4485", "title": "mixed membership stochastic blockmodels", "abstract": "observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. in this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. this model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. we develop a general variational inference algorithm for fast approximate posterior inference. we explore applications to social and protein interaction networks.", "date_create": "2007-05-30", "area": "stat.me cs.lg math.st physics.soc-ph stat.ml stat.th", "authors": ["airoldi", "blei", "fienberg", "xing"]}, {"idpaper": "0705.4618", "title": "an improved tight closure algorithm for integer octagonal constraints", "abstract": "integer octagonal constraints (a.k.a. ``unit two variables per inequality'' or ``utvpi integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. the main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. the latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. in this paper we present and fully justify an o(n^3) algorithm to compute the tight closure of a set of utvpi integer constraints.", "date_create": "2007-05-31", "area": "cs.ds cs.cg cs.lo", "authors": ["bagnara", "hill", "zaffanella"]}, {"idpaper": "0705.4618", "title": "an improved tight closure algorithm for integer octagonal constraints", "abstract": "integer octagonal constraints (a.k.a. ``unit two variables per inequality'' or ``utvpi integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. the main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. the latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. in this paper we present and fully justify an o(n^3) algorithm to compute the tight closure of a set of utvpi integer constraints.", "date_create": "2007-05-31", "area": "cs.ds cs.cg cs.lo", "authors": ["bagnara", "hill", "zaffanella"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0022", "title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative. the semantic web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. currently, the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. however, new models have emerged that explicitly encode procedural information within the semantic network substrate. with these new technologies, the semantic web has evolved from a data modeling medium to a computational medium. this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", "date_create": "2007-05-31", "area": "cs.ai cs.gl", "authors": ["rodriguez", "bollen"]}, {"idpaper": "0706.0507", "title": "a collaborative framework to exchange and share product information   within a supply chain context", "abstract": "the new requirement for \"collaboration\" between multidisciplinary collaborators induces to exchange and share adequate information on the product, processes throughout the products' lifecycle. thus, effective capture of information, and also its extraction, recording, exchange, sharing, and reuse become increasingly critical. these lead companies to adopt new improved methodologies in managing the exchange and sharing of information. the aim of this paper is to describe a collaborative framework system to exchange and share information, which is based on: (i) the product process collaboration organization model (ppco) which defines product and process information, and the various collaboration methods for the organizations involved in the supply chain. (ii) viewpoint model describes relationships between each actor and the comprehensive product/process model, defining each actor's \"domain of interest\" within the evolving product definition. (iii) a layer which defines the comprehensive organization and collaboration relationships between the actors within the supply chain. (iv) based on the above relationships, the last layer proposes a typology of exchanged messages. a communication method, based on xml, is developed that supports optimal exchange/sharing of information. to illustrate the proposed framework system, an example is presented related to collaborative design of a new piston for an automotive engine. the focus is on user-viewpoint integration to ensure that the adequate information is retrieved from the ppco.", "date_create": "2007-06-04", "area": "cs.hc", "authors": ["geryville", "ouzrout", "bouras", "sapidis"]}, {"idpaper": "0706.0507", "title": "a collaborative framework to exchange and share product information   within a supply chain context", "abstract": "the new requirement for \"collaboration\" between multidisciplinary collaborators induces to exchange and share adequate information on the product, processes throughout the products' lifecycle. thus, effective capture of information, and also its extraction, recording, exchange, sharing, and reuse become increasingly critical. these lead companies to adopt new improved methodologies in managing the exchange and sharing of information. the aim of this paper is to describe a collaborative framework system to exchange and share information, which is based on: (i) the product process collaboration organization model (ppco) which defines product and process information, and the various collaboration methods for the organizations involved in the supply chain. (ii) viewpoint model describes relationships between each actor and the comprehensive product/process model, defining each actor's \"domain of interest\" within the evolving product definition. (iii) a layer which defines the comprehensive organization and collaboration relationships between the actors within the supply chain. (iv) based on the above relationships, the last layer proposes a typology of exchanged messages. a communication method, based on xml, is developed that supports optimal exchange/sharing of information. to illustrate the proposed framework system, an example is presented related to collaborative design of a new piston for an automotive engine. the focus is on user-viewpoint integration to ensure that the adequate information is retrieved from the ppco.", "date_create": "2007-06-04", "area": "cs.hc", "authors": ["geryville", "ouzrout", "bouras", "sapidis"]}, {"idpaper": "0706.0507", "title": "a collaborative framework to exchange and share product information   within a supply chain context", "abstract": "the new requirement for \"collaboration\" between multidisciplinary collaborators induces to exchange and share adequate information on the product, processes throughout the products' lifecycle. thus, effective capture of information, and also its extraction, recording, exchange, sharing, and reuse become increasingly critical. these lead companies to adopt new improved methodologies in managing the exchange and sharing of information. the aim of this paper is to describe a collaborative framework system to exchange and share information, which is based on: (i) the product process collaboration organization model (ppco) which defines product and process information, and the various collaboration methods for the organizations involved in the supply chain. (ii) viewpoint model describes relationships between each actor and the comprehensive product/process model, defining each actor's \"domain of interest\" within the evolving product definition. (iii) a layer which defines the comprehensive organization and collaboration relationships between the actors within the supply chain. (iv) based on the above relationships, the last layer proposes a typology of exchanged messages. a communication method, based on xml, is developed that supports optimal exchange/sharing of information. to illustrate the proposed framework system, an example is presented related to collaborative design of a new piston for an automotive engine. the focus is on user-viewpoint integration to ensure that the adequate information is retrieved from the ppco.", "date_create": "2007-06-04", "area": "cs.hc", "authors": ["geryville", "ouzrout", "bouras", "sapidis"]}, {"idpaper": "0706.0564", "title": "tropical implicitization and mixed fiber polytopes", "abstract": "the software trim offers implementations of tropical implicitization and tropical elimination, as developed by tevelev and the authors. given a polynomial map with generic coefficients, trim computes the tropical variety of the image. when the image is a hypersurface, the output is the newton polytope of the defining polynomial. trim can thus be used to compute mixed fiber polytopes, including secondary polytopes.", "date_create": "2007-06-04", "area": "cs.sc math.ag math.co", "authors": ["sturmfels", "yu"]}, {"idpaper": "0706.0869", "title": "position coding", "abstract": "a position coding pattern is an array of symbols in which subarrays of a certain fixed size appear at most once. so, each subarray uniquely identifies a location in the larger array, which means there is a bijection of some sort from this set of subarrays to a set of coordinates. the key to fly pentop computer paper and other examples of position codes is a method to read the subarray and then convert it to coordinates. position coding makes use of ideas from discrete mathematics and number theory. in this paper, we will describe the underlying mathematics of two position codes, one being the anoto code that is the basis of \"fly paper\". then, we will present two new codes, one which uses binary wavelets as part of the bijection.", "date_create": "2007-06-06", "area": "cs.it math.co math.it", "authors": ["aboufadel", "armstrong", "smietana"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1002", "title": "moving vertices to make drawings plane", "abstract": "a straight-line drawing $\\delta$ of a planar graph $g$ need not be plane, but can be made so by moving some of the vertices. let shift$(g,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $g$. we show that shift$(g,\\delta)$ is np-hard to compute and to approximate, and we give explicit bounds on shift$(g,\\delta)$ when $g$ is a tree or a general planar graph. our hardness results extend to 1bendpointsetembeddability, a well-known graph-drawing problem.", "date_create": "2007-06-07", "area": "cs.cg cs.cc cs.dm", "authors": ["goaoc", "kratochvil", "okamoto", "shin", "wolff"]}, {"idpaper": "0706.1019", "title": "probabilistic anonymity and admissible schedulers", "abstract": "when studying safety properties of (formal) protocol models, it is customary to view the scheduler as an adversary: an entity trying to falsify the safety property. we show that in the context of security protocols, and in particular of anonymizing protocols, this gives the adversary too much power; for instance, the contents of encrypted messages and internal computations by the parties should be considered invisible to the adversary.   we restrict the class of schedulers to a class of admissible schedulers which better model adversarial behaviour. these admissible schedulers base their decision solely on the past behaviour of the system that is visible to the adversary.   using this, we propose a definition of anonymity: for all admissible schedulers the identity of the users and the observations of the adversary are independent stochastic variables. we also develop a proof technique for typical cases that can be used to proof anonymity: a system is anonymous if it is possible to `exchange' the behaviour of two users without the adversary `noticing'.", "date_create": "2007-06-07", "area": "cs.cr", "authors": ["garcia", "van rossum", "sokolova"]}, {"idpaper": "0706.1019", "title": "probabilistic anonymity and admissible schedulers", "abstract": "when studying safety properties of (formal) protocol models, it is customary to view the scheduler as an adversary: an entity trying to falsify the safety property. we show that in the context of security protocols, and in particular of anonymizing protocols, this gives the adversary too much power; for instance, the contents of encrypted messages and internal computations by the parties should be considered invisible to the adversary.   we restrict the class of schedulers to a class of admissible schedulers which better model adversarial behaviour. these admissible schedulers base their decision solely on the past behaviour of the system that is visible to the adversary.   using this, we propose a definition of anonymity: for all admissible schedulers the identity of the users and the observations of the adversary are independent stochastic variables. we also develop a proof technique for typical cases that can be used to proof anonymity: a system is anonymous if it is possible to `exchange' the behaviour of two users without the adversary `noticing'.", "date_create": "2007-06-07", "area": "cs.cr", "authors": ["garcia", "van rossum", "sokolova"]}, {"idpaper": "0706.1061", "title": "design, implementation, and cooperative coevolution of an autonomous/   teleoperated control system for a serpentine robotic manipulator", "abstract": "design, implementation, and machine learning issues associated with developing a control system for a serpentine robotic manipulator are explored. the controller developed provides autonomous control of the serpentine robotic manipulatorduring operation of the manipulator within an enclosed environment such as an underground storage tank. the controller algorithms make use of both low-level joint angle control employing force/position feedback constraints, and high-level coordinated control of end-effector positioning. this approach has resulted in both high-level full robotic control and low-level telerobotic control modes, and provides a high level of dexterity for the operator.", "date_create": "2007-06-07", "area": "cs.ne cs.ro", "authors": ["sofge", "chiang"]}, {"idpaper": "0706.1063", "title": "small worlds: strong clustering in wireless networks", "abstract": "small-worlds represent efficient communication networks that obey two distinguishing characteristics: a high clustering coefficient together with a small characteristic path length. this paper focuses on an interesting paradox, that removing links in a network can increase the overall clustering coefficient. reckful roaming, as introduced in this paper, is a 2-localized algorithm that takes advantage of this paradox in order to selectively remove superfluous links, this way optimizing the clustering coefficient while still retaining a sufficiently small characteristic path length.", "date_create": "2007-06-07", "area": "cs.ni cs.dc cs.ds", "authors": ["brust", "rothkugel"]}, {"idpaper": "0706.1084", "title": "sublinear algorithms for approximating string compressibility", "abstract": "we raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. we study this question in detail for two popular lossless compression schemes: run-length encoding (rle) and lempel-ziv (lz), and present sublinear algorithms for approximating compressibility with respect to both schemes. we also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly.   our investigation of lz yields results whose interest goes beyond the initial questions we set out to study. in particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to lempel-ziv to the number of distinct short substrings contained in it. in addition, we show that approximating the compressibility with respect to lz is related to approximating the support size of a distribution.", "date_create": "2007-06-07", "area": "cs.ds", "authors": ["raskhodnikova", "ron", "rubinfeld", "smith"]}, {"idpaper": "0706.1084", "title": "sublinear algorithms for approximating string compressibility", "abstract": "we raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. we study this question in detail for two popular lossless compression schemes: run-length encoding (rle) and lempel-ziv (lz), and present sublinear algorithms for approximating compressibility with respect to both schemes. we also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly.   our investigation of lz yields results whose interest goes beyond the initial questions we set out to study. in particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to lempel-ziv to the number of distinct short substrings contained in it. in addition, we show that approximating the compressibility with respect to lz is related to approximating the support size of a distribution.", "date_create": "2007-06-07", "area": "cs.ds", "authors": ["raskhodnikova", "ron", "rubinfeld", "smith"]}, {"idpaper": "0706.1084", "title": "sublinear algorithms for approximating string compressibility", "abstract": "we raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. we study this question in detail for two popular lossless compression schemes: run-length encoding (rle) and lempel-ziv (lz), and present sublinear algorithms for approximating compressibility with respect to both schemes. we also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly.   our investigation of lz yields results whose interest goes beyond the initial questions we set out to study. in particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to lempel-ziv to the number of distinct short substrings contained in it. in addition, we show that approximating the compressibility with respect to lz is related to approximating the support size of a distribution.", "date_create": "2007-06-07", "area": "cs.ds", "authors": ["raskhodnikova", "ron", "rubinfeld", "smith"]}, {"idpaper": "0706.1127", "title": "redesigning computer-based learning environments: evaluation as   communication", "abstract": "in the field of evaluation research, computer scientists live constantly upon dilemmas and conflicting theories. as evaluation is differently perceived and modeled among educational areas, it is not difficult to become trapped in dilemmas, which reflects an epistemological weakness. additionally, designing and developing a computer-based learning scenario is not an easy task. advancing further, with end-users probing the system in realistic settings, is even harder. computer science research in evaluation faces an immense challenge, having to cope with contributions from several conflicting and controversial research fields. we believe that deep changes must be made in our field if we are to advance beyond the cbt (computer-based training) learning model and to build an adequate epistemology for this challenge. the first task is to relocate our field by building upon recent results from philosophy, psychology, social sciences, and engineering. in this article we locate evaluation in respect to communication studies. evaluation presupposes a definition of goals to be reached, and we suggest that it is, by many means, a silent communication between teacher and student, peers, and institutional entities. if we accept that evaluation can be viewed as set of invisible rules known by nobody, but somehow understood by everybody, we should add anthropological inquiries to our research toolkit. the paper is organized around some elements of the social communication and how they convey new insights to evaluation research for computer and related scientists. we found some technical limitations and offer discussions on how we relate to technology at same time we establish expectancies and perceive others work.", "date_create": "2007-06-08", "area": "cs.cy cs.hc", "authors": ["brust", "adriano", "ricarte"]}, {"idpaper": "0706.1318", "title": "constructing a maximum utility slate of on-line advertisements", "abstract": "we present an algorithm for constructing an optimal slate of sponsored search advertisements which respects the ordering that is the outcome of a generalized second price auction, but which must also accommodate complicating factors such as overall budget constraints. the algorithm is easily fast enough to use on the fly for typical problem sizes, or as a subroutine in an overall optimization.", "date_create": "2007-06-09", "area": "cs.dm cs.ds", "authors": ["keerthi", "tomlin"]}, {"idpaper": "0706.1395", "title": "opportunistic network coding for video streaming over wireless", "abstract": "in this paper, we study video streaming over wireless networks with network coding capabilities. we build upon recent work, which demonstrated that network coding can increase throughput over a broadcast medium, by mixing packets from different flows into a single packet, thus increasing the information content per transmission. our key insight is that, when the transmitted flows are video streams, network codes should be selected so as to maximize not only the network throughput but also the video quality. we propose video-aware opportunistic network coding schemes that take into account both (i) the decodability of network codes by several receivers and (ii) the importance and deadlines of video packets. simulation results show that our schemes significantly improve both video quality and throughput.", "date_create": "2007-06-11", "area": "cs.ni", "authors": ["seferoglu", "markopoulou"]}, {"idpaper": "0706.1477", "title": "vpspace and a transfer theorem over the complex field", "abstract": "we extend the transfer theorem of [kp2007] to the complex field. that is, we investigate the links between the class vpspace of families of polynomials and the blum-shub-smale model of computation over c. roughly speaking, a family of polynomials is in vpspace if its coefficients can be computed in polynomial space. our main result is that if (uniform, constant-free) vpspace families can be evaluated efficiently then the class par of decision problems that can be solved in parallel polynomial time over the complex field collapses to p. as a result, one must first be able to show that there are vpspace families which are hard to evaluate in order to separate p from np over c, or even from par.", "date_create": "2007-06-11", "area": "cs.cc", "authors": ["koiran", "perifel"]}, {"idpaper": "0706.1692", "title": "a methodology for efficient space-time adapter design space exploration:   a case study of an ultra wide band interleaver", "abstract": "this paper presents a solution to efficiently explore the design space of communication adapters. in most digital signal processing (dsp) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. by explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named space-time adapter (star). our design flow inputs a c description of input/output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a resource constraints graph (rcg). the rcg properties enable an efficient architecture space exploration in order to synthesize a star component. the proposed approach has been tested to design an industrial data mixing block example: an ultra-wideband interleaver.", "date_create": "2007-06-12", "area": "cs.ar", "authors": ["chavet", "coussy", "urard", "martin"]}, {"idpaper": "0706.1700", "title": "information criteria and arithmetic codings : an illustration on raw   images", "abstract": "in this paper we give a short theoretical description of the general predictive adaptive arithmetic coding technique. the links between this technique and the works of j. rissanen in the 80's, in particular the bic information criterion used in parametrical model selection problems, are established. we also design lossless and lossy coding techniques of images. the lossless technique uses a mix between fixed-length coding and arithmetic coding and provides better compression results than those separate methods. that technique is also seen to have an interesting application in the domain of statistics since it gives a data-driven procedure for the non-parametrical histogram selection problem. the lossy technique uses only predictive adaptive arithmetic codes and shows how a good choice of the order of prediction might lead to better results in terms of compression. we illustrate those coding techniques on a raw grayscale image.", "date_create": "2007-06-12", "area": "cs.it math.it", "authors": ["coq", "alata", "arnaudon", "olivier"]}, {"idpaper": "0706.1860", "title": "fipa-based interoperable agent mobility proposal", "abstract": "this paper presents a proposal for a flexible agent mobility architecture based on ieee-fipa standards and intended to be one of them. this proposal is a first step towards interoperable mobility mechanisms, which are needed for future agent migration between different kinds of platforms. our proposal is presented as a flexible and robust architecture that has been successfully implemented in the jade and agentscape platforms. it is based on an open set of protocols, allowing new protocols and future improvements to be accommodated in the architecture. with this proposal we demonstrate that a standard architecture for agent mobility capable of supporting several agent platforms can be defined and implemented.", "date_create": "2007-06-13", "area": "cs.ma cs.ni", "authors": ["cucurull", "marti", "robles", "borrell", "navarro"]}, {"idpaper": "0706.1860", "title": "fipa-based interoperable agent mobility proposal", "abstract": "this paper presents a proposal for a flexible agent mobility architecture based on ieee-fipa standards and intended to be one of them. this proposal is a first step towards interoperable mobility mechanisms, which are needed for future agent migration between different kinds of platforms. our proposal is presented as a flexible and robust architecture that has been successfully implemented in the jade and agentscape platforms. it is based on an open set of protocols, allowing new protocols and future improvements to be accommodated in the architecture. with this proposal we demonstrate that a standard architecture for agent mobility capable of supporting several agent platforms can be defined and implemented.", "date_create": "2007-06-13", "area": "cs.ma cs.ni", "authors": ["cucurull", "marti", "robles", "borrell", "navarro"]}, {"idpaper": "0706.2069", "title": "building portable thread schedulers for hierarchical multiprocessors:   the bubblesched framework", "abstract": "exploiting full computational power of current more and more hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. unfortunately, most operating systems only provide a poor scheduling api that does not allow applications to transmit valuable scheduling hints to the system. in a previous paper, we showed that using a bubble-based thread scheduler can significantly improve applications' performance in a portable way. however, since multithreaded applications have various scheduling requirements, there is no universal scheduler that could meet all these needs. in this paper, we present a framework that allows scheduling experts to implement and experiment with customized thread schedulers. it provides a powerful api for dynamically distributing bubbles among the machine in a high-level, portable, and efficient way. several examples show how experts can then develop, debug and tune their own portable bubble schedulers.", "date_create": "2007-06-14", "area": "cs.dc", "authors": ["thibault", "namyst", "wacrenier"]}, {"idpaper": "0706.2606", "title": "randomness extraction via delta-biased masking in the presence of a   quantum attacker", "abstract": "randomness extraction is of fundamental importance for information-theoretic cryptography. it allows to transform a raw key about which an attacker has some limited knowledge into a fully secure random key, on which the attacker has essentially no information. up to date, only very few randomness-extraction techniques are known to work against an attacker holding quantum information on the raw key. this is very much in contrast to the classical (non-quantum) setting, which is much better understood and for which a vast amount of different techniques are known and proven to work.   we prove a new randomness-extraction technique, which is known to work in the classical setting, to be secure against a quantum attacker as well. randomness extraction is done by xor'ing a so-called delta-biased mask to the raw key. our result allows to extend the classical applications of this extractor to the quantum setting. we discuss the following two applications. we show how to encrypt a long message with a short key, information-theoretically secure against a quantum attacker, provided that the attacker has enough quantum uncertainty on the message. this generalizes the concept of entropically-secure encryption to the case of a quantum attacker. as second application, we show how to do error-correction without leaking partial information to a quantum attacker. such a technique is useful in settings where the raw key may contain errors, since standard error-correction techniques may provide the attacker with information on, say, a secret key that was used to obtain the raw key.", "date_create": "2007-06-18", "area": "quant-ph cs.cr", "authors": ["fehr", "schaffner"]}, {"idpaper": "0706.2748", "title": "a survey of unix init schemes", "abstract": "in most modern operating systems, init (as in \"initialization\") is the program launched by the kernel at boot time. it runs as a daemon and typically has pid 1. init is responsible for spawning all other processes and scavenging zombies. it is also responsible for reboot and shutdown operations. this document describes existing solutions that implement the init process and/or init scripts in unix-like systems. these solutions range from the legacy and still-in-use bsd and systemv schemes, to recent and promising schemes from ubuntu, apple, sun and independent developers. our goal is to highlight their focus and compare their sets of features.", "date_create": "2007-06-19", "area": "cs.os cs.gl", "authors": ["royon", "fr\u00e9not"]}, {"idpaper": "0706.2795", "title": "dirty-paper coding without channel information at the transmitter and   imperfect estimation at the receiver", "abstract": "in this paper, we examine the effects of imperfect channel estimation at the receiver and no channel knowledge at the transmitter on the capacity of the fading costa's channel with channel state information non-causally known at the transmitter. we derive the optimal dirty-paper coding (dpc) scheme and its corresponding achievable rates with the assumption of gaussian inputs. our results, for uncorrelated rayleigh fading, provide intuitive insights on the impact of the channel estimate and the channel characteristics (e.g. snr, fading process, channel training) on the achievable rates. these are useful in practical scenarios of multiuser wireless communications (e.g. broadcast channels) and information embedding applications (e.g. robust watermarking). we also studied optimal training design adapted to each application. we provide numerical results for a single-user fading costa's channel with maximum-likehood (ml) channel estimation. these illustrate an interesting practical trade-off between the amount of training and its impact to the interference cancellation performance using dpc scheme.", "date_create": "2007-06-19", "area": "cs.it math.it", "authors": ["piantanida", "duhamel"]}, {"idpaper": "0706.2839", "title": "cache analysis of non-uniform distribution sorting algorithms", "abstract": "we analyse the average-case cache performance of distribution sorting algorithms in the case when keys are independently but not necessarily uniformly distributed. the analysis is for both `in-place' and `out-of-place' distribution sorting algorithms and is more accurate than the analysis presented in \\cite{rresa00}. in particular, this new analysis yields tighter upper and lower bounds when the keys are drawn from a uniform distribution.   we use this analysis to tune the performance of the integer sorting algorithm msb radix sort when it is used to sort independent uniform floating-point numbers (floats). our tuned msb radix sort algorithm comfortably outperforms a cache-tuned implementations of bucketsort \\cite{rr99} and quicksort when sorting uniform floats from $[0, 1)$.", "date_create": "2007-06-19", "area": "cs.ds cs.pf", "authors": ["rahman", "raman"]}, {"idpaper": "0706.3060", "title": "n-body simulations on gpus", "abstract": "commercial graphics processors (gpus) have high compute capacity at very low cost, which makes them attractive for general purpose scientific computing. in this paper we show how graphics processors can be used for n-body simulations to obtain improvements in performance over current generation cpus. we have developed a highly optimized algorithm for performing the o(n^2) force calculations that constitute the major part of stellar and molecular dynamics simulations. in some of the calculations, we achieve sustained performance of nearly 100 gflops on an ati x1900xtx. the performance on gpus is comparable to specialized processors such as grape-6a and mdgrape-3, but at a fraction of the cost. furthermore, the wide availability of gpus has significant implications for cluster computing and distributed computing efforts like folding@home.", "date_create": "2007-06-20", "area": "cs.ce cs.dc", "authors": ["elsen", "vishal", "houston", "pande", "hanrahan", "darve"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3146", "title": "wifi epidemiology: can your neighbors' router make yours sick?", "abstract": "in densely populated urban areas wifi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas wifi networks. in this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the us. we develop an epidemiological model that takes into consideration prevalent security flaws on these routers. the spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. we uncover a major weakness of wifi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. we indicate possible containment and prevention measure to limit the eventual harm of such an attack.", "date_create": "2007-06-21", "area": "cs.cr physics.soc-ph", "authors": ["hu", "myers", "colizza", "vespignani"]}, {"idpaper": "0706.3502", "title": "approximately-universal space-time codes for the parallel, multi-block   and cooperative-dynamic-decode-and-forward channels", "abstract": "explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays.   a particularly simple code construction that makes use of the alamouti code as a basic building block is provided for the single relay case.   along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the dmt for any fading distribution. it is shown how approximate universality of these codes leads to the first dmt-optimum code construction for the general, mimo-ofdm channel.", "date_create": "2007-06-24", "area": "cs.it cs.dm cs.ni math.it", "authors": ["elia", "kumar"]}, {"idpaper": "0706.3502", "title": "approximately-universal space-time codes for the parallel, multi-block   and cooperative-dynamic-decode-and-forward channels", "abstract": "explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays.   a particularly simple code construction that makes use of the alamouti code as a basic building block is provided for the single relay case.   along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the dmt for any fading distribution. it is shown how approximate universality of these codes leads to the first dmt-optimum code construction for the general, mimo-ofdm channel.", "date_create": "2007-06-24", "area": "cs.it cs.dm cs.ni math.it", "authors": ["elia", "kumar"]}, {"idpaper": "0706.3502", "title": "approximately-universal space-time codes for the parallel, multi-block   and cooperative-dynamic-decode-and-forward channels", "abstract": "explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays.   a particularly simple code construction that makes use of the alamouti code as a basic building block is provided for the single relay case.   along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the dmt for any fading distribution. it is shown how approximate universality of these codes leads to the first dmt-optimum code construction for the general, mimo-ofdm channel.", "date_create": "2007-06-24", "area": "cs.it cs.dm cs.ni math.it", "authors": ["elia", "kumar"]}, {"idpaper": "0706.3502", "title": "approximately-universal space-time codes for the parallel, multi-block   and cooperative-dynamic-decode-and-forward channels", "abstract": "explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays.   a particularly simple code construction that makes use of the alamouti code as a basic building block is provided for the single relay case.   along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the dmt for any fading distribution. it is shown how approximate universality of these codes leads to the first dmt-optimum code construction for the general, mimo-ofdm channel.", "date_create": "2007-06-24", "area": "cs.it cs.dm cs.ni math.it", "authors": ["elia", "kumar"]}, {"idpaper": "0706.3502", "title": "approximately-universal space-time codes for the parallel, multi-block   and cooperative-dynamic-decode-and-forward channels", "abstract": "explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays.   a particularly simple code construction that makes use of the alamouti code as a basic building block is provided for the single relay case.   along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the dmt for any fading distribution. it is shown how approximate universality of these codes leads to the first dmt-optimum code construction for the general, mimo-ofdm channel.", "date_create": "2007-06-24", "area": "cs.it cs.dm cs.ni math.it", "authors": ["elia", "kumar"]}, {"idpaper": "0706.3523", "title": "there exist some omega-powers of any borel rank", "abstract": "omega-powers of finitary languages are languages of infinite words (omega-languages) in the form v^omega, where v is a finitary language over a finite alphabet x. they appear very naturally in the characterizaton of regular or context-free omega-languages. since the set of infinite words over a finite alphabet x can be equipped with the usual cantor topology, the question of the topological complexity of omega-powers of finitary languages naturally arises and has been posed by niwinski (1990), simonnet (1992) and staiger (1997). it has been recently proved that for each integer n > 0, there exist some omega-powers of context free languages which are pi^0_n-complete borel sets, that there exists a context free language l such that l^omega is analytic but not borel, and that there exists a finitary language v such that v^omega is a borel set of infinite rank. but it was still unknown which could be the possible infinite borel ranks of omega-powers. we fill this gap here, proving the following very surprising result which shows that omega-powers exhibit a great topological complexity: for each non-null countable ordinal alpha, there exist some sigma^0_alpha-complete omega-powers, and some pi^0_alpha-complete omega-powers.", "date_create": "2007-06-25", "area": "cs.lo cs.cc math.lo", "authors": ["lecomte", "finkel"]}, {"idpaper": "0706.4004", "title": "end-to-end available bandwidth measurement tools : a comparative   evaluation of performances", "abstract": "in recent years, there has been a strong interest in measuring the available bandwidth of network paths. several methods and techniques have been proposed and various measurement tools have been developed and evaluated. however, there have been few comparative studies with regards to the actual performance of these tools. this paper presents a study of available bandwidth measurement techniques and undertakes a comparative analysis in terms of accuracy, intrusiveness and response time of active probing tools. finally, measurement errors and the uncertainty of the tools are analysed and overall conclusions made.", "date_create": "2007-06-27", "area": "cs.ni", "authors": ["ali", "michaut", "lepage"]}, {"idpaper": "0706.4004", "title": "end-to-end available bandwidth measurement tools : a comparative   evaluation of performances", "abstract": "in recent years, there has been a strong interest in measuring the available bandwidth of network paths. several methods and techniques have been proposed and various measurement tools have been developed and evaluated. however, there have been few comparative studies with regards to the actual performance of these tools. this paper presents a study of available bandwidth measurement techniques and undertakes a comparative analysis in terms of accuracy, intrusiveness and response time of active probing tools. finally, measurement errors and the uncertainty of the tools are analysed and overall conclusions made.", "date_create": "2007-06-27", "area": "cs.ni", "authors": ["ali", "michaut", "lepage"]}, {"idpaper": "0706.4009", "title": "multi-criteria scheduling of pipeline workflows", "abstract": "mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. several antagonist criteria should be optimized, such as throughput and latency (or a combination). in this paper, we study the complexity of the bi-criteria mapping problem for pipeline graphs on communication homogeneous platforms. in particular, we assess the complexity of the well-known chains-to-chains problem for different-speed processors, which turns out to be np-hard. we provide several efficient polynomial bi-criteria heuristics, and their relative performance is evaluated through extensive simulations.", "date_create": "2007-06-27", "area": "cs.dc", "authors": ["benoit", "rehn-sonigo", "robert"]}, {"idpaper": "0706.4009", "title": "multi-criteria scheduling of pipeline workflows", "abstract": "mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. several antagonist criteria should be optimized, such as throughput and latency (or a combination). in this paper, we study the complexity of the bi-criteria mapping problem for pipeline graphs on communication homogeneous platforms. in particular, we assess the complexity of the well-known chains-to-chains problem for different-speed processors, which turns out to be np-hard. we provide several efficient polynomial bi-criteria heuristics, and their relative performance is evaluated through extensive simulations.", "date_create": "2007-06-27", "area": "cs.dc", "authors": ["benoit", "rehn-sonigo", "robert"]}, {"idpaper": "0706.4009", "title": "multi-criteria scheduling of pipeline workflows", "abstract": "mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. several antagonist criteria should be optimized, such as throughput and latency (or a combination). in this paper, we study the complexity of the bi-criteria mapping problem for pipeline graphs on communication homogeneous platforms. in particular, we assess the complexity of the well-known chains-to-chains problem for different-speed processors, which turns out to be np-hard. we provide several efficient polynomial bi-criteria heuristics, and their relative performance is evaluated through extensive simulations.", "date_create": "2007-06-27", "area": "cs.dc", "authors": ["benoit", "rehn-sonigo", "robert"]}, {"idpaper": "0706.4038", "title": "scheduling multiple divisible loads on a linear processor network", "abstract": "min, veeravalli, and barlas have recently proposed strategies to minimize the overall execution time of one or several divisible loads on a heterogeneous linear network, using one or more installments. we show on a very simple example that their approach does not always produce a solution and that, when it does, the solution is often suboptimal. we also show how to find an optimal schedule for any instance, once the number of installments per load is given. then, we formally state that any optimal schedule has an infinite number of installments under a linear cost model as the one assumed in the original papers. therefore, such a cost model cannot be used to design practical multi-installment strategies. finally, through extensive simulations we confirmed that the best solution is always produced by the linear programming approach, while solutions of the original papers can be far away from the optimal.", "date_create": "2007-06-27", "area": "cs.dc", "authors": ["gallet", "robert", "vivien"]}, {"idpaper": "0706.4044", "title": "pspace bounds for rank-1 modal logics", "abstract": "for lack of general algorithmic methods that apply to wide classes of logics, establishing a complexity bound for a given modal logic is often a laborious task. the present work is a step towards a general theory of the complexity of modal logics. our main result is that all rank-1 logics enjoy a shallow model property and thus are, under mild assumptions on the format of their axiomatisation, in pspace. this leads to a unified derivation of tight pspace-bounds for a number of logics including k, kd, coalition logic, graded modal logic, majority logic, and probabilistic modal logic. our generic algorithm moreover finds tableau proofs that witness pleasant proof-theoretic properties including a weak subformula property. this generality is made possible by a coalgebraic semantics, which conveniently abstracts from the details of a given model class and thus allows covering a broad range of logics in a uniform way.", "date_create": "2007-06-27", "area": "cs.lo cs.cc", "authors": ["schr\u00f6der", "pattinson"]}, {"idpaper": "0706.4175", "title": "heuristics for network coding in wireless networks", "abstract": "multicast is a central challenge for emerging multi-hop wireless architectures such as wireless mesh networks, because of its substantial cost in terms of bandwidth. in this report, we study one specific case of multicast: broadcasting, sending data from one source to all nodes, in a multi-hop wireless network. the broadcast we focus on is based on network coding, a promising avenue for reducing cost; previous work of ours showed that the performance of network coding with simple heuristics is asymptotically optimal: each transmission is beneficial to nearly every receiver. this is for homogenous and large networks of the plan. but for small, sparse or for inhomogeneous networks, some additional heuristics are required. this report proposes such additional new heuristics (for selecting rates) for broadcasting with network coding. our heuristics are intended to use only simple local topology information. we detail the logic of the heuristics, and with experimental results, we illustrate the behavior of the heuristics, and demonstrate their excellent performance.", "date_create": "2007-06-28", "area": "cs.ni", "authors": ["cho", "adjih", "jacquet"]}, {"idpaper": "0706.4375", "title": "a robust linguistic platform for efficient and domain specific web   content analysis", "abstract": "web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities, which pertain both to information retrieval (ir) and to information extraction (ie). a rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution, or as the basis of an information extraction process. recent developments make natural language processing (nlp) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations. this paper focuses on the design and the development of a text processing platform, ogmios, which has been developed in the alvis project. the ogmios platform exploits existing nlp modules and resources, which may be tuned to specific domains and produces linguistically annotated documents. we show how the three constraints of genericity, domain semantic awareness and performance can be handled all together.", "date_create": "2007-06-29", "area": "cs.ai", "authors": ["hamon", "nazarenko", "poibeau", "aubin", "derivi\u00e8re"]}, {"idpaper": "0707.0234", "title": "selection relaying at low signal to noise ratios", "abstract": "performance of cooperative diversity schemes at low signal to noise ratios (lsnr) was recently studied by avestimehr et. al. [1] who emphasized the importance of diversity gain over multiplexing gain at low snrs. it has also been pointed out that continuous energy transfer to the channel is necessary for achieving the max-flow min-cut bound at lsnr. motivated by this we propose the use of selection decode and forward (sdf) at lsnr and analyze its performance in terms of the outage probability. we also propose an energy optimization scheme which further brings down the outage probability.", "date_create": "2007-07-02", "area": "cs.it math.it", "authors": ["rajawat", "banerjee"]}, {"idpaper": "0707.0365", "title": "performance analysis of publish/subscribe systems", "abstract": "the desktop grid offers solutions to overcome several challenges and to answer increasingly needs of scientific computing. its technology consists mainly in exploiting resources, geographically dispersed, to treat complex applications needing big power of calculation and/or important storage capacity. however, as resources number increases, the need for scalability, self-organisation, dynamic reconfigurations, decentralisation and performance becomes more and more essential. since such properties are exhibited by p2p systems, the convergence of grid computing and p2p computing seems natural. in this context, this paper evaluates the scalability and performance of p2p tools for discovering and registering services. three protocols are used for this purpose: bonjour, avahi and free-pastry. we have studied the behaviour of theses protocols related to two criteria: the elapsed time for registrations services and the needed time to discover new services. our aim is to analyse these results in order to choose the best protocol we can use in order to create a decentralised middleware for desktop grid.", "date_create": "2007-07-03", "area": "cs.dc", "authors": ["abbes", "c\u00e9rin", "dubacq", "jemni"]}, {"idpaper": "0707.0454", "title": "optimal strategies for gaussian jamming in block-fading channels under   delay and power constraints", "abstract": "without assuming any knowledge on source's codebook and its output signals, we formulate a gaussian jamming problem in block fading channels as a two-player zero sum game. the outage probability is adopted as an objective function, over which transmitter aims at minimization and jammer aims at maximization by selecting their power control strategies. optimal power control strategies for each player are obtained under both short-term and long-term power constraints. for the latter case, we first prove the non-existence of a nash equilibrium, and then provide a complete solution for both maxmin and minimax problems. numerical results demonstrate a sharp difference between the outage probabilities of the minimax and maxmin solutions.", "date_create": "2007-07-03", "area": "cs.it math.it", "authors": ["amariucai", "wei", "kannan"]}, {"idpaper": "0707.0454", "title": "optimal strategies for gaussian jamming in block-fading channels under   delay and power constraints", "abstract": "without assuming any knowledge on source's codebook and its output signals, we formulate a gaussian jamming problem in block fading channels as a two-player zero sum game. the outage probability is adopted as an objective function, over which transmitter aims at minimization and jammer aims at maximization by selecting their power control strategies. optimal power control strategies for each player are obtained under both short-term and long-term power constraints. for the latter case, we first prove the non-existence of a nash equilibrium, and then provide a complete solution for both maxmin and minimax problems. numerical results demonstrate a sharp difference between the outage probabilities of the minimax and maxmin solutions.", "date_create": "2007-07-03", "area": "cs.it math.it", "authors": ["amariucai", "wei", "kannan"]}, {"idpaper": "0707.0548", "title": "from royal road to epistatic road for variable length evolution   algorithm", "abstract": "although there are some real world applications where the use of variable length representation (vlr) in evolutionary algorithm is natural and suitable, an academic framework is lacking for such representations. in this work we propose a family of tunable fitness landscapes based on vlr of genotypes. the fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the royal road fitness landscapes, and the other hand by the nk fitness landscapes. so these landscapes offer a scale of continuity from royal road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. to gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.", "date_create": "2007-07-04", "area": "cs.ne", "authors": ["platel", "verel", "clergue", "collard"]}, {"idpaper": "0707.0548", "title": "from royal road to epistatic road for variable length evolution   algorithm", "abstract": "although there are some real world applications where the use of variable length representation (vlr) in evolutionary algorithm is natural and suitable, an academic framework is lacking for such representations. in this work we propose a family of tunable fitness landscapes based on vlr of genotypes. the fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the royal road fitness landscapes, and the other hand by the nk fitness landscapes. so these landscapes offer a scale of continuity from royal road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. to gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.", "date_create": "2007-07-04", "area": "cs.ne", "authors": ["platel", "verel", "clergue", "collard"]}, {"idpaper": "0707.0649", "title": "sphere lower bound for rotated lattice constellations in fading channels", "abstract": "we study the error probability performance of rotated lattice constellations in frequency-flat nakagami-$m$ block-fading channels. in particular, we use the sphere lower bound on the underlying infinite lattice as a performance benchmark. we show that the sphere lower bound has full diversity. we observe that optimally rotated lattices with largest known minimum product distance perform very close to the lower bound, while the ensemble of random rotations is shown to lack diversity and perform far from it.", "date_create": "2007-07-04", "area": "cs.it math.it", "authors": ["fabregas", "viterbo"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0740", "title": "a multi interface grid discovery system", "abstract": "discovery systems (ds) can be considered as entry points for global loosely coupled distributed systems. an efficient discovery system in essence increases the performance, reliability and decision making capability of distributed systems. with the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. they are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. in this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["ali", "anjum", "bunn", "khan", "mcclatchey", "newman", "steenberg", "thomas", "willers"]}, {"idpaper": "0707.0745", "title": "semantic information retrieval from distributed heterogeneous data   sources", "abstract": "information retrieval from distributed heterogeneous data sources remains a challenging issue. as the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. in this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. our goal is to provide an integrated view on biomedical information sources for the health-e-child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. in particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.", "date_create": "2007-07-05", "area": "cs.db", "authors": ["munir", "odeh", "mcclatchey", "khan", "habib"]}, {"idpaper": "0707.0745", "title": "semantic information retrieval from distributed heterogeneous data   sources", "abstract": "information retrieval from distributed heterogeneous data sources remains a challenging issue. as the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. in this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. our goal is to provide an integrated view on biomedical information sources for the health-e-child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. in particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.", "date_create": "2007-07-05", "area": "cs.db", "authors": ["munir", "odeh", "mcclatchey", "khan", "habib"]}, {"idpaper": "0707.0745", "title": "semantic information retrieval from distributed heterogeneous data   sources", "abstract": "information retrieval from distributed heterogeneous data sources remains a challenging issue. as the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. in this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. our goal is to provide an integrated view on biomedical information sources for the health-e-child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. in particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.", "date_create": "2007-07-05", "area": "cs.db", "authors": ["munir", "odeh", "mcclatchey", "khan", "habib"]}, {"idpaper": "0707.0745", "title": "semantic information retrieval from distributed heterogeneous data   sources", "abstract": "information retrieval from distributed heterogeneous data sources remains a challenging issue. as the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. in this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. our goal is to provide an integrated view on biomedical information sources for the health-e-child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. in particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.", "date_create": "2007-07-05", "area": "cs.db", "authors": ["munir", "odeh", "mcclatchey", "khan", "habib"]}, {"idpaper": "0707.0748", "title": "experiences of engineering grid-based medical software", "abstract": "objectives: grid-based technologies are emerging as potential solutions for managing and collaborating distributed resources in the biomedical domain. few examples exist, however, of successful implementations of grid-enabled medical systems and even fewer have been deployed for evaluation in practice. the objective of this paper is to evaluate the use in clinical practice of a grid-based imaging prototype and to establish directions for engineering future medical grid developments and their subsequent deployment. method: the mammogrid project has deployed a prototype system for clinicians using the grid as its information infrastructure. to assist in the specification of the system requirements (and for the first time in healthgrid applications), use-case modelling has been carried out in close collaboration with clinicians and radiologists who had no prior experience of this modelling technique. a critical qualitative and, where possible, quantitative analysis of the mammogrid prototype is presented leading to a set of recommendations from the delivery of the first deployed grid-based medical imaging application. results: we report critically on the application of software engineering techniques in the specification and implementation of the mammogrid project and show that use-case modelling is a suitable vehicle for representing medical requirements and for communicating effectively with the clinical community. this paper also discusses the practical advantages and limitations of applying the grid to real-life clinical applications and presents the consequent lessons learned.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["estrella", "hauer", "mcclatchey", "odeh", "rogulin", "solomonides"]}, {"idpaper": "0707.0748", "title": "experiences of engineering grid-based medical software", "abstract": "objectives: grid-based technologies are emerging as potential solutions for managing and collaborating distributed resources in the biomedical domain. few examples exist, however, of successful implementations of grid-enabled medical systems and even fewer have been deployed for evaluation in practice. the objective of this paper is to evaluate the use in clinical practice of a grid-based imaging prototype and to establish directions for engineering future medical grid developments and their subsequent deployment. method: the mammogrid project has deployed a prototype system for clinicians using the grid as its information infrastructure. to assist in the specification of the system requirements (and for the first time in healthgrid applications), use-case modelling has been carried out in close collaboration with clinicians and radiologists who had no prior experience of this modelling technique. a critical qualitative and, where possible, quantitative analysis of the mammogrid prototype is presented leading to a set of recommendations from the delivery of the first deployed grid-based medical imaging application. results: we report critically on the application of software engineering techniques in the specification and implementation of the mammogrid project and show that use-case modelling is a suitable vehicle for representing medical requirements and for communicating effectively with the clinical community. this paper also discusses the practical advantages and limitations of applying the grid to real-life clinical applications and presents the consequent lessons learned.", "date_create": "2007-07-05", "area": "cs.dc", "authors": ["estrella", "hauer", "mcclatchey", "odeh", "rogulin", "solomonides"]}, {"idpaper": "0707.0761", "title": "managing separation of concerns in grid applications through   architectural model transformations", "abstract": "grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (vo). most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. as a consequence, grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific grid configurations, platforms and infra-structures. having neither guidelines nor rules in the design of a grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. it is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows grid develop-ments requirements. because grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. this paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of grids.", "date_create": "2007-07-05", "area": "cs.se cs.dc", "authors": ["manset", "verjus", "mcclatchey"]}, {"idpaper": "0707.0761", "title": "managing separation of concerns in grid applications through   architectural model transformations", "abstract": "grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (vo). most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. as a consequence, grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific grid configurations, platforms and infra-structures. having neither guidelines nor rules in the design of a grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. it is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows grid develop-ments requirements. because grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. this paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of grids.", "date_create": "2007-07-05", "area": "cs.se cs.dc", "authors": ["manset", "verjus", "mcclatchey"]}, {"idpaper": "0707.0796", "title": "performance of linear field reconstruction techniques with noise and   uncertain sensor locations", "abstract": "we consider a wireless sensor network, sampling a bandlimited field, described by a limited number of harmonics. sensor nodes are irregularly deployed over the area of interest or subject to random motion; in addition sensors measurements are affected by noise. our goal is to obtain a high quality reconstruction of the field, with the mean square error (mse) of the estimate as performance metric. in particular, we analytically derive the performance of several reconstruction/estimation techniques based on linear filtering. for each technique, we obtain the mse, as well as its asymptotic expression in the case where the field number of harmonics and the number of sensors grow to infinity, while their ratio is kept constant. through numerical simulations, we show the validity of the asymptotic analysis, even for a small number of sensors. we provide some novel guidelines for the design of sensor networks when many parameters, such as field bandwidth, number of sensors, reconstruction quality, sensor motion characteristics, and noise level of the measures, have to be traded off.", "date_create": "2007-07-05", "area": "cs.oh", "authors": ["nordio", "chiasserini", "viterbo"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0808", "title": "the cyborg astrobiologist: porting from a wearable computer to the   astrobiology phone-cam", "abstract": "we have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. this camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. we envision that the `astrobiology phone-cam' exploration system can be fruitfully used in other problem domains as well.", "date_create": "2007-07-05", "area": "cs.cv astro-ph cs.ai cs.ce cs.hc cs.ni cs.ro cs.se", "authors": ["bartolo", "mcguire", "camilleri", "spiteri", "borg", "farrugia", "ormo", "gomez-elvira", "rodriguez-manfredi", "diaz-martinez", "ritter", "haschke", "oesker", "ontrup"]}, {"idpaper": "0707.0860", "title": "on the minimum number of transmissions in single-hop wireless coding   networks", "abstract": "the advent of network coding presents promising opportunities in many areas of communication and networking. it has been recently shown that network coding technique can significantly increase the overall throughput of wireless networks by taking advantage of their broadcast nature. in wireless networks, each transmitted packet is broadcasted within a certain area and can be overheard by the neighboring nodes. when a node needs to transmit packets, it employs the opportunistic coding approach that uses the knowledge of what the node's neighbors have heard in order to reduce the number of transmissions. with this approach, each transmitted packet is a linear combination of the original packets over a certain finite field.   in this paper, we focus on the fundamental problem of finding the optimal encoding for the broadcasted packets that minimizes the overall number of transmissions. we show that this problem is np-complete over gf(2) and establish several fundamental properties of the optimal solution. we also propose a simple heuristic solution for the problem based on graph coloring and present some empirical results for random settings.", "date_create": "2007-07-05", "area": "cs.it cs.ni math.it", "authors": ["rouayheb", "chaudhry", "sprintson"]}, {"idpaper": "0707.0860", "title": "on the minimum number of transmissions in single-hop wireless coding   networks", "abstract": "the advent of network coding presents promising opportunities in many areas of communication and networking. it has been recently shown that network coding technique can significantly increase the overall throughput of wireless networks by taking advantage of their broadcast nature. in wireless networks, each transmitted packet is broadcasted within a certain area and can be overheard by the neighboring nodes. when a node needs to transmit packets, it employs the opportunistic coding approach that uses the knowledge of what the node's neighbors have heard in order to reduce the number of transmissions. with this approach, each transmitted packet is a linear combination of the original packets over a certain finite field.   in this paper, we focus on the fundamental problem of finding the optimal encoding for the broadcasted packets that minimizes the overall number of transmissions. we show that this problem is np-complete over gf(2) and establish several fundamental properties of the optimal solution. we also propose a simple heuristic solution for the problem based on graph coloring and present some empirical results for random settings.", "date_create": "2007-07-05", "area": "cs.it cs.ni math.it", "authors": ["rouayheb", "chaudhry", "sprintson"]}, {"idpaper": "0707.0969", "title": "resource allocation for wireless fading relay channels: max-min solution", "abstract": "as a basic information-theoretic model for fading relay channels, the parallel relay channel is first studied, for which lower and upper bounds on the capacity are derived. for the parallel relay channel with degraded subchannels, the capacity is established, and is further demonstrated via the gaussian case, for which the synchronized and asynchronized capacities are obtained. the capacity achieving power allocation at the source and relay nodes among the subchannels is characterized. the fading relay channel is then studied, for which resource allocations that maximize the achievable rates are obtained for both the full-duplex and half-duplex cases. capacities are established for fading relay channels that satisfy certain conditions.", "date_create": "2007-07-06", "area": "cs.it math.it", "authors": ["liang", "veeravalli", "poor"]}, {"idpaper": "0707.0978", "title": "when network coding and dirty paper coding meet in a cooperative ad hoc   network", "abstract": "we develop and analyze new cooperative strategies for ad hoc networks that are more spectrally efficient than classical df cooperative protocols. using analog network coding, our strategies preserve the practical half-duplex assumption but relax the orthogonality constraint. the introduction of interference due to non-orthogonality is mitigated thanks to precoding, in particular dirty paper coding. combined with smart power allocation, our cooperation strategies allow to save time and lead to more efficient use of bandwidth and to improved network throughput with respect to classical rdf/pdf.", "date_create": "2007-07-06", "area": "cs.it math.it", "authors": ["fawaz", "gesbert", "debbah"]}, {"idpaper": "0707.0978", "title": "when network coding and dirty paper coding meet in a cooperative ad hoc   network", "abstract": "we develop and analyze new cooperative strategies for ad hoc networks that are more spectrally efficient than classical df cooperative protocols. using analog network coding, our strategies preserve the practical half-duplex assumption but relax the orthogonality constraint. the introduction of interference due to non-orthogonality is mitigated thanks to precoding, in particular dirty paper coding. combined with smart power allocation, our cooperation strategies allow to save time and lead to more efficient use of bandwidth and to improved network throughput with respect to classical rdf/pdf.", "date_create": "2007-07-06", "area": "cs.it math.it", "authors": ["fawaz", "gesbert", "debbah"]}, {"idpaper": "0707.0978", "title": "when network coding and dirty paper coding meet in a cooperative ad hoc   network", "abstract": "we develop and analyze new cooperative strategies for ad hoc networks that are more spectrally efficient than classical df cooperative protocols. using analog network coding, our strategies preserve the practical half-duplex assumption but relax the orthogonality constraint. the introduction of interference due to non-orthogonality is mitigated thanks to precoding, in particular dirty paper coding. combined with smart power allocation, our cooperation strategies allow to save time and lead to more efficient use of bandwidth and to improved network throughput with respect to classical rdf/pdf.", "date_create": "2007-07-06", "area": "cs.it math.it", "authors": ["fawaz", "gesbert", "debbah"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1053", "title": "exploration via design and the cost of uncertainty in keyword auctions", "abstract": "we present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. this exploratory mechanism deviates only minimally from the mechanism being currently used by google and yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. in particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. this allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. we show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. we compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. we also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. sne). thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. we also discuss variations of the new mechanism such as truthful implementations.", "date_create": "2007-07-06", "area": "cs.gt", "authors": ["singh", "roychowdhury", "bradonji\u0107", "rezaei"]}, {"idpaper": "0707.1095", "title": "better algorithms and bounds for directed maximum leaf problems", "abstract": "the {\\sc directed maximum leaf out-branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. in this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. we show that   \\begin{itemize} \\item every strongly connected digraph $d$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $d$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $o(k\\log k)$; \\item it can be decided in time $2^{o(k\\log^2 k)}\\cdot n^{o(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   all improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", "date_create": "2007-07-07", "area": "cs.ds cs.dm", "authors": ["alon", "fomin", "gutin", "krivelevich", "saurabh"]}, {"idpaper": "0707.1095", "title": "better algorithms and bounds for directed maximum leaf problems", "abstract": "the {\\sc directed maximum leaf out-branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. in this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. we show that   \\begin{itemize} \\item every strongly connected digraph $d$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $d$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $o(k\\log k)$; \\item it can be decided in time $2^{o(k\\log^2 k)}\\cdot n^{o(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   all improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", "date_create": "2007-07-07", "area": "cs.ds cs.dm", "authors": ["alon", "fomin", "gutin", "krivelevich", "saurabh"]}, {"idpaper": "0707.1095", "title": "better algorithms and bounds for directed maximum leaf problems", "abstract": "the {\\sc directed maximum leaf out-branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. in this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. we show that   \\begin{itemize} \\item every strongly connected digraph $d$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $d$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $o(k\\log k)$; \\item it can be decided in time $2^{o(k\\log^2 k)}\\cdot n^{o(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   all improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", "date_create": "2007-07-07", "area": "cs.ds cs.dm", "authors": ["alon", "fomin", "gutin", "krivelevich", "saurabh"]}, {"idpaper": "0707.1095", "title": "better algorithms and bounds for directed maximum leaf problems", "abstract": "the {\\sc directed maximum leaf out-branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. in this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. we show that   \\begin{itemize} \\item every strongly connected digraph $d$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $d$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $o(k\\log k)$; \\item it can be decided in time $2^{o(k\\log^2 k)}\\cdot n^{o(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   all improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", "date_create": "2007-07-07", "area": "cs.ds cs.dm", "authors": ["alon", "fomin", "gutin", "krivelevich", "saurabh"]}, {"idpaper": "0707.1095", "title": "better algorithms and bounds for directed maximum leaf problems", "abstract": "the {\\sc directed maximum leaf out-branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. in this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. we show that   \\begin{itemize} \\item every strongly connected digraph $d$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $d$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $o(k\\log k)$; \\item it can be decided in time $2^{o(k\\log^2 k)}\\cdot n^{o(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize}   all improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", "date_create": "2007-07-07", "area": "cs.ds cs.dm", "authors": ["alon", "fomin", "gutin", "krivelevich", "saurabh"]}, {"idpaper": "0707.1295", "title": "efficient supervised learning in networks with binary synapses", "abstract": "recent experimental studies indicate that synaptic changes induced by neuronal activity are discrete jumps between a small number of stable states. learning in systems with discrete synapses is known to be a computationally hard problem. here, we study a neurobiologically plausible on-line learning algorithm that derives from belief propagation algorithms. we show that it performs remarkably well in a model neuron with binary synapses, and a finite number of `hidden' states per synapse, that has to learn a random classification task. such system is able to learn a number of associations close to the theoretical limit, in time which is sublinear in system size. this is to our knowledge the first on-line algorithm that is able to achieve efficiently a finite number of patterns learned per binary synapse. furthermore, we show that performance is optimal for a finite number of hidden states which becomes very small for sparse coding. the algorithm is similar to the standard `perceptron' learning algorithm, with an additional rule for synaptic transitions which occur only if a currently presented pattern is `barely correct'. in this case, the synaptic changes are meta-plastic only (change in hidden states and not in actual synaptic state), stabilizing the synapse in its current state. finally, we show that a system with two visible states and k hidden states is much more robust to noise than a system with k visible states. we suggest this rule is sufficiently simple to be easily implemented by neurobiological systems or in hardware.", "date_create": "2007-07-09", "area": "q-bio.nc cond-mat.stat-mech cs.ne q-bio.qm", "authors": ["baldassi", "braunstein", "brunel", "zecchina"]}, {"idpaper": "0707.1532", "title": "sorting and selection in posets", "abstract": "classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. in this paper, we study a more general setting, in which some pairs of objects are incomparable. this generalization is relevant in applications related to rankings in sports, college admissions, or conference submissions. it also has potential applications in biology, such as comparing the evolutionary fitness of different strains of bacteria, or understanding input-output relations among a set of metabolic reactions or the causal influences among a set of interacting genes or proteins. our results improve and extend results from two decades ago of faigle and tur\\'{a}n.   a measure of complexity of a partially ordered set (poset) is its width. our algorithms obtain information about a poset by queries that compare two elements. we present an algorithm that sorts, i.e. completely identifies, a width w poset of size n and has query complexity o(wn + nlog(n)), which is within a constant factor of the information-theoretic lower bound. we also show that a variant of mergesort has query complexity o(wn(log(n/w))) and total complexity o((w^2)nlog(n/w)). faigle and tur\\'{a}n have shown that the sorting problem has query complexity o(wn(log(n/w))) but did not address its total complexity.   for the related problem of determining the minimal elements of a poset, we give efficient deterministic and randomized algorithms with o(wn) query and total complexity, along with matching lower bounds for the query complexity up to a factor of 2. we generalize these results to the k-selection problem of determining the elements of height at most k. we also derive upper bounds on the total complexity of some other problems of a similar flavor.", "date_create": "2007-07-10", "area": "cs.ds cs.dm", "authors": ["daskalakis", "karp", "mossel", "riesenfeld", "verbin"]}, {"idpaper": "0707.1607", "title": "cactus framework: black holes to gamma ray bursts", "abstract": "gamma ray bursts (grbs) are intense narrowly-beamed flashes of gamma-rays of cosmological origin. they are among the most scientifically interesting astrophysical systems, and the riddle concerning their central engines and emission mechanisms is one of the most complex and challenging problems of astrophysics today. in this article we outline our petascale approach to the grb problem and discuss the computational toolkits and numerical codes that are currently in use and that will be scaled up to run on emerging petaflop scale computing platforms in the near future.   petascale computing will require additional ingredients over conventional parallelism. we consider some of the challenges which will be caused by future petascale architectures, and discuss our plans for the future development of the cactus framework and its applications to meet these challenges in order to profit from these new architectures.", "date_create": "2007-07-11", "area": "cs.dc", "authors": ["schnetter", "ott", "allen", "diener", "goodale", "radke", "seidel", "shalf"]}, {"idpaper": "0707.1618", "title": "the trade-offs with space time cube representation of spatiotemporal   patterns", "abstract": "space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. fast and correct analysis of such information is important in for instance geospatial and social visualization applications. information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a dataset to users. the argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. however, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. to fill this gap we report on a between-subjects experiment comparing novice users error rates and response times when answering a set of questions using either space time cube or a baseline 2d representation. for some simple questions the error rates were lower when using the baseline representation. for complex questions where the participants needed an overall understanding of the spatiotemporal structure of the dataset, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. these results provide an empirical foundation for the hypothesis that space time cube representation benefits users when analyzing complex spatiotemporal patterns.", "date_create": "2007-07-11", "area": "cs.hc cs.gr", "authors": ["kristensson", "dahlback", "anundi", "bjornstad", "gillberg", "haraldsson", "martensson", "nordvall", "stahl"]}, {"idpaper": "0707.1618", "title": "the trade-offs with space time cube representation of spatiotemporal   patterns", "abstract": "space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. fast and correct analysis of such information is important in for instance geospatial and social visualization applications. information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a dataset to users. the argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. however, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. to fill this gap we report on a between-subjects experiment comparing novice users error rates and response times when answering a set of questions using either space time cube or a baseline 2d representation. for some simple questions the error rates were lower when using the baseline representation. for complex questions where the participants needed an overall understanding of the spatiotemporal structure of the dataset, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. these results provide an empirical foundation for the hypothesis that space time cube representation benefits users when analyzing complex spatiotemporal patterns.", "date_create": "2007-07-11", "area": "cs.hc cs.gr", "authors": ["kristensson", "dahlback", "anundi", "bjornstad", "gillberg", "haraldsson", "martensson", "nordvall", "stahl"]}, {"idpaper": "0707.1714", "title": "sampling algorithms and coresets for lp regression", "abstract": "the lp regression problem takes as input a matrix $a \\in \\real^{n \\times d}$, a vector $b \\in \\real^n$, and a number $p \\in [1,\\infty)$, and it returns as output a number ${\\cal z}$ and a vector $x_{opt} \\in \\real^d$ such that ${\\cal z} = \\min_{x \\in \\real^d} ||ax -b||_p = ||ax_{opt}-b||_p$. in this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained ($n \\gg d$) version of this classical problem, for all $p \\in [1, \\infty)$. the first stage of our algorithm non-uniformly samples $\\hat{r}_1 = o(36^p d^{\\max\\{p/2+1, p\\}+1})$ rows of $a$ and the corresponding elements of $b$, and then it solves the lp regression problem on the sample; we prove this is an 8-approximation. the second stage of our algorithm uses the output of the first stage to resample $\\hat{r}_1/\\epsilon^2$ constraints, and then it solves the lp regression problem on the new sample; we prove this is a $(1+\\epsilon)$-approximation. our algorithm unifies, improves upon, and extends the existing algorithms for special cases of lp regression, namely $p = 1,2$. in course of proving our result, we develop two concepts--well-conditioned bases and subspace-preserving sampling--that are of independent interest.", "date_create": "2007-07-11", "area": "cs.ds", "authors": ["dasgupta", "drineas", "harb", "kumar", "mahoney"]}, {"idpaper": "0707.1716", "title": "numerical calculation with arbitrary precision", "abstract": "the vast use of computers on scientific numerical computation makes the awareness of the limited precision that these machines are able to provide us an essential matter. a limited and insufficient precision allied to the truncation and rounding errors may induce the user to incorrect interpretation of his/hers answer. in this work, we have developed a computational package to minimize this kind of error by offering arbitrary precision numbers and calculation. this is very important in physics where we can work with numbers too small and too big simultaneously.", "date_create": "2007-07-11", "area": "cs.na cs.ms", "authors": ["rodrigues", "da mota", "duarte"]}, {"idpaper": "0707.1820", "title": "understanding the properties of the bittorrent overlay", "abstract": "in this paper, we conduct extensive simulations to understand the properties of the overlay generated by bittorrent. we start by analyzing how the overlay properties impact the efficiency of bittorrent. we focus on the average peer set size (i.e., average number of neighbors), the time for a peer to reach its maximum peer set size, and the diameter of the overlay. in particular, we show that the later a peer arrives in a torrent, the longer it takes to reach its maximum peer set size. then, we evaluate the impact of the maximum peer set size, the maximum number of outgoing connections per peer, and the number of nated peers on the overlay properties. we show that bittorrent generates a robust overlay, but that this overlay is not a random graph. in particular, the connectivity of a peer to its neighbors depends on its arriving order in the torrent. we also show that a large number of nated peers significantly compromise the robustness of the overlay to attacks. finally, we evaluate the impact of peer exchange on the overlay properties, and we show that it generates a chain-like overlay with a large diameter, which will adversely impact the efficiency of large torrents.", "date_create": "2007-07-12", "area": "cs.ni", "authors": ["hamra", "legout", "barakat"]}, {"idpaper": "0707.1820", "title": "understanding the properties of the bittorrent overlay", "abstract": "in this paper, we conduct extensive simulations to understand the properties of the overlay generated by bittorrent. we start by analyzing how the overlay properties impact the efficiency of bittorrent. we focus on the average peer set size (i.e., average number of neighbors), the time for a peer to reach its maximum peer set size, and the diameter of the overlay. in particular, we show that the later a peer arrives in a torrent, the longer it takes to reach its maximum peer set size. then, we evaluate the impact of the maximum peer set size, the maximum number of outgoing connections per peer, and the number of nated peers on the overlay properties. we show that bittorrent generates a robust overlay, but that this overlay is not a random graph. in particular, the connectivity of a peer to its neighbors depends on its arriving order in the torrent. we also show that a large number of nated peers significantly compromise the robustness of the overlay to attacks. finally, we evaluate the impact of peer exchange on the overlay properties, and we show that it generates a chain-like overlay with a large diameter, which will adversely impact the efficiency of large torrents.", "date_create": "2007-07-12", "area": "cs.ni", "authors": ["hamra", "legout", "barakat"]}, {"idpaper": "0707.1859", "title": "on the degrees of freedom in cognitive radio channels", "abstract": "after receiving useful peer comments, we would like to withdraw this paper.", "date_create": "2007-07-12", "area": "cs.it math.it", "authors": ["devroye", "tarokh"]}, {"idpaper": "0707.2042", "title": "a distributed approach for access and visibility task under ergonomic   constraints with a manikin in a virtual reality environment", "abstract": "this paper presents a new method, based on a multi-agent system and on digital mock-up technology, to assess an efficient path planner for a manikin for access and visibility task under ergonomic constraints. in order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. this operator cooperates, in real-time, with several automatic local elementary agents. the result of this work validates solutions brought by digital mock-up and that can be applied to simulate maintenance task.", "date_create": "2007-07-13", "area": "cs.ro", "authors": ["bidault", "chablat", "chedmail", "pino"]}, {"idpaper": "0707.2042", "title": "a distributed approach for access and visibility task under ergonomic   constraints with a manikin in a virtual reality environment", "abstract": "this paper presents a new method, based on a multi-agent system and on digital mock-up technology, to assess an efficient path planner for a manikin for access and visibility task under ergonomic constraints. in order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. this operator cooperates, in real-time, with several automatic local elementary agents. the result of this work validates solutions brought by digital mock-up and that can be applied to simulate maintenance task.", "date_create": "2007-07-13", "area": "cs.ro", "authors": ["bidault", "chablat", "chedmail", "pino"]}, {"idpaper": "0707.2042", "title": "a distributed approach for access and visibility task under ergonomic   constraints with a manikin in a virtual reality environment", "abstract": "this paper presents a new method, based on a multi-agent system and on digital mock-up technology, to assess an efficient path planner for a manikin for access and visibility task under ergonomic constraints. in order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. this operator cooperates, in real-time, with several automatic local elementary agents. the result of this work validates solutions brought by digital mock-up and that can be applied to simulate maintenance task.", "date_create": "2007-07-13", "area": "cs.ro", "authors": ["bidault", "chablat", "chedmail", "pino"]}, {"idpaper": "0707.2090", "title": "a training based distributed non-coherent space-time coding strategy", "abstract": "unitary space-time modulation is known to be an efficient means to communicate over non-coherent multiple input multiple output (mimo) channels. in this letter, differential unitary space-time coding and non-coherent space-time coding for the training based approach of kim and tarokh are addressed. for this approach, necessary and sufficient conditions for multi-group decodability are derived in a simple way assuming a generalized likelihood ratio test receiver and a unitary codebook. extending kim and tarokh's approach for colocated mimo systems, a novel training based approach to distributed non-coherent space-time coding for wireless relay networks is proposed. an explicit construction of two-group decodable distributed non-coherent space-time codes achieving full cooperative diversity for all even number of relays is provided.", "date_create": "2007-07-13", "area": "cs.it math.it", "authors": ["rajan", "rajan"]}, {"idpaper": "0707.2265", "title": "separable convex optimization problems with linear ascending constraints", "abstract": "separable convex optimization problems with linear ascending inequality and equality constraints are addressed in this paper. under an ordering condition on the slopes of the functions at the origin, an algorithm that determines the optimum point in a finite number of steps is described. the optimum value is shown to be monotone with respect to a partial order on the constraint parameters. moreover, the optimum value is convex with respect to these parameters. examples motivated by optimizations for communication systems are used to illustrate the algorithm.", "date_create": "2007-07-16", "area": "cs.it math.it math.oc", "authors": ["padakandla", "sundaresan"]}, {"idpaper": "0707.2275", "title": "passive control architecture for virtual humans", "abstract": "in the present paper, we introduce a new control architecture aimed at driving virtual humans in interaction with virtual environments, by motion capture. it brings decoupling of functionalities, and also of stability thanks to passivity. we show projections can break passivity, and thus must be used carefully. our control scheme enables task space and internal control, contact, and joint limits management. thanks to passivity, it can be easily extended. besides, we introduce a new tool as for manikin's control, which makes it able to build passive projections, so as to guide the virtual manikin when sharp movements are needed.", "date_create": "2007-07-16", "area": "cs.ro", "authors": ["rennuit", "micaelli", "merlhiot", "andriot", "guillaume", "chevassus", "chablat", "chedmail"]}, {"idpaper": "0707.2630", "title": "multi-physics extension of openfmo framework", "abstract": "openfmo framework, an open-source software (oss) platform for fragment molecular orbital (fmo) method, is extended to multi-physics simulations (mps). after reviewing the several fmo implementations on distributed computer environments, the subsequent development planning corresponding to mps is presented. it is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.", "date_create": "2007-07-18", "area": "cs.dc physics.comp-ph", "authors": ["takami", "maki", "ooba", "inadomi", "honda", "susukita", "inoue", "kobayashi", "nogita", "aoyagi"]}, {"idpaper": "0707.2630", "title": "multi-physics extension of openfmo framework", "abstract": "openfmo framework, an open-source software (oss) platform for fragment molecular orbital (fmo) method, is extended to multi-physics simulations (mps). after reviewing the several fmo implementations on distributed computer environments, the subsequent development planning corresponding to mps is presented. it is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.", "date_create": "2007-07-18", "area": "cs.dc physics.comp-ph", "authors": ["takami", "maki", "ooba", "inadomi", "honda", "susukita", "inoue", "kobayashi", "nogita", "aoyagi"]}, {"idpaper": "0707.2630", "title": "multi-physics extension of openfmo framework", "abstract": "openfmo framework, an open-source software (oss) platform for fragment molecular orbital (fmo) method, is extended to multi-physics simulations (mps). after reviewing the several fmo implementations on distributed computer environments, the subsequent development planning corresponding to mps is presented. it is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.", "date_create": "2007-07-18", "area": "cs.dc physics.comp-ph", "authors": ["takami", "maki", "ooba", "inadomi", "honda", "susukita", "inoue", "kobayashi", "nogita", "aoyagi"]}, {"idpaper": "0707.2630", "title": "multi-physics extension of openfmo framework", "abstract": "openfmo framework, an open-source software (oss) platform for fragment molecular orbital (fmo) method, is extended to multi-physics simulations (mps). after reviewing the several fmo implementations on distributed computer environments, the subsequent development planning corresponding to mps is presented. it is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.", "date_create": "2007-07-18", "area": "cs.dc physics.comp-ph", "authors": ["takami", "maki", "ooba", "inadomi", "honda", "susukita", "inoue", "kobayashi", "nogita", "aoyagi"]}, {"idpaper": "0707.3269", "title": "international standard for a linguistic annotation framework", "abstract": "this paper describes the linguistic annotation framework under development within iso tc37 sc4 wg1. the linguistic annotation framework is intended to serve as a basis for harmonizing existing language resources as well as developing new ones.", "date_create": "2007-07-22", "area": "cs.cl", "authors": ["romary", "ide"]}, {"idpaper": "0707.3269", "title": "international standard for a linguistic annotation framework", "abstract": "this paper describes the linguistic annotation framework under development within iso tc37 sc4 wg1. the linguistic annotation framework is intended to serve as a basis for harmonizing existing language resources as well as developing new ones.", "date_create": "2007-07-22", "area": "cs.cl", "authors": ["romary", "ide"]}, {"idpaper": "0707.3461", "title": "lattices for distributed source coding: jointly gaussian sources and   reconstruction of a linear function", "abstract": "consider a pair of correlated gaussian sources (x1,x2). two separate encoders observe the two components and communicate compressed versions of their observations to a common decoder. the decoder is interested in reconstructing a linear combination of x1 and x2 to within a mean-square distortion of d. we obtain an inner bound to the optimal rate-distortion region for this problem. a portion of this inner bound is achieved by a scheme that reconstructs the linear function directly rather than reconstructing the individual components x1 and x2 first. this results in a better rate region for certain parameter values. our coding scheme relies on lattice coding techniques in contrast to more prevalent random coding arguments used to demonstrate achievable rate regions in information theory. we then consider the case of linear reconstruction of k sources and provide an inner bound to the optimal rate-distortion region. some parts of the inner bound are achieved using the following coding structure: lattice vector quantization followed by \"correlated\" lattice-structured binning.", "date_create": "2007-07-23", "area": "cs.it math.it", "authors": ["krithivasan", "pradhan"]}, {"idpaper": "0707.3531", "title": "e-science initiatives in venezuela", "abstract": "within the context of the nascent e-science infrastructure in venezuela, we describe several web-based scientific applications developed at the centro nacional de calculo cientifico universidad de los andes (cecalcula), merida, and at the instituto venezolano de investigaciones cientificas (ivic), caracas. the different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. we also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{e-infrastructure shared between europe and latin america} (eela) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", "date_create": "2007-07-24", "area": "cs.ce cs.dc", "authors": ["chaves", "diaz", "hamar", "isea", "rojas", "ruiz", "torrens", "uzcategui", "florez-lopez", "hoeger", "mendoza", "nunez"]}, {"idpaper": "0707.3531", "title": "e-science initiatives in venezuela", "abstract": "within the context of the nascent e-science infrastructure in venezuela, we describe several web-based scientific applications developed at the centro nacional de calculo cientifico universidad de los andes (cecalcula), merida, and at the instituto venezolano de investigaciones cientificas (ivic), caracas. the different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. we also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{e-infrastructure shared between europe and latin america} (eela) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", "date_create": "2007-07-24", "area": "cs.ce cs.dc", "authors": ["chaves", "diaz", "hamar", "isea", "rojas", "ruiz", "torrens", "uzcategui", "florez-lopez", "hoeger", "mendoza", "nunez"]}, {"idpaper": "0707.3531", "title": "e-science initiatives in venezuela", "abstract": "within the context of the nascent e-science infrastructure in venezuela, we describe several web-based scientific applications developed at the centro nacional de calculo cientifico universidad de los andes (cecalcula), merida, and at the instituto venezolano de investigaciones cientificas (ivic), caracas. the different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. we also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{e-infrastructure shared between europe and latin america} (eela) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", "date_create": "2007-07-24", "area": "cs.ce cs.dc", "authors": ["chaves", "diaz", "hamar", "isea", "rojas", "ruiz", "torrens", "uzcategui", "florez-lopez", "hoeger", "mendoza", "nunez"]}, {"idpaper": "0707.3531", "title": "e-science initiatives in venezuela", "abstract": "within the context of the nascent e-science infrastructure in venezuela, we describe several web-based scientific applications developed at the centro nacional de calculo cientifico universidad de los andes (cecalcula), merida, and at the instituto venezolano de investigaciones cientificas (ivic), caracas. the different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. we also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{e-infrastructure shared between europe and latin america} (eela) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", "date_create": "2007-07-24", "area": "cs.ce cs.dc", "authors": ["chaves", "diaz", "hamar", "isea", "rojas", "ruiz", "torrens", "uzcategui", "florez-lopez", "hoeger", "mendoza", "nunez"]}, {"idpaper": "0707.3531", "title": "e-science initiatives in venezuela", "abstract": "within the context of the nascent e-science infrastructure in venezuela, we describe several web-based scientific applications developed at the centro nacional de calculo cientifico universidad de los andes (cecalcula), merida, and at the instituto venezolano de investigaciones cientificas (ivic), caracas. the different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. we also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{e-infrastructure shared between europe and latin america} (eela) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", "date_create": "2007-07-24", "area": "cs.ce cs.dc", "authors": ["chaves", "diaz", "hamar", "isea", "rojas", "ruiz", "torrens", "uzcategui", "florez-lopez", "hoeger", "mendoza", "nunez"]}, {"idpaper": "0707.3584", "title": "the effect of fading, channel inversion, and threshold scheduling on ad   hoc networks", "abstract": "this paper addresses three issues in the field of ad hoc network capacity: the impact of i)channel fading, ii) channel inversion power control, and iii) threshold-based scheduling on capacity. channel inversion and threshold scheduling may be viewed as simple ways to exploit channel state information (csi) without requiring cooperation across transmitters. we use the transmission capacity (tc) as our metric, defined as the maximum spatial intensity of successful simultaneous transmissions subject to a constraint on the outage probability (op). by assuming the nodes are located on the infinite plane according to a poisson process, we are able to employ tools from stochastic geometry to obtain asymptotically tight bounds on the distribution of the signal-to-interference (sir) level, yielding in turn tight bounds on the op (relative to a given sir threshold) and the tc. we demonstrate that in the absence of csi, fading can significantly reduce the tc and somewhat surprisingly, channel inversion only makes matters worse. we develop a threshold-based transmission rule where transmitters are active only if the channel to their receiver is acceptably strong, obtain expressions for the optimal threshold, and show that this simple, fully distributed scheme can significantly reduce the effect of fading.", "date_create": "2007-07-24", "area": "cs.it math.it", "authors": ["weber", "andrews", "jindal"]}, {"idpaper": "0707.3717", "title": "gcp: gossip-based code propagation for large-scale mobile wireless   sensor networks", "abstract": "wireless sensor networks (wsn) have recently received an increasing interest. they are now expected to be deployed for long periods of time, thus requiring software updates. updating the software code automatically on a huge number of sensors is a tremendous task, as ''by hand'' updates can obviously not be considered, especially when all participating sensors are embedded on mobile entities. in this paper, we investigate an approach to automatically update software in mobile sensor-based application when no localization mechanism is available. we leverage the peer-to-peer cooperation paradigm to achieve a good trade-off between reliability and scalability of code propagation. more specifically, we present the design and evaluation of gcp ({\\emph gossip-based code propagation}), a distributed software update algorithm for mobile wireless sensor networks. gcp relies on two different mechanisms (piggy-backing and forwarding control) to improve significantly the load balance without sacrificing on the propagation speed. we compare gcp against traditional dissemination approaches. simulation results based on both synthetic and realistic workloads show that gcp achieves a good convergence speed while balancing the load evenly between sensors.", "date_create": "2007-07-25", "area": "cs.ni", "authors": ["busnel", "bertier", "fleury", "kermarrec"]}, {"idpaper": "0707.3717", "title": "gcp: gossip-based code propagation for large-scale mobile wireless   sensor networks", "abstract": "wireless sensor networks (wsn) have recently received an increasing interest. they are now expected to be deployed for long periods of time, thus requiring software updates. updating the software code automatically on a huge number of sensors is a tremendous task, as ''by hand'' updates can obviously not be considered, especially when all participating sensors are embedded on mobile entities. in this paper, we investigate an approach to automatically update software in mobile sensor-based application when no localization mechanism is available. we leverage the peer-to-peer cooperation paradigm to achieve a good trade-off between reliability and scalability of code propagation. more specifically, we present the design and evaluation of gcp ({\\emph gossip-based code propagation}), a distributed software update algorithm for mobile wireless sensor networks. gcp relies on two different mechanisms (piggy-backing and forwarding control) to improve significantly the load balance without sacrificing on the propagation speed. we compare gcp against traditional dissemination approaches. simulation results based on both synthetic and realistic workloads show that gcp achieves a good convergence speed while balancing the load evenly between sensors.", "date_create": "2007-07-25", "area": "cs.ni", "authors": ["busnel", "bertier", "fleury", "kermarrec"]}, {"idpaper": "0707.3750", "title": "recent advances in solving the protein threading problem", "abstract": "the fold recognition methods are promissing tools for capturing the structure of a protein by its amino acid residues sequence but their use is still restricted by the needs of huge computational resources and suitable efficient algorithms as well. in the recent version of frost (fold recognition oriented search tool) package the most efficient algorithm for solving the protein threading problem (ptp) is implemented due to the strong collaboration between the symbiose group in irisa and mig in jouy-en-josas. in this paper, we present the diverse components of frost, emphasizing on the recent advances in formulating and solving new versions of the ptp and on the way of solving on a computer cluster a million of instances in a easonable time.", "date_create": "2007-07-25", "area": "q-bio.qm cs.dc", "authors": ["andonov", "collet", "gibrat", "marin", "poirriez", "yanev"]}, {"idpaper": "0707.4289", "title": "a leaf recognition algorithm for plant classification using   probabilistic neural network", "abstract": "in this paper, we employ probabilistic neural network (pnn) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the pnn. the pnn is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", "date_create": "2007-07-29", "area": "cs.ai", "authors": ["wu", "bao", "xu", "wang", "chang", "xiang"]}, {"idpaper": "0707.4289", "title": "a leaf recognition algorithm for plant classification using   probabilistic neural network", "abstract": "in this paper, we employ probabilistic neural network (pnn) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the pnn. the pnn is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", "date_create": "2007-07-29", "area": "cs.ai", "authors": ["wu", "bao", "xu", "wang", "chang", "xiang"]}, {"idpaper": "0707.4289", "title": "a leaf recognition algorithm for plant classification using   probabilistic neural network", "abstract": "in this paper, we employ probabilistic neural network (pnn) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the pnn. the pnn is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", "date_create": "2007-07-29", "area": "cs.ai", "authors": ["wu", "bao", "xu", "wang", "chang", "xiang"]}, {"idpaper": "0707.4289", "title": "a leaf recognition algorithm for plant classification using   probabilistic neural network", "abstract": "in this paper, we employ probabilistic neural network (pnn) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the pnn. the pnn is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", "date_create": "2007-07-29", "area": "cs.ai", "authors": ["wu", "bao", "xu", "wang", "chang", "xiang"]}, {"idpaper": "0707.4289", "title": "a leaf recognition algorithm for plant classification using   probabilistic neural network", "abstract": "in this paper, we employ probabilistic neural network (pnn) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the pnn. the pnn is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", "date_create": "2007-07-29", "area": "cs.ai", "authors": ["wu", "bao", "xu", "wang", "chang", "xiang"]}, {"idpaper": "0707.4298", "title": "a note on equipartition", "abstract": "the problem of the existence of an equi-partition of a curve in $\\r^n$ has recently been raised in the context of computational geometry. the problem is to show that for a (continuous) curve $\\gamma : [0,1] \\to \\r^n$ and for any positive integer n, there exist points $t_0=0<t_1<...<t_{n-1}<1=t_n$, such that $d(\\gamma(t_{i-1}),\\gamma(t_i))=d(\\gamma(t_{i}),\\gamma(t_{i+1}))$ for all $i=1,...,n$, where d is a metric or even a semi-metric (a weaker notion) on $\\r^n$. we show here that the existence of such points, in a broader context, is a consequence of brower's fixed point theorem.", "date_create": "2007-07-29", "area": "cs.cg math.fa", "authors": ["lopez", "reisner"]}, {"idpaper": "0707.4298", "title": "a note on equipartition", "abstract": "the problem of the existence of an equi-partition of a curve in $\\r^n$ has recently been raised in the context of computational geometry. the problem is to show that for a (continuous) curve $\\gamma : [0,1] \\to \\r^n$ and for any positive integer n, there exist points $t_0=0<t_1<...<t_{n-1}<1=t_n$, such that $d(\\gamma(t_{i-1}),\\gamma(t_i))=d(\\gamma(t_{i}),\\gamma(t_{i+1}))$ for all $i=1,...,n$, where d is a metric or even a semi-metric (a weaker notion) on $\\r^n$. we show here that the existence of such points, in a broader context, is a consequence of brower's fixed point theorem.", "date_create": "2007-07-29", "area": "cs.cg math.fa", "authors": ["lopez", "reisner"]}, {"idpaper": "0707.4304", "title": "spatial aggregation: data model and implementation", "abstract": "data aggregation in geographic information systems (gis) is only marginally present in commercial systems nowadays, mostly through ad-hoc solutions. in this paper, we first present a formal model for representing spatial data. this model integrates geographic data and information contained in data warehouses external to the gis. we define the notion of geometric aggregation, a general framework for aggregate queries in a gis setting. we also identify the class of summable queries, which can be efficiently evaluated by precomputing the overlay of two or more of the thematic layers involved in the query. we also sketch a language, denoted gisolap-ql, for expressing queries that involve gis and olap features. in addition, we introduce piet, an implementation of our proposal, that makes use of overlay precomputation for answering spatial queries (aggregate or not). our experimental evaluation showed that for a certain class of geometric queries with or without aggregation, overlay precomputation outperforms r-tree-based techniques. finally, as a particular application of our proposal, we study topological queries.", "date_create": "2007-07-29", "area": "cs.db", "authors": ["gomez", "haesevoets", "kuijpers", "vaisman"]}, {"idpaper": "0707.4518", "title": "on throughput scaling of wireless networks: effect of node density and   propagation model", "abstract": "this paper derives a lower bound to the per-node throughput achievable by a wireless network when n source-destination pairs are randomly distributed throughout a disk of radius $n^\\gamma$, $ \\gamma \\geq 0$, propagation is modeled by attenuation of the form $1/(1+d)^\\alpha$, $\\alpha >2$, and successful transmission occurs at a fixed rate w when received signal to noise and interference ratio is greater than some threshold $\\beta$, and at rate 0 otherwise. the lower bound has the form $n^{1-\\gamma}$ when $\\gamma < 1/2$, and $(n \\ln n)^{-1/2}$ when $\\gamma \\geq 1/2$. the methods are similar to, but somewhat simpler than, those in the seminal paper by gupta and kumar.", "date_create": "2007-07-30", "area": "cs.it math.it", "authors": ["duarte-melo", "josan", "liu", "neuhoff", "pradhan"]}, {"idpaper": "0707.4618", "title": "nonlinear matroid optimization and experimental design", "abstract": "we study the problem of optimizing nonlinear objective functions over matroids presented by oracles or explicitly. such functions can be interpreted as the balancing of multi-criteria optimization. we provide a combinatorial polynomial time algorithm for arbitrary oracle-presented matroids, that makes repeated use of matroid intersection, and an algebraic algorithm for vectorial matroids.   our work is partly motivated by applications to minimum-aberration model-fitting in experimental design in statistics, which we discuss and demonstrate in detail.", "date_create": "2007-07-31", "area": "math.co cs.cc cs.dm math.oc", "authors": ["berstein", "lee", "maruri-aguilar", "onn", "riccomagno", "weismantel", "wynn"]}, {"idpaper": "0707.4656", "title": "communication under strong asynchronism", "abstract": "we consider asynchronous communication over point-to-point discrete memoryless channels. the transmitter starts sending one block codeword at an instant that is uniformly distributed within a certain time period, which represents the level of asynchronism. the receiver, by means of a sequential decoder, must isolate the message without knowing when the codeword transmission starts but being cognizant of the asynchronism level a. we are interested in how quickly can the receiver isolate the sent message, particularly in the regime where a is exponentially larger than the codeword length n, which we refer to as `strong asynchronism.'   this model of sparse communication may represent the situation of a sensor that remains idle most of the time and, only occasionally, transmits information to a remote base station which needs to quickly take action.   the first result shows that vanishing error probability can be guaranteed as n tends to infinity while a grows as exp(n*k) if and only if k does not exceed the `synchronization threshold,' a constant that admits a simple closed form expression, and is at least as large as the capacity of the synchronized channel. the second result is the characterization of a set of achievable strictly positive rates in the regime where a is exponential in n, and where the rate is defined with respect to the expected delay between the time information starts being emitted until the time the receiver makes a decision.   as an application of the first result we consider antipodal signaling over a gaussian channel and derive a simple necessary condition between a, n, and snr for achieving reliable communication.", "date_create": "2007-07-31", "area": "cs.it math.it", "authors": ["tchamkerten", "chandar", "wornell"]}, {"idpaper": "0707.4659", "title": "difference equations in massive higher order calculations", "abstract": "the calculation of massive 2--loop operator matrix elements, required for the higher order wilson coefficients for heavy flavor production in deeply inelastic scattering, leads to new types of multiple infinite sums over harmonic sums and related functions, which depend on the mellin parameter $n$. we report on the solution of these sums through higher order difference equations using the summation package {\\tt sigma}.", "date_create": "2007-07-31", "area": "math-ph cs.ms hep-ph math.mp", "authors": ["bierenbaum", "bl\u00fcmlein", "klein", "schneider"]}, {"idpaper": "0707.4659", "title": "difference equations in massive higher order calculations", "abstract": "the calculation of massive 2--loop operator matrix elements, required for the higher order wilson coefficients for heavy flavor production in deeply inelastic scattering, leads to new types of multiple infinite sums over harmonic sums and related functions, which depend on the mellin parameter $n$. we report on the solution of these sums through higher order difference equations using the summation package {\\tt sigma}.", "date_create": "2007-07-31", "area": "math-ph cs.ms hep-ph math.mp", "authors": ["bierenbaum", "bl\u00fcmlein", "klein", "schneider"]}, {"idpaper": "0708.0603", "title": "public cluster : parallel machine with multi-block approach", "abstract": "we introduce a new approach to enable an open and public parallel machine which is accessible for multi users with multi jobs belong to different blocks running at the same time. the concept is required especially for parallel machines which are dedicated for public use as implemented at the lipi public cluster. we have deployed the simplest technique by running multi daemons of parallel processing engine with different configuration files specified for each user assigned to access the system, and also developed an integrated system to fully control and monitor the whole system over web. a brief performance analysis is also given for message parsing interface (mpi) engine. it is shown that the proposed approach is quite reliable and affect the whole performances only slightly.", "date_create": "2007-08-04", "area": "cs.dc cs.cy", "authors": ["akbar", "slamet", "ajinagoro", "ohara", "firmansyah", "hermanto", "handoko"]}, {"idpaper": "0708.0694", "title": "reconstruction of protein-protein interaction pathways by mining   subject-verb-objects intermediates", "abstract": "the exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. this has prompted the use of text mining tools to extract key biological information. previous studies have reported extensive modification of existing generic text processors to process biological text. however, this requirement for modification had not been examined. in this study, we have constructed muscorian, using montylingua, a generic text processor. it uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.", "date_create": "2007-08-05", "area": "cs.ir cs.cl cs.dl", "authors": ["ling", "lefevre", "nicholas", "lin"]}, {"idpaper": "0708.1150", "title": "a practical ontology for the large-scale modeling of scholarly artifacts   and their usage", "abstract": "the large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving, privacy issues concerned with the dissemination of usage data, and the lack of a practical ontology for modeling the usage domain. as a remedy to the third constraint, this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists, supports usage research, and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts (e.g. authors and journals) and an accompanying 1 billion usage events. the real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support. we present the ontology, discuss its instantiation, and provide some example inference rules for calculating various scholarly artifact metrics.", "date_create": "2007-08-08", "area": "cs.dl cs.ai", "authors": ["rodriguez", "bollen", "van de sompel"]}, {"idpaper": "0708.1579", "title": "homogeneous temporal activity patterns in a large online communication   space", "abstract": "the many-to-many social communication activity on the popular technology-news website slashdot has been studied. we have concentrated on the dynamics of message production without considering semantic relations and have found regular temporal patterns in the reaction time of the community to a news-post as well as in single user behavior. the statistics of these activities follow log-normal distributions. daily and weekly oscillatory cycles, which cause slight variations of this simple behavior, are identified. a superposition of two log-normal distributions can account for these variations. the findings are remarkable since the distribution of the number of comments per users, which is also analyzed, indicates a great amount of heterogeneity in the community. the reader may find surprising that only a few parameters allow a detailed description, or even prediction, of social many-to-many information exchange in this kind of popular public spaces.", "date_create": "2007-08-11", "area": "cs.ni", "authors": ["kaltenbrunner", "g\u00f3mez", "moghnieh", "meza", "blat", "l\u00f3pez"]}, {"idpaper": "0708.1579", "title": "homogeneous temporal activity patterns in a large online communication   space", "abstract": "the many-to-many social communication activity on the popular technology-news website slashdot has been studied. we have concentrated on the dynamics of message production without considering semantic relations and have found regular temporal patterns in the reaction time of the community to a news-post as well as in single user behavior. the statistics of these activities follow log-normal distributions. daily and weekly oscillatory cycles, which cause slight variations of this simple behavior, are identified. a superposition of two log-normal distributions can account for these variations. the findings are remarkable since the distribution of the number of comments per users, which is also analyzed, indicates a great amount of heterogeneity in the community. the reader may find surprising that only a few parameters allow a detailed description, or even prediction, of social many-to-many information exchange in this kind of popular public spaces.", "date_create": "2007-08-11", "area": "cs.ni", "authors": ["kaltenbrunner", "g\u00f3mez", "moghnieh", "meza", "blat", "l\u00f3pez"]}, {"idpaper": "0708.1818", "title": "computational simulation and 3d virtual reality engineering tools for   dynamical modeling and imaging of composite nanomaterials", "abstract": "an adventure at engineering design and modeling is possible with a virtual reality environment (vre) that uses multiple computer-generated media to let a user experience situations that are temporally and spatially prohibiting. in this paper, an approach to developing some advanced architecture and modeling tools is presented to allow multiple frameworks work together while being shielded from the application program. this architecture is being developed in a framework of workbench interactive tools for next generation nanoparticle-reinforced damping/dynamic systems. through the use of system, an engineer/programmer can respectively concentrate on tailoring an engineering design concept of novel system and the application software design while using existing databases/software outputs.", "date_create": "2007-08-14", "area": "cs.ce cond-mat.other", "authors": ["bochkareva", "kireitseu", "tomlinson", "altenbach", "kompis", "hui"]}, {"idpaper": "0708.2021", "title": "who is the best connected ec researcher? centrality analysis of the   complex network of authors in evolutionary computation", "abstract": "co-authorship graphs (that is, the graph of authors linked by co-authorship of papers) are complex networks, which expresses the dynamics of a complex system. only recently its study has started to draw interest from the ec community, the first paper dealing with it having been published two years ago. in this paper we will study the co-authorship network of ec at a microscopic level. our objective is ascertaining which are the most relevant nodes (i.e. authors) in it. for this purpose, we examine several metrics defined in the complex-network literature, and analyze them both in isolation and combined within a pareto-dominance approach. the result of our analysis indicates that there are some well-known researchers that appear systematically in top rankings. this also provides some hints on the social behavior of our community.", "date_create": "2007-08-15", "area": "cs.cy cs.ne", "authors": ["merelo", "cotta"]}, {"idpaper": "0708.2021", "title": "who is the best connected ec researcher? centrality analysis of the   complex network of authors in evolutionary computation", "abstract": "co-authorship graphs (that is, the graph of authors linked by co-authorship of papers) are complex networks, which expresses the dynamics of a complex system. only recently its study has started to draw interest from the ec community, the first paper dealing with it having been published two years ago. in this paper we will study the co-authorship network of ec at a microscopic level. our objective is ascertaining which are the most relevant nodes (i.e. authors) in it. for this purpose, we examine several metrics defined in the complex-network literature, and analyze them both in isolation and combined within a pareto-dominance approach. the result of our analysis indicates that there are some well-known researchers that appear systematically in top rankings. this also provides some hints on the social behavior of our community.", "date_create": "2007-08-15", "area": "cs.cy cs.ne", "authors": ["merelo", "cotta"]}, {"idpaper": "0708.2026", "title": "derivative of bicm mutual information", "abstract": "in this letter we determine the derivative of the mutual information corresponding to bit-interleaved coded modulation systems. the derivative follows as a linear combination of minimum-mean-squared error functions of coded modulation sets. the result finds applications to the analysis of communications systems in the wideband regime and to the design of power allocation over parallel channels.", "date_create": "2007-08-15", "area": "cs.it math.it", "authors": ["fabregas", "martinez"]}, {"idpaper": "0708.2173", "title": "provenance as dependency analysis", "abstract": "provenance is information recording the source, derivation, or history of some information. provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. in this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. we introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.", "date_create": "2007-08-16", "area": "cs.db cs.pl", "authors": ["cheney", "ahmed", "acar"]}, {"idpaper": "0708.2575", "title": "rateless coding for gaussian channels", "abstract": "a rateless code-i.e., a rate-compatible family of codes-has the property that codewords of the higher rate codes are prefixes of those of the lower rate ones. a perfect family of such codes is one in which each of the codes in the family is capacity-achieving. we show by construction that perfect rateless codes with low-complexity decoding algorithms exist for additive white gaussian noise channels. our construction involves the use of layered encoding and successive decoding, together with repetition using time-varying layer weights. as an illustration of our framework, we design a practical three-rate code family. we further construct rich sets of near-perfect rateless codes within our architecture that require either significantly fewer layers or lower complexity than their perfect counterparts. variations of the basic construction are also developed, including one for time-varying channels in which there is no a priori stochastic model.", "date_create": "2007-08-19", "area": "cs.it math.it", "authors": ["erez", "trott", "wornell"]}, {"idpaper": "0708.2616", "title": "an experimental investigation of secure communication with chaos masking", "abstract": "the most exciting recent development in nonlinear dynamics is realization that chaos can be useful. one application involves \"secure communication\". two piecewise linear systems with switching nonlinearities have been taken as chaos generators. in the present work the phenomenon of secure communication with chaos masking has been investigated experimentally. in this investigation chaos which is generated from two chaos generators is masked with the massage signal to be transmitted, thus makes communication is more secure.", "date_create": "2007-08-20", "area": "cs.cr", "authors": ["dhar", "chakraborty"]}, {"idpaper": "0708.2893", "title": "fast recursive coding based on grouping of symbols", "abstract": "a novel fast recursive coding technique is proposed. it operates with only integer values not longer 8 bits and is multiplication free. recursion the algorithm is based on indirectly provides rather effective coding of symbols for very large alphabets. the code length for the proposed technique can be up to 20-30% less than for arithmetic coding and, in the worst case it is only by 1-3% larger.", "date_create": "2007-08-21", "area": "cs.it math.it", "authors": ["ponomarenko", "lukin", "egiazarian", "astola", "ryabko"]}, {"idpaper": "0708.3070", "title": "network coding capacity of random wireless networks under a   signal-to-interference-and-noise model", "abstract": "in this paper, we study network coding capacity for random wireless networks. previous work on network coding capacity for wired and wireless networks have focused on the case where the capacities of links in the network are independent. in this paper, we consider a more realistic model, where wireless networks are modeled by random geometric graphs with interference and noise. in this model, the capacities of links are not independent. we consider two scenarios, single source multiple destinations and multiple sources multiple destinations. in the first scenario, employing coupling and martingale methods, we show that the network coding capacity for random wireless networks still exhibits a concentration behavior around the mean value of the minimum cut under some mild conditions. furthermore, we establish upper and lower bounds on the network coding capacity for dependent and independent nodes. in the second one, we also show that the network coding capacity still follows a concentration behavior. our simulation results confirm our theoretical predictions.", "date_create": "2007-08-22", "area": "cs.it math.it", "authors": ["kong", "aly", "soljanin", "yeh", "klappenecker"]}, {"idpaper": "0708.3070", "title": "network coding capacity of random wireless networks under a   signal-to-interference-and-noise model", "abstract": "in this paper, we study network coding capacity for random wireless networks. previous work on network coding capacity for wired and wireless networks have focused on the case where the capacities of links in the network are independent. in this paper, we consider a more realistic model, where wireless networks are modeled by random geometric graphs with interference and noise. in this model, the capacities of links are not independent. we consider two scenarios, single source multiple destinations and multiple sources multiple destinations. in the first scenario, employing coupling and martingale methods, we show that the network coding capacity for random wireless networks still exhibits a concentration behavior around the mean value of the minimum cut under some mild conditions. furthermore, we establish upper and lower bounds on the network coding capacity for dependent and independent nodes. in the second one, we also show that the network coding capacity still follows a concentration behavior. our simulation results confirm our theoretical predictions.", "date_create": "2007-08-22", "area": "cs.it math.it", "authors": ["kong", "aly", "soljanin", "yeh", "klappenecker"]}, {"idpaper": "0708.3464", "title": "a non parametric study of the volatility of the economy as a country   risk predictor", "abstract": "this paper intends to explain venezuela's country spread behavior through the neural networks analysis of a monthly economic activity general index of economic indicators constructed by the central bank of venezuela, a measure of the shocks affecting country risk of emerging markets and the u.s. short term interest rate. the use of non parametric methods allowed the finding of non linear relationship between these inputs and the country risk. the networks performance was evaluated using the method of excess predictability.", "date_create": "2007-08-26", "area": "cs.ce cs.ne", "authors": ["costanzo", "trigo", "dominguez", "moreno"]}, {"idpaper": "0708.3522", "title": "bounding the betti numbers and computing the euler-poincar\\'e   characteristic of semi-algebraic sets defined by partly quadratic systems of   polynomials", "abstract": "let $\\r$ be a real closed field, $ {\\mathcal q} \\subset \\r[y_1,...,y_\\ell,x_1,...,x_k], $ with $ \\deg_{y}(q) \\leq 2, \\deg_{x}(q) \\leq d, q \\in {\\mathcal q}, #({\\mathcal q})=m,$ and $ {\\mathcal p} \\subset \\r[x_1,...,x_k] $ with $\\deg_{x}(p) \\leq d, p \\in {\\mathcal p}, #({\\mathcal p})=s$, and $s \\subset \\r^{\\ell+k}$ a semi-algebraic set defined by a boolean formula without negations, with atoms $p=0, p \\geq 0, p \\leq 0, p \\in {\\mathcal p} \\cup {\\mathcal q}$. we prove that the sum of the betti numbers of $s$ is bounded by \\[ \\ell^2 (o(s+\\ell+m)\\ell d)^{k+2m}. \\] this is a common generalization of previous results on bounding the betti numbers of closed semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.   we also describe an algorithm for computing the euler-poincar\\'e characteristic of such sets, generalizing similar algorithms known before. the complexity of the algorithm is bounded by $(\\ell s m d)^{o(m(m+k))}$.", "date_create": "2007-08-26", "area": "math.ag cs.sc math.at math.gt", "authors": ["basu", "pasechnik", "roy"]}, {"idpaper": "0708.3522", "title": "bounding the betti numbers and computing the euler-poincar\\'e   characteristic of semi-algebraic sets defined by partly quadratic systems of   polynomials", "abstract": "let $\\r$ be a real closed field, $ {\\mathcal q} \\subset \\r[y_1,...,y_\\ell,x_1,...,x_k], $ with $ \\deg_{y}(q) \\leq 2, \\deg_{x}(q) \\leq d, q \\in {\\mathcal q}, #({\\mathcal q})=m,$ and $ {\\mathcal p} \\subset \\r[x_1,...,x_k] $ with $\\deg_{x}(p) \\leq d, p \\in {\\mathcal p}, #({\\mathcal p})=s$, and $s \\subset \\r^{\\ell+k}$ a semi-algebraic set defined by a boolean formula without negations, with atoms $p=0, p \\geq 0, p \\leq 0, p \\in {\\mathcal p} \\cup {\\mathcal q}$. we prove that the sum of the betti numbers of $s$ is bounded by \\[ \\ell^2 (o(s+\\ell+m)\\ell d)^{k+2m}. \\] this is a common generalization of previous results on bounding the betti numbers of closed semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.   we also describe an algorithm for computing the euler-poincar\\'e characteristic of such sets, generalizing similar algorithms known before. the complexity of the algorithm is bounded by $(\\ell s m d)^{o(m(m+k))}$.", "date_create": "2007-08-26", "area": "math.ag cs.sc math.at math.gt", "authors": ["basu", "pasechnik", "roy"]}, {"idpaper": "0708.3522", "title": "bounding the betti numbers and computing the euler-poincar\\'e   characteristic of semi-algebraic sets defined by partly quadratic systems of   polynomials", "abstract": "let $\\r$ be a real closed field, $ {\\mathcal q} \\subset \\r[y_1,...,y_\\ell,x_1,...,x_k], $ with $ \\deg_{y}(q) \\leq 2, \\deg_{x}(q) \\leq d, q \\in {\\mathcal q}, #({\\mathcal q})=m,$ and $ {\\mathcal p} \\subset \\r[x_1,...,x_k] $ with $\\deg_{x}(p) \\leq d, p \\in {\\mathcal p}, #({\\mathcal p})=s$, and $s \\subset \\r^{\\ell+k}$ a semi-algebraic set defined by a boolean formula without negations, with atoms $p=0, p \\geq 0, p \\leq 0, p \\in {\\mathcal p} \\cup {\\mathcal q}$. we prove that the sum of the betti numbers of $s$ is bounded by \\[ \\ell^2 (o(s+\\ell+m)\\ell d)^{k+2m}. \\] this is a common generalization of previous results on bounding the betti numbers of closed semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.   we also describe an algorithm for computing the euler-poincar\\'e characteristic of such sets, generalizing similar algorithms known before. the complexity of the algorithm is bounded by $(\\ell s m d)^{o(m(m+k))}$.", "date_create": "2007-08-26", "area": "math.ag cs.sc math.at math.gt", "authors": ["basu", "pasechnik", "roy"]}, {"idpaper": "0708.3522", "title": "bounding the betti numbers and computing the euler-poincar\\'e   characteristic of semi-algebraic sets defined by partly quadratic systems of   polynomials", "abstract": "let $\\r$ be a real closed field, $ {\\mathcal q} \\subset \\r[y_1,...,y_\\ell,x_1,...,x_k], $ with $ \\deg_{y}(q) \\leq 2, \\deg_{x}(q) \\leq d, q \\in {\\mathcal q}, #({\\mathcal q})=m,$ and $ {\\mathcal p} \\subset \\r[x_1,...,x_k] $ with $\\deg_{x}(p) \\leq d, p \\in {\\mathcal p}, #({\\mathcal p})=s$, and $s \\subset \\r^{\\ell+k}$ a semi-algebraic set defined by a boolean formula without negations, with atoms $p=0, p \\geq 0, p \\leq 0, p \\in {\\mathcal p} \\cup {\\mathcal q}$. we prove that the sum of the betti numbers of $s$ is bounded by \\[ \\ell^2 (o(s+\\ell+m)\\ell d)^{k+2m}. \\] this is a common generalization of previous results on bounding the betti numbers of closed semi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.   we also describe an algorithm for computing the euler-poincar\\'e characteristic of such sets, generalizing similar algorithms known before. the complexity of the algorithm is bounded by $(\\ell s m d)^{o(m(m+k))}$.", "date_create": "2007-08-26", "area": "math.ag cs.sc math.at math.gt", "authors": ["basu", "pasechnik", "roy"]}, {"idpaper": "0708.3699", "title": "convolutional entanglement distillation", "abstract": "we develop a theory of entanglement distillation that exploits a convolutional coding structure. we provide a method for converting an arbitrary classical binary or quaternary convolutional code into a convolutional entanglement distillation protocol. the imported classical convolutional code does not have to be dual-containing or self-orthogonal. the yield and error-correcting properties of such a protocol depend respectively on the rate and error-correcting properties of the imported classical convolutional code. a convolutional entanglement distillation protocol has several other benefits. two parties sharing noisy ebits can distill noiseless ebits ``online'' as they acquire more noisy ebits. distillation yield is high and decoding complexity is simple for a convolutional entanglement distillation protocol. our theory of convolutional entanglement distillation reduces the problem of finding a good convolutional entanglement distillation protocol to the well-established problem of finding a good classical convolutional code.", "date_create": "2007-08-27", "area": "quant-ph cs.it math.it", "authors": ["wilde", "krovi", "brun"]}, {"idpaper": "0708.3699", "title": "convolutional entanglement distillation", "abstract": "we develop a theory of entanglement distillation that exploits a convolutional coding structure. we provide a method for converting an arbitrary classical binary or quaternary convolutional code into a convolutional entanglement distillation protocol. the imported classical convolutional code does not have to be dual-containing or self-orthogonal. the yield and error-correcting properties of such a protocol depend respectively on the rate and error-correcting properties of the imported classical convolutional code. a convolutional entanglement distillation protocol has several other benefits. two parties sharing noisy ebits can distill noiseless ebits ``online'' as they acquire more noisy ebits. distillation yield is high and decoding complexity is simple for a convolutional entanglement distillation protocol. our theory of convolutional entanglement distillation reduces the problem of finding a good convolutional entanglement distillation protocol to the well-established problem of finding a good classical convolutional code.", "date_create": "2007-08-27", "area": "quant-ph cs.it math.it", "authors": ["wilde", "krovi", "brun"]}, {"idpaper": "0708.3829", "title": "a non parametric model for the forecasting of the venezuelan oil prices", "abstract": "a neural net model for forecasting the prices of venezuelan crude oil is proposed. the inputs of the neural net are selected by reference to a dynamic system model of oil prices by mashayekhi (1995, 2001) and its performance is evaluated using two criteria: the excess profitability test by anatoliev and gerko (2005) and the characteristics of the equity curve generated by a trading strategy based on the neural net predictions.   -----   se introduce aqui un modelo no parametrico para pronosticar los precios del petroleo venezolano cuyos insumos son seleccionados en base a un sistema dinamico que explica los precios en terminos de dichos insumos. se describe el proceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se evaluan sus pronosticos a traves de un test estadistico de predictibilidad y de las caracteristicas del equity curve inducido por la estrategia de compraventa bursatil generada por dichos pronosticos.", "date_create": "2007-08-28", "area": "cs.ce cs.ne", "authors": ["costanzo", "trigo", "dehne", "prato"]}, {"idpaper": "0708.3879", "title": "graph annotations in modeling complex network topologies", "abstract": "the coarsest approximation of the structure of a complex network, such as the internet, is a simple undirected unweighted graph. this approximation, however, loses too much detail. in reality, objects represented by vertices and edges in such a graph possess some non-trivial internal structure that varies across and differentiates among distinct types of links or nodes. in this work, we abstract such additional information as network annotations. we introduce a network topology modeling framework that treats annotations as an extended correlation profile of a network. assuming we have this profile measured for a given network, we present an algorithm to rescale it in order to construct networks of varying size that still reproduce the original measured annotation profile.   using this methodology, we accurately capture the network properties essential for realistic simulations of network applications and protocols, or any other simulations involving complex network topologies, including modeling and simulation of network evolution. we apply our approach to the autonomous system (as) topology of the internet annotated with business relationships between ass. this topology captures the large-scale structure of the internet. in depth understanding of this structure and tools to model it are cornerstones of research on future internet architectures and designs. we find that our techniques are able to accurately capture the structure of annotation correlations within this topology, thus reproducing a number of its important properties in synthetically-generated random graphs.", "date_create": "2007-08-28", "area": "cs.ni cond-mat.dis-nn physics.data-an physics.soc-ph", "authors": ["dimitropoulos", "krioukov", "vahdat", "riley"]}, {"idpaper": "0708.3879", "title": "graph annotations in modeling complex network topologies", "abstract": "the coarsest approximation of the structure of a complex network, such as the internet, is a simple undirected unweighted graph. this approximation, however, loses too much detail. in reality, objects represented by vertices and edges in such a graph possess some non-trivial internal structure that varies across and differentiates among distinct types of links or nodes. in this work, we abstract such additional information as network annotations. we introduce a network topology modeling framework that treats annotations as an extended correlation profile of a network. assuming we have this profile measured for a given network, we present an algorithm to rescale it in order to construct networks of varying size that still reproduce the original measured annotation profile.   using this methodology, we accurately capture the network properties essential for realistic simulations of network applications and protocols, or any other simulations involving complex network topologies, including modeling and simulation of network evolution. we apply our approach to the autonomous system (as) topology of the internet annotated with business relationships between ass. this topology captures the large-scale structure of the internet. in depth understanding of this structure and tools to model it are cornerstones of research on future internet architectures and designs. we find that our techniques are able to accurately capture the structure of annotation correlations within this topology, thus reproducing a number of its important properties in synthetically-generated random graphs.", "date_create": "2007-08-28", "area": "cs.ni cond-mat.dis-nn physics.data-an physics.soc-ph", "authors": ["dimitropoulos", "krioukov", "vahdat", "riley"]}, {"idpaper": "0708.4328", "title": "dualities between entropy functions and network codes", "abstract": "this paper provides a new duality between entropy functions and network codes. given a function $g\\geq 0$ defined on all proper subsets of $n$ random variables, we provide a construction for a network multicast problem which is solvable if and only if $g$ is entropic. the underlying network topology is fixed and the multicast problem depends on $g$ only through edge capacities and source rates. relaxing the requirement that the domain of $g$ be subsets of random variables, we obtain a similar duality between polymatroids and the linear programming bound. these duality results provide an alternative proof of the insufficiency of linear (and abelian) network codes, and demonstrate the utility of non-shannon inequalities to tighten outer bounds on network coding capacity regions.", "date_create": "2007-08-31", "area": "cs.it math.it", "authors": ["chan", "grant"]}, {"idpaper": "0708.4328", "title": "dualities between entropy functions and network codes", "abstract": "this paper provides a new duality between entropy functions and network codes. given a function $g\\geq 0$ defined on all proper subsets of $n$ random variables, we provide a construction for a network multicast problem which is solvable if and only if $g$ is entropic. the underlying network topology is fixed and the multicast problem depends on $g$ only through edge capacities and source rates. relaxing the requirement that the domain of $g$ be subsets of random variables, we obtain a similar duality between polymatroids and the linear programming bound. these duality results provide an alternative proof of the insufficiency of linear (and abelian) network codes, and demonstrate the utility of non-shannon inequalities to tighten outer bounds on network coding capacity regions.", "date_create": "2007-08-31", "area": "cs.it math.it", "authors": ["chan", "grant"]}, {"idpaper": "0708.4328", "title": "dualities between entropy functions and network codes", "abstract": "this paper provides a new duality between entropy functions and network codes. given a function $g\\geq 0$ defined on all proper subsets of $n$ random variables, we provide a construction for a network multicast problem which is solvable if and only if $g$ is entropic. the underlying network topology is fixed and the multicast problem depends on $g$ only through edge capacities and source rates. relaxing the requirement that the domain of $g$ be subsets of random variables, we obtain a similar duality between polymatroids and the linear programming bound. these duality results provide an alternative proof of the insufficiency of linear (and abelian) network codes, and demonstrate the utility of non-shannon inequalities to tighten outer bounds on network coding capacity regions.", "date_create": "2007-08-31", "area": "cs.it math.it", "authors": ["chan", "grant"]}, {"idpaper": "0708.4399", "title": "type-iv dct, dst, and mdct algorithms with reduced numbers of arithmetic   operations", "abstract": "we present algorithms for the type-iv discrete cosine transform (dct-iv) and discrete sine transform (dst-iv), as well as for the modified discrete cosine transform (mdct) and its inverse, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. asymptotically, the operation count is reduced from ~2nlogn to ~(17/9)nlogn for a power-of-two transform size n, and the exact count is strictly lowered for all n > 4. these results are derived by considering the dct to be a special case of a dft of length 8n, with certain symmetries, and then pruning redundant operations from a recent improved fast fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split radix algorithm). the improved algorithms for dst-iv and mdct follow immediately from the improved count for the dct-iv.", "date_create": "2007-08-31", "area": "cs.ds cs.na", "authors": ["shao", "johnson"]}, {"idpaper": "0708.4399", "title": "type-iv dct, dst, and mdct algorithms with reduced numbers of arithmetic   operations", "abstract": "we present algorithms for the type-iv discrete cosine transform (dct-iv) and discrete sine transform (dst-iv), as well as for the modified discrete cosine transform (mdct) and its inverse, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. asymptotically, the operation count is reduced from ~2nlogn to ~(17/9)nlogn for a power-of-two transform size n, and the exact count is strictly lowered for all n > 4. these results are derived by considering the dct to be a special case of a dft of length 8n, with certain symmetries, and then pruning redundant operations from a recent improved fast fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split radix algorithm). the improved algorithms for dst-iv and mdct follow immediately from the improved count for the dct-iv.", "date_create": "2007-08-31", "area": "cs.ds cs.na", "authors": ["shao", "johnson"]}, {"idpaper": "0709.0624", "title": "on faster integer calculations using non-arithmetic primitives", "abstract": "the unit cost model is both convenient and largely realistic for describing integer decision algorithms over (+,*). additional operations like division with remainder or bitwise conjunction, although equally supported by computing hardware, may lead to a considerable drop in complexity. we show a variety of concrete problems to benefit from such non-arithmetic primitives by presenting and analyzing corresponding fast algorithms.", "date_create": "2007-09-05", "area": "cs.ds", "authors": ["l\u00fcrwer-br\u00fcggemeier", "ziegler"]}, {"idpaper": "0709.0896", "title": "open access does not increase citations for research articles from the   astrophysical journal", "abstract": "we demonstrate conclusively that there is no \"open access advantage\" for papers from the astrophysical journal. the two to one citation advantage enjoyed by papers deposited in the arxiv e-print server is due entirely to the nature and timing of the deposited papers. this may have implications for other disciplines.", "date_create": "2007-09-06", "area": "cs.dl cs.cy", "authors": ["kurtz", "henneken"]}, {"idpaper": "0709.1024", "title": "computational performance of a parallelized high-order spectral and   mortar element toolbox", "abstract": "in this paper, a comprehensive performance review of a mpi-based high-order spectral and mortar element method c++ toolbox is presented. the focus is put on the performance evaluation of several aspects with a particular emphasis on the parallel efficiency. the performance evaluation is analyzed and compared to predictions given by a heuristic model, the so-called gamma model. a tailor-made cfd computation benchmark case is introduced and used to carry out this review, stressing the particular interest for commodity clusters. conclusions are drawn from this extensive series of analyses and modeling leading to specific recommendations concerning such toolbox development and parallel implementation.", "date_create": "2007-09-07", "area": "cs.dc cs.pf", "authors": ["bouffanais", "keller", "gruber", "deville"]}, {"idpaper": "0709.1074", "title": "johnson type bounds on constant dimension codes", "abstract": "very recently, an operator channel was defined by koetter and kschischang when they studied random network coding. they also introduced constant dimension codes and demonstrated that these codes can be employed to correct errors and/or erasures over the operator channel. constant dimension codes are equivalent to the so-called linear authentication codes introduced by wang, xing and safavi-naini when constructing distributed authentication systems in 2003. in this paper, we study constant dimension codes. it is shown that steiner structures are optimal constant dimension codes achieving the wang-xing-safavi-naini bound. furthermore, we show that constant dimension codes achieve the wang-xing-safavi-naini bound if and only if they are certain steiner structures. then, we derive two johnson type upper bounds, say i and ii, on constant dimension codes. the johnson type bound ii slightly improves on the wang-xing-safavi-naini bound. finally, we point out that a family of known steiner structures is actually a family of optimal constant dimension codes achieving both the johnson type bounds i and ii.", "date_create": "2007-09-07", "area": "cs.it math.it", "authors": ["xia", "fu"]}, {"idpaper": "0709.1942", "title": "connecting polygonizations via stretches and twangs", "abstract": "we show that the space of polygonizations of a fixed planar point set s of n points is connected by o(n^2) ``moves'' between simple polygons. each move is composed of a sequence of atomic moves called ``stretches'' and ``twangs''. these atomic moves walk between weakly simple ``polygonal wraps'' of s. these moves show promise to serve as a basis for generating random polygons.", "date_create": "2007-09-12", "area": "cs.cg cs.dm", "authors": ["damian", "flatland", "o'rourke", "ramaswami"]}, {"idpaper": "0709.2196", "title": "bregman voronoi diagrams: properties, algorithms and applications", "abstract": "the voronoi diagram of a finite set of objects is a fundamental geometric structure that subdivides the embedding space into regions, each region consisting of the points that are closer to a given object than to the others. we may define many variants of voronoi diagrams depending on the class of objects, the distance functions and the embedding space. in this paper, we investigate a framework for defining and building voronoi diagrams for a broad class of distance functions called bregman divergences. bregman divergences include not only the traditional (squared) euclidean distance but also various divergence measures based on entropic functions. accordingly, bregman voronoi diagrams allow to define information-theoretic voronoi diagrams in statistical parametric spaces based on the relative entropy of distributions. we define several types of bregman diagrams, establish correspondences between those diagrams (using the legendre transformation), and show how to compute them efficiently. we also introduce extensions of these diagrams, e.g. k-order and k-bag bregman voronoi diagrams, and introduce bregman triangulations of a set of points and their connexion with bregman voronoi diagrams. we show that these triangulations capture many of the properties of the celebrated delaunay triangulation. finally, we give some applications of bregman voronoi diagrams which are of interest in the context of computational geometry and machine learning.", "date_create": "2007-09-13", "area": "cs.cg", "authors": ["nielsen", "boissonnat", "nock"]}, {"idpaper": "0709.2196", "title": "bregman voronoi diagrams: properties, algorithms and applications", "abstract": "the voronoi diagram of a finite set of objects is a fundamental geometric structure that subdivides the embedding space into regions, each region consisting of the points that are closer to a given object than to the others. we may define many variants of voronoi diagrams depending on the class of objects, the distance functions and the embedding space. in this paper, we investigate a framework for defining and building voronoi diagrams for a broad class of distance functions called bregman divergences. bregman divergences include not only the traditional (squared) euclidean distance but also various divergence measures based on entropic functions. accordingly, bregman voronoi diagrams allow to define information-theoretic voronoi diagrams in statistical parametric spaces based on the relative entropy of distributions. we define several types of bregman diagrams, establish correspondences between those diagrams (using the legendre transformation), and show how to compute them efficiently. we also introduce extensions of these diagrams, e.g. k-order and k-bag bregman voronoi diagrams, and introduce bregman triangulations of a set of points and their connexion with bregman voronoi diagrams. we show that these triangulations capture many of the properties of the celebrated delaunay triangulation. finally, we give some applications of bregman voronoi diagrams which are of interest in the context of computational geometry and machine learning.", "date_create": "2007-09-13", "area": "cs.cg", "authors": ["nielsen", "boissonnat", "nock"]}, {"idpaper": "0709.2225", "title": "improved linear parallel interference cancellers", "abstract": "in this paper, taking the view that a linear parallel interference canceller (lpic) can be seen as a linear matrix filter, we propose new linear matrix filters that can result in improved bit error performance compared to other lpics in the literature. the motivation for the proposed filters arises from the possibility of avoiding the generation of certain interference and noise terms in a given stage that would have been present in a conventional lpic (clpic). in the proposed filters, we achieve such avoidance of the generation of interference and noise terms in a given stage by simply making the diagonal elements of a certain matrix in that stage equal to zero. hence, the proposed filters do not require additional complexity compared to the clpic, and they can allow achieving a certain error performance using fewer lpic stages. we also extend the proposed matrix filter solutions to a multicarrier ds-cdma system, where we consider two types of receivers. in one receiver (referred to as type-i receiver), lpic is performed on each subcarrier first, followed by multicarrier combining (mcc). in the other receiver (called type-ii receiver), mcc is performed first, followed by lpic. we show that in both type-i and type-ii receivers, the proposed matrix filters outperform other matrix filters. also, type-ii receiver performs better than type-i receiver because of enhanced accuracy of the interference estimates achieved due to frequency diversity offered by mcc.", "date_create": "2007-09-14", "area": "cs.it cs.sc cs.sd cs.se math.it", "authors": ["srikanth", "vardhan", "chockalingam", "milstein"]}, {"idpaper": "0709.2635", "title": "non-blocking signature of very large soap messages", "abstract": "data transfer and staging services are common components in grid-based, or more generally, in service-oriented applications. security mechanisms play a central role in such services, especially when they are deployed in sensitive application fields like e-health. the adoption of ws-security and related standards to soap-based transfer services is, however, problematic as a straightforward adoption of soap with mtom introduces considerable inefficiencies in the signature generation process when large data sets are involved. this paper proposes a non-blocking, signature generation approach enabling a stream-like processing with considerable performance enhancements.", "date_create": "2007-09-17", "area": "cs.dc", "authors": ["kohring", "iacono"]}, {"idpaper": "0709.3034", "title": "query evaluation in p2p systems of taxonomy-based sources: algorithms,   complexity, and optimizations", "abstract": "in this study, we address the problem of answering queries over a peer-to-peer system of taxonomy-based sources. a taxonomy states subsumption relationships between negation-free dnf formulas on terms and negation-free conjunctions of terms. to the end of laying the foundations of our study, we first consider the centralized case, deriving the complexity of the decision problem and of query evaluation. we conclude by presenting an algorithm that is efficient in data complexity and is based on hypergraphs. more expressive forms of taxonomies are also investigated, which however lead to intractability. we then move to the distributed case, and introduce a logical model of a network of taxonomy-based sources. on such network, a distributed version of the centralized algorithm is then presented, based on a message passing paradigm, and its correctness is proved. we finally discuss optimization issues, and relate our work to the literature.", "date_create": "2007-09-19", "area": "cs.db cs.dc cs.ds cs.lo", "authors": ["meghini", "tzitzikas", "analyti"]}, {"idpaper": "0709.3262", "title": "a software for learning information theory basics with emphasis on   entropy of spanish", "abstract": "in this paper, a tutorial software to learn information theory basics in a practical way is reported. the software, called it-tutor-uv, makes use of a modern existing spanish corpus for the modeling of the source. both the source and the channel coding are also included in this educational tool as part of the learning experience. entropy values of the spanish language obtained with the it-tutor-uv are discussed and compared to others that were previously calculated under limited conditions.", "date_create": "2007-09-20", "area": "cs.it math.it", "authors": ["guerrero", "perez"]}, {"idpaper": "0709.3427", "title": "mutual information for the selection of relevant variables in   spectrometric nonlinear modelling", "abstract": "data from spectrophotometers form vectors of a large number of exploitable variables. building quantitative models using these variables most often requires using a smaller set of variables than the initial one. indeed, a too large number of input variables to a model results in a too large number of parameters, leading to overfitting and poor generalization abilities. in this paper, we suggest the use of the mutual information measure to select variables from the initial set. the mutual information measures the information content in input variables with respect to the model output, without making any assumption on the model that will be used; it is thus suitable for nonlinear modelling. in addition, it leads to the selection of variables among the initial set, and not to linear or nonlinear combinations of them. without decreasing the model performances compared to other variable projection methods, it allows therefore a greater interpretability of the results.", "date_create": "2007-09-21", "area": "cs.lg cs.ne stat.ap", "authors": ["rossi", "lendasse", "fran\u00e7ois", "wertz", "verleysen"]}, {"idpaper": "0709.3600", "title": "cooperative multiplexing in a half duplex relay network: performance and   constraints", "abstract": "previous work on relay networks has concentrated primarily on the diversity benefits of such techniques. this paper explores the possibility of also obtaining multiplexing gain in a relay network, while retaining diversity gain. specifically, consider a network in which a single source node is equipped with one antenna and a destination is equipped with two antennas. it is shown that, in certain scenarios, by adding a relay with two antennas and using a successive relaying protocol, the diversity multiplexing tradeoff performance of the network can be lower bounded by that of a 2 by 2 mimo channel, when the decode-and-forward protocol is applied at the relay. a distributed d-blast architecture is developed, in which parallel channel coding is applied to achieve this tradeoff. a space-time coding strategy, which can bring a maximal multiplexing gain of more than one, is also derived for this scenario. as will be shown, while this space-time coding strategy exploits maximal diversity for a small multiplexing gain, the proposed successive relaying scheme offers a significant performance advantage for higher data rate transmission. in addition to the specific results shown here, these ideas open a new direction for exploiting the benefits of wireless relay networks.", "date_create": "2007-09-24", "area": "cs.it math.it", "authors": ["yijia", "fan", "poor", "thompson"]}, {"idpaper": "0709.3600", "title": "cooperative multiplexing in a half duplex relay network: performance and   constraints", "abstract": "previous work on relay networks has concentrated primarily on the diversity benefits of such techniques. this paper explores the possibility of also obtaining multiplexing gain in a relay network, while retaining diversity gain. specifically, consider a network in which a single source node is equipped with one antenna and a destination is equipped with two antennas. it is shown that, in certain scenarios, by adding a relay with two antennas and using a successive relaying protocol, the diversity multiplexing tradeoff performance of the network can be lower bounded by that of a 2 by 2 mimo channel, when the decode-and-forward protocol is applied at the relay. a distributed d-blast architecture is developed, in which parallel channel coding is applied to achieve this tradeoff. a space-time coding strategy, which can bring a maximal multiplexing gain of more than one, is also derived for this scenario. as will be shown, while this space-time coding strategy exploits maximal diversity for a small multiplexing gain, the proposed successive relaying scheme offers a significant performance advantage for higher data rate transmission. in addition to the specific results shown here, these ideas open a new direction for exploiting the benefits of wireless relay networks.", "date_create": "2007-09-24", "area": "cs.it math.it", "authors": ["yijia", "fan", "poor", "thompson"]}, {"idpaper": "0709.4552", "title": "distributed n-body simulation on the grid using dedicated hardware", "abstract": "we present performance measurements of direct gravitational n -body simulation on the grid, with and without specialized (grape-6) hardware. our inter-continental virtual organization consists of three sites, one in tokyo, one in philadelphia and one in amsterdam. we run simulations with up to 196608 particles for a variety of topologies. in many cases, high performance simulations over the entire planet are dominated by network bandwidth rather than latency. with this global grid of grapes our calculation time remains dominated by communication over the entire range of n, which was limited due to the use of three sites. increasing the number of particles will result in a more efficient execution. based on these timings we construct and calibrate a model to predict the performance of our simulation on any grid infrastructure with or without grape. we apply this model to predict the simulation performance on the netherlands das-3 wide area computer. equipping the das-3 with grape-6af hardware would achieve break-even between calculation and communication at a few million particles, resulting in a compute time of just over ten hours for 1 n -body time unit. key words: high-performance computing, grid, n-body simulation, performance modelling", "date_create": "2007-09-28", "area": "astro-ph cs.dc", "authors": ["groen", "zwart", "mcmillan", "makino"]}, {"idpaper": "0710.0116", "title": "distributed mimo receiver - achievable rates and upper bounds", "abstract": "in this paper we investigate the achievable rate of a system that includes a nomadic transmitter with several antennas, which is received by multiple agents, exhibiting independent channel gains and additive circular-symmetric complex gaussian noise. in the nomadic regime, we assume that the agents do not have any decoding ability. these agents process their channel observations and forward them to the final destination through lossless links with a fixed capacity. we propose new achievable rates based on elementary compression and also on a wyner-ziv (ceo-like) processing, for both fast fading and block fading channels, as well as for general discrete channels. the simpler two agents scheme is solved, up to an implicit equation with a single variable. limiting the nomadic transmitter to a circular-symmetric complex gaussian signalling, new upper bounds are derived for both fast and block fading, based on the vector version of the entropy power inequality. these bounds are then compared to the achievable rates in several extreme scenarios. the asymptotic setting with numbers of agents and transmitter's antennas taken to infinity is analyzed. in addition, the upper bounds are analytically shown to be tight in several examples, while numerical calculations reveal a rather small gap in a finite $2\\times2$ setting. the advantage of the wyner-ziv approach over elementary compression is shown where only the former can achieve the full diversity-multiplexing tradeoff. we also consider the non-nomadic setting, with agents that can decode. here we give an achievable rate, over fast fading channel, which combines broadcast with dirty paper coding and the decentralized reception, which was introduced for the nomadic setting.", "date_create": "2007-09-30", "area": "cs.it math.it", "authors": ["sanderovich", "shamai", "steinberg"]}, {"idpaper": "0710.0116", "title": "distributed mimo receiver - achievable rates and upper bounds", "abstract": "in this paper we investigate the achievable rate of a system that includes a nomadic transmitter with several antennas, which is received by multiple agents, exhibiting independent channel gains and additive circular-symmetric complex gaussian noise. in the nomadic regime, we assume that the agents do not have any decoding ability. these agents process their channel observations and forward them to the final destination through lossless links with a fixed capacity. we propose new achievable rates based on elementary compression and also on a wyner-ziv (ceo-like) processing, for both fast fading and block fading channels, as well as for general discrete channels. the simpler two agents scheme is solved, up to an implicit equation with a single variable. limiting the nomadic transmitter to a circular-symmetric complex gaussian signalling, new upper bounds are derived for both fast and block fading, based on the vector version of the entropy power inequality. these bounds are then compared to the achievable rates in several extreme scenarios. the asymptotic setting with numbers of agents and transmitter's antennas taken to infinity is analyzed. in addition, the upper bounds are analytically shown to be tight in several examples, while numerical calculations reveal a rather small gap in a finite $2\\times2$ setting. the advantage of the wyner-ziv approach over elementary compression is shown where only the former can achieve the full diversity-multiplexing tradeoff. we also consider the non-nomadic setting, with agents that can decode. here we give an achievable rate, over fast fading channel, which combines broadcast with dirty paper coding and the decentralized reception, which was introduced for the nomadic setting.", "date_create": "2007-09-30", "area": "cs.it math.it", "authors": ["sanderovich", "shamai", "steinberg"]}, {"idpaper": "0710.0291", "title": "on outage behavior of wideband slow-fading channels", "abstract": "this paper investigates point-to-point information transmission over a wideband slow-fading channel, modeled as an (asymptotically) large number of independent identically distributed parallel channels, with the random channel fading realizations remaining constant over the entire coding block. on the one hand, in the wideband limit the minimum achievable energy per nat required for reliable transmission, as a random variable, converges in probability to certain deterministic quantity. on the other hand, the exponential decay rate of the outage probability, termed as the wideband outage exponent, characterizes how the number of parallel channels, {\\it i.e.}, the ``bandwidth'', should asymptotically scale in order to achieve a targeted outage probability at a targeted energy per nat. we examine two scenarios: when the transmitter has no channel state information and adopts uniform transmit power allocation among parallel channels; and when the transmitter is endowed with an one-bit channel state feedback for each parallel channel and accordingly allocates its transmit power. for both scenarios, we evaluate the wideband minimum energy per nat and the wideband outage exponent, and discuss their implication for system performance.", "date_create": "2007-09-30", "area": "cs.it math.it", "authors": ["zhang", "mitra"]}, {"idpaper": "0710.0386", "title": "comparing maintenance strategies for overlays", "abstract": "in this paper, we present an analytical tool for understanding the performance of structured overlay networks under churn based on the master-equation approach of physics. we motivate and derive an equation for the average number of hops taken by lookups during churn, for the chord network. we analyse this equation in detail to understand the behaviour with and without churn. we then use this understanding to predict how lookups will scale for varying peer population as well as varying the sizes of the routing tables. we then consider a change in the maintenance algorithm of the overlay, from periodic stabilisation to a reactive one which corrects fingers only when a change is detected. we generalise our earlier analysis to underdstand how the reactive strategy compares with the periodic one.", "date_create": "2007-10-01", "area": "cs.ni cond-mat.stat-mech cs.dc", "authors": ["krishnamurthy", "el-ansary", "aurell", "haridi"]}, {"idpaper": "0710.0431", "title": "new counting codes for distributed video coding", "abstract": "this paper introduces a new counting code. its design was motivated by distributed video coding where, for decoding, error correction methods are applied to improve predictions. those error corrections sometimes fail which results in decoded values worse than the initial prediction. our code exploits the fact that bit errors are relatively unlikely events: more than a few bit errors in a decoded pixel value are rare. with a carefully designed counting code combined with a prediction those bit errors can be corrected and sometimes the original pixel value recovered. the error correction improves significantly. our new code not only maximizes the hamming distance between adjacent (or \"near 1\") codewords but also between nearby (for example \"near 2\") codewords. this is why our code is significantly different from the well-known maximal counting sequences which have maximal average hamming distance. fortunately, the new counting code can be derived from gray codes for every code word length (i.e. bit depth).", "date_create": "2007-10-01", "area": "cs.it math.it", "authors": ["lakus-becker", "leung"]}, {"idpaper": "0710.0865", "title": "secrecy capacity of the wiretap channel with noisy feedback", "abstract": "in this work, the role of noisy feedback in enhancing the secrecy capacity of the wiretap channel is investigated. a model is considered in which the feed-forward and feedback signals share the same noisy channel. more specifically, a discrete memoryless modulo-additive channel with a full-duplex destination node is considered first, and it is shown that a judicious use of feedback increases the perfect secrecy capacity to the capacity of the source-destination channel in the absence of the wiretapper. in the achievability scheme, the feedback signal corresponds to a private key, known only to the destination. then a half-duplex system is considered, for which a novel feedback technique that always achieves a positive perfect secrecy rate (even when the source-wiretapper channel is less noisy than the source-destination channel) is proposed. these results hinge on the modulo-additive property of the channel, which is exploited by the destination to perform encryption over the channel without revealing its key to the source.", "date_create": "2007-10-03", "area": "cs.it cs.cr math.it", "authors": ["lai", "gamal", "poor"]}, {"idpaper": "0710.1149", "title": "z2z4-linear codes: generator matrices and duality", "abstract": "a code ${\\cal c}$ is $\\z_2\\z_4$-additive if the set of coordinates can be partitioned into two subsets $x$ and $y$ such that the punctured code of ${\\cal c}$ by deleting the coordinates outside $x$ (respectively, $y$) is a binary linear code (respectively, a quaternary linear code). in this paper $\\z_2\\z_4$-additive codes are studied. their corresponding binary images, via the gray map, are $\\z_2\\z_4$-linear codes, which seem to be a very distinguished class of binary group codes.   as for binary and quaternary linear codes, for these codes the fundamental parameters are found and standard forms for generator and parity check matrices are given. for this, the appropriate inner product is deduced and the concept of duality for $\\z_2\\z_4$-additive codes is defined. moreover, the parameters of the dual codes are computed. finally, some conditions for self-duality of $\\z_2\\z_4$-additive codes are given.", "date_create": "2007-10-05", "area": "cs.it cs.dm math.co math.it", "authors": ["borges", "fernandez", "pujol", "rifa", "villanueva"]}, {"idpaper": "0710.1254", "title": "a group theoretic model for information", "abstract": "in this paper we formalize the notions of information elements and information lattices, first proposed by shannon. exploiting this formalization, we identify a comprehensive parallelism between information lattices and subgroup lattices. qualitatively, we demonstrate isomorphisms between information lattices and subgroup lattices. quantitatively, we establish a decisive approximation relation between the entropy structures of information lattices and the log-index structures of the corresponding subgroup lattices. this approximation extends the approximation for joint entropies carried out previously by chan and yeung. as a consequence of our approximation result, we show that any continuous law holds in general for the entropies of information elements if and only if the same law holds in general for the log-indices of subgroups. as an application, by constructing subgroup counterexamples we find surprisingly that common information, unlike joint information, obeys neither the submodularity nor the supermodularity law. we emphasize that the notion of information elements is conceptually significant--formalizing it helps to reveal the deep connection between information theory and group theory. the parallelism established in this paper admits an appealing group-action explanation and provides useful insights into the intrinsic structure among information elements from a group-theoretic perspective.", "date_create": "2007-10-05", "area": "cs.it math.it", "authors": ["li", "chong"]}, {"idpaper": "0710.1482", "title": "heap reference analysis for functional programs", "abstract": "current garbage collectors leave a lot of garbage uncollected because they conservatively approximate liveness by reachability from program variables. in this paper, we describe a sequence of static analyses that takes as input a program written in a first-order, eager functional programming language, and finds at each program point the references to objects that are guaranteed not to be used in the future. such references are made null by a transformation pass. if this makes the object unreachable, it can be collected by the garbage collector. this causes more garbage to be collected, resulting in fewer collections. additionally, for those garbage collectors which scavenge live objects, it makes each collection faster.   the interesting aspects of our method are both in the identification of the analyses required to solve the problem and the way they are carried out. we identify three different analyses -- liveness, sharing and accessibility. in liveness and sharing analyses, the function definitions are analyzed independently of the calling context. this is achieved by using a variable to represent the unknown context of the function being analyzed and setting up constraints expressing the effect of the function with respect to the variable. the solution of the constraints is a summary of the function that is parameterized with respect to a calling context and is used to analyze function calls. as a result we achieve context sensitivity at call sites without analyzing the function multiple number of times.", "date_create": "2007-10-08", "area": "cs.pl cs.se", "authors": ["karkare", "sanyal", "khedker"]}, {"idpaper": "0710.1482", "title": "heap reference analysis for functional programs", "abstract": "current garbage collectors leave a lot of garbage uncollected because they conservatively approximate liveness by reachability from program variables. in this paper, we describe a sequence of static analyses that takes as input a program written in a first-order, eager functional programming language, and finds at each program point the references to objects that are guaranteed not to be used in the future. such references are made null by a transformation pass. if this makes the object unreachable, it can be collected by the garbage collector. this causes more garbage to be collected, resulting in fewer collections. additionally, for those garbage collectors which scavenge live objects, it makes each collection faster.   the interesting aspects of our method are both in the identification of the analyses required to solve the problem and the way they are carried out. we identify three different analyses -- liveness, sharing and accessibility. in liveness and sharing analyses, the function definitions are analyzed independently of the calling context. this is achieved by using a variable to represent the unknown context of the function being analyzed and setting up constraints expressing the effect of the function with respect to the variable. the solution of the constraints is a summary of the function that is parameterized with respect to a calling context and is used to analyze function calls. as a result we achieve context sensitivity at call sites without analyzing the function multiple number of times.", "date_create": "2007-10-08", "area": "cs.pl cs.se", "authors": ["karkare", "sanyal", "khedker"]}, {"idpaper": "0710.1482", "title": "heap reference analysis for functional programs", "abstract": "current garbage collectors leave a lot of garbage uncollected because they conservatively approximate liveness by reachability from program variables. in this paper, we describe a sequence of static analyses that takes as input a program written in a first-order, eager functional programming language, and finds at each program point the references to objects that are guaranteed not to be used in the future. such references are made null by a transformation pass. if this makes the object unreachable, it can be collected by the garbage collector. this causes more garbage to be collected, resulting in fewer collections. additionally, for those garbage collectors which scavenge live objects, it makes each collection faster.   the interesting aspects of our method are both in the identification of the analyses required to solve the problem and the way they are carried out. we identify three different analyses -- liveness, sharing and accessibility. in liveness and sharing analyses, the function definitions are analyzed independently of the calling context. this is achieved by using a variable to represent the unknown context of the function being analyzed and setting up constraints expressing the effect of the function with respect to the variable. the solution of the constraints is a summary of the function that is parameterized with respect to a calling context and is used to analyze function calls. as a result we achieve context sensitivity at call sites without analyzing the function multiple number of times.", "date_create": "2007-10-08", "area": "cs.pl cs.se", "authors": ["karkare", "sanyal", "khedker"]}, {"idpaper": "0710.1499", "title": "approximating max-min linear programs with local algorithms", "abstract": "a local algorithm is a distributed algorithm where each node must operate solely based on the information that was available at system startup within a constant-size neighbourhood of the node. we study the applicability of local algorithms to max-min lps where the objective is to maximise $\\min_k \\sum_v c_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$ for each $v$. here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $v_i = \\{v : a_{iv} > 0 \\}$, $v_k = \\{v : c_{kv}>0 \\}$, $i_v = \\{i : a_{iv} > 0 \\}$ and $k_v = \\{k : c_{kv} > 0 \\}$ have bounded size. in the distributed setting, each agent $v$ is responsible for choosing the value of $x_v$, and the communication network is a hypergraph $\\mathcal{h}$ where the sets $v_k$ and $v_i$ constitute the hyperedges. we present inapproximability results for a wide range of structural assumptions; for example, even if $|v_i|$ and $|v_k|$ are bounded by some constants larger than 2, there is no local approximation scheme. to contrast the negative results, we present a local approximation algorithm which achieves good approximation ratios if we can bound the relative growth of the vertex neighbourhoods in $\\mathcal{h}$.", "date_create": "2007-10-08", "area": "cs.dc", "authors": ["flor\u00e9en", "kaski", "musto", "suomela"]}, {"idpaper": "0710.1589", "title": "fast reliability-based algorithm of finding minimum-weight codewords for   ldpc codes", "abstract": "despite the np hardness of acquiring minimum distance $d_m$ for linear codes theoretically, in this paper we propose one experimental method of finding minimum-weight codewords, the weight of which is equal to $d_m$ for ldpc codes. one existing syndrome decoding method, called serial belief propagation (bp) with ordered statistic decoding (osd), is adapted to serve our purpose. we hold the conjecture that among many candidate error patterns in osd reprocessing, modulo 2 addition of the lightest error pattern with one of the left error patterns may generate a light codeword. when the decoding syndrome changes to all-zero state, the lightest error pattern reduces to all-zero, the lightest non-zero error pattern is a valid codeword to update lightest codeword list.   given sufficient codewords sending, the survived lightest codewords are likely to be the target. compared with existing techniques, our method demonstrates its efficiency in the simulation of several interested ldpc codes.", "date_create": "2007-10-08", "area": "cs.it math.it", "authors": ["li", "feng"]}, {"idpaper": "0710.1641", "title": "a polynomial bound for untangling geometric planar graphs", "abstract": "to untangle a geometric graph means to move some of the vertices so that the resulting geometric graph has no crossings. pach and tardos [discrete comput. geom., 2002] asked if every n-vertex geometric planar graph can be untangled while keeping at least n^\\epsilon vertices fixed. we answer this question in the affirmative with \\epsilon=1/4. the previous best known bound was \\omega((\\log n / \\log\\log n)^{1/2}). we also consider untangling geometric trees. it is known that every n-vertex geometric tree can be untangled while keeping at least (n/3)^{1/2} vertices fixed, while the best upper bound was o(n\\log n)^{2/3}. we answer a question of spillner and wolff [arxiv:0709.0170 2007] by closing this gap for untangling trees. in particular, we show that for infinitely many values of n, there is an n-vertex geometric tree that cannot be untangled while keeping more than 3(n^{1/2}-1) vertices fixed. moreover, we improve the lower bound to (n/2)^{1/2}.", "date_create": "2007-10-09", "area": "cs.cg cs.dm math.co", "authors": ["bose", "dujmovic", "hurtado", "langerman", "morin", "wood"]}, {"idpaper": "0710.1641", "title": "a polynomial bound for untangling geometric planar graphs", "abstract": "to untangle a geometric graph means to move some of the vertices so that the resulting geometric graph has no crossings. pach and tardos [discrete comput. geom., 2002] asked if every n-vertex geometric planar graph can be untangled while keeping at least n^\\epsilon vertices fixed. we answer this question in the affirmative with \\epsilon=1/4. the previous best known bound was \\omega((\\log n / \\log\\log n)^{1/2}). we also consider untangling geometric trees. it is known that every n-vertex geometric tree can be untangled while keeping at least (n/3)^{1/2} vertices fixed, while the best upper bound was o(n\\log n)^{2/3}. we answer a question of spillner and wolff [arxiv:0709.0170 2007] by closing this gap for untangling trees. in particular, we show that for infinitely many values of n, there is an n-vertex geometric tree that cannot be untangled while keeping more than 3(n^{1/2}-1) vertices fixed. moreover, we improve the lower bound to (n/2)^{1/2}.", "date_create": "2007-10-09", "area": "cs.cg cs.dm math.co", "authors": ["bose", "dujmovic", "hurtado", "langerman", "morin", "wood"]}, {"idpaper": "0710.1641", "title": "a polynomial bound for untangling geometric planar graphs", "abstract": "to untangle a geometric graph means to move some of the vertices so that the resulting geometric graph has no crossings. pach and tardos [discrete comput. geom., 2002] asked if every n-vertex geometric planar graph can be untangled while keeping at least n^\\epsilon vertices fixed. we answer this question in the affirmative with \\epsilon=1/4. the previous best known bound was \\omega((\\log n / \\log\\log n)^{1/2}). we also consider untangling geometric trees. it is known that every n-vertex geometric tree can be untangled while keeping at least (n/3)^{1/2} vertices fixed, while the best upper bound was o(n\\log n)^{2/3}. we answer a question of spillner and wolff [arxiv:0709.0170 2007] by closing this gap for untangling trees. in particular, we show that for infinitely many values of n, there is an n-vertex geometric tree that cannot be untangled while keeping more than 3(n^{1/2}-1) vertices fixed. moreover, we improve the lower bound to (n/2)^{1/2}.", "date_create": "2007-10-09", "area": "cs.cg cs.dm math.co", "authors": ["bose", "dujmovic", "hurtado", "langerman", "morin", "wood"]}, {"idpaper": "0710.1641", "title": "a polynomial bound for untangling geometric planar graphs", "abstract": "to untangle a geometric graph means to move some of the vertices so that the resulting geometric graph has no crossings. pach and tardos [discrete comput. geom., 2002] asked if every n-vertex geometric planar graph can be untangled while keeping at least n^\\epsilon vertices fixed. we answer this question in the affirmative with \\epsilon=1/4. the previous best known bound was \\omega((\\log n / \\log\\log n)^{1/2}). we also consider untangling geometric trees. it is known that every n-vertex geometric tree can be untangled while keeping at least (n/3)^{1/2} vertices fixed, while the best upper bound was o(n\\log n)^{2/3}. we answer a question of spillner and wolff [arxiv:0709.0170 2007] by closing this gap for untangling trees. in particular, we show that for infinitely many values of n, there is an n-vertex geometric tree that cannot be untangled while keeping more than 3(n^{1/2}-1) vertices fixed. moreover, we improve the lower bound to (n/2)^{1/2}.", "date_create": "2007-10-09", "area": "cs.cg cs.dm math.co", "authors": ["bose", "dujmovic", "hurtado", "langerman", "morin", "wood"]}, {"idpaper": "0710.1842", "title": "an explicit universal cycle for the (n-1)-permutations of an n-set", "abstract": "we show how to construct an explicit hamilton cycle in the directed cayley graph cay({\\sigma_n, sigma_{n-1}} : \\mathbb{s}_n), where \\sigma_k = (1 2 >... k). the existence of such cycles was shown by jackson (discrete mathematics, 149 (1996) 123-129) but the proof only shows that a certain directed graph is eulerian, and knuth (volume 4 fascicle 2, generating all tuples and permutations (2005)) asks for an explicit construction. we show that a simple recursion describes our hamilton cycle and that the cycle can be generated by an iterative algorithm that uses o(n) space. moreover, the algorithm produces each successive edge of the cycle in constant time; such algorithms are said to be loopless.", "date_create": "2007-10-09", "area": "cs.dm cs.ds", "authors": ["ruskey", "williams"]}, {"idpaper": "0710.2092", "title": "self-similarity of complex networks and hidden metric spaces", "abstract": "we demonstrate that the self-similarity of some scale-free networks with respect to a simple degree-thresholding renormalization scheme finds a natural interpretation in the assumption that network nodes exist in hidden metric spaces. clustering, i.e., cycles of length three, plays a crucial role in this framework as a topological reflection of the triangle inequality in the hidden geometry. we prove that a class of hidden variable models with underlying metric spaces are able to accurately reproduce the self-similarity properties that we measured in the real networks. our findings indicate that hidden geometries underlying these real networks are a plausible explanation for their observed topologies and, in particular, for their self-similarity with respect to the degree-based renormalization.", "date_create": "2007-10-10", "area": "cond-mat.dis-nn cs.ni physics.soc-ph", "authors": ["serrano", "krioukov", "boguna"]}, {"idpaper": "0710.2139", "title": "approximation algorithms and hardness for domination with propagation", "abstract": "the power dominating set (pds) problem is the following extension of the well-known dominating set problem: find a smallest-size set of nodes $s$ that power dominates all the nodes, where a node $v$ is power dominated if (1) $v$ is in $s$ or $v$ has a neighbor in $s$, or (2) $v$ has a neighbor $w$ such that $w$ and all of its neighbors except $v$ are power dominated. we show a hardness of approximation threshold of $2^{\\log^{1-\\epsilon}{n}}$ in contrast to the logarithmic hardness for the dominating set problem. we give an $o(\\sqrt{n})$ approximation algorithm for planar graphs, and show that our methods cannot improve on this approximation guarantee. finally, we initiate the study of pds on directed graphs, and show the same hardness threshold of $2^{\\log^{1-\\epsilon}{n}}$ for directed \\emph{acyclic} graphs. also we show that the directed pds problem can be solved optimally in linear time if the underlying undirected graph has bounded tree-width.", "date_create": "2007-10-10", "area": "cs.cc cs.dm", "authors": ["aazami", "stilp"]}, {"idpaper": "0710.2532", "title": "sleeping on the job: energy-efficient broadcast for radio networks", "abstract": "we address the problem of minimizing power consumption when performing reliable broadcast on a radio network under the following popular model. each node in the network is located on a point in a two dimensional grid, and whenever a node sends a message, all awake nodes within distance r receive the message. in the broadcast problem, some node wants to successfully send a message to all other nodes in the network even when up to a 1/2 fraction of the nodes within every neighborhood can be deleted by an adversary. the set of deleted nodes is carefully chosen by the adversary to foil our algorithm and moreover, the set of deleted nodes may change periodically. this models worst-case behavior due to mobile nodes, static nodes losing power or simply some points in the grid being unoccupied. a trivial solution requires each node in the network to be awake roughly 1/2 the time, and a trivial lower bound shows that each node must be awake for at least a 1/n fraction of the time. our first result is an algorithm that requires each node to be awake for only a 1/sqrt(n) fraction of the time in expectation. our algorithm achieves this while ensuring correctness with probability 1, and keeping optimal values for other resource costs such as latency and number of messages sent. we give a lower-bound that shows that this reduction in power consumption is asymptotically optimal when latency and number of messages sent must be optimal. if we can increase the latency and messages sent by only a log*n factor we give a las vegas algorithm that requires each node to be awake for only a (log*n)/n expected fraction of the time; we give a lower-bound showing that this second algorithm is near optimal. finally, we show how to ensure energy-efficient broadcast in the presence of byzantine faults.", "date_create": "2007-10-12", "area": "cs.ds", "authors": ["king", "phillips", "saia", "young"]}, {"idpaper": "0710.2532", "title": "sleeping on the job: energy-efficient broadcast for radio networks", "abstract": "we address the problem of minimizing power consumption when performing reliable broadcast on a radio network under the following popular model. each node in the network is located on a point in a two dimensional grid, and whenever a node sends a message, all awake nodes within distance r receive the message. in the broadcast problem, some node wants to successfully send a message to all other nodes in the network even when up to a 1/2 fraction of the nodes within every neighborhood can be deleted by an adversary. the set of deleted nodes is carefully chosen by the adversary to foil our algorithm and moreover, the set of deleted nodes may change periodically. this models worst-case behavior due to mobile nodes, static nodes losing power or simply some points in the grid being unoccupied. a trivial solution requires each node in the network to be awake roughly 1/2 the time, and a trivial lower bound shows that each node must be awake for at least a 1/n fraction of the time. our first result is an algorithm that requires each node to be awake for only a 1/sqrt(n) fraction of the time in expectation. our algorithm achieves this while ensuring correctness with probability 1, and keeping optimal values for other resource costs such as latency and number of messages sent. we give a lower-bound that shows that this reduction in power consumption is asymptotically optimal when latency and number of messages sent must be optimal. if we can increase the latency and messages sent by only a log*n factor we give a las vegas algorithm that requires each node to be awake for only a (log*n)/n expected fraction of the time; we give a lower-bound showing that this second algorithm is near optimal. finally, we show how to ensure energy-efficient broadcast in the presence of byzantine faults.", "date_create": "2007-10-12", "area": "cs.ds", "authors": ["king", "phillips", "saia", "young"]}, {"idpaper": "0710.2887", "title": "implementation, compilation, optimization of object-oriented languages,   programs and systems - report on the workshop icooolps'2006 at ecoop'06", "abstract": "icooolps'2006 was the first edition of ecoop-icooolps workshop. it intended to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. this succeeded, as can be seen from the papers, the attendance and the liveliness of the discussions that took place during and after the workshop, not to mention a few new cooperations or postdoctoral contracts. the 22 talented people from different groups who participated were unanimous to appreciate this first edition and recommend that icooolps be continued next year. a community is thus beginning to form, and should be reinforced by a second edition next year, with all the improvements this first edition made emerge.", "date_create": "2007-10-15", "area": "cs.pf cs.pl cs.se", "authors": ["ducournau", "gagnon", "krintz", "mulet", "vitek", "zendra"]}, {"idpaper": "0710.3185", "title": "fuzzy modeling of electrical impedance tomography image of the lungs", "abstract": "electrical impedance tomography (eit) is a functional imaging method that is being developed for bedside use in critical care medicine. aiming at improving the chest anatomical resolution of eit images we developed a fuzzy model based on eit high temporal resolution and the functional information contained in the pulmonary perfusion and ventilation signals. eit data from an experimental animal model were collected during normal ventilation and apnea while an injection of hypertonic saline was used as a reference . the fuzzy model was elaborated in three parts: a modeling of the heart, a pulmonary map from ventilation images and, a pulmonary map from perfusion images. image segmentation was performed using a threshold method and a ventilation/perfusion map was generated. eit images treated by the fuzzy model were compared with the hypertonic saline injection method and ct-scan images, presenting good results in both qualitative (the image obtained by the model was very similar to that of the ct-scan) and quantitative (the roc curve provided an area equal to 0.93) point of view. undoubtedly, these results represent an important step in the eit images area, since they open the possibility of developing eit-based bedside clinical methods, which are not available nowadays. these achievements could serve as the base to develop eit diagnosis system for some life-threatening diseases commonly found in critical care medicine.", "date_create": "2007-10-16", "area": "cs.ai cs.cv", "authors": ["tanaka", "ortega", "galizia", "sobrinho", "amato"]}, {"idpaper": "0710.3621", "title": "numerical removal of water-vapor effects from thz-tds measurements", "abstract": "one source of disturbance in a pulsed t-ray signal is attributed to ambient water vapor. water molecules in the gas phase selectively absorb t-rays at discrete frequencies corresponding to their molecular rotational transitions. this results in prominent resonances spread over the t-ray spectrum, and in the time domain the t-ray signal is observed as fluctuations after the main pulse. these effects are generally undesired, since they may mask critical spectroscopic data. so, ambient water vapor is commonly removed from the t-ray path by using a closed chamber during the measurement. yet, in some applications a closed chamber is not applicable. this situation, therefore, motivates the need for another method to reduce these unwanted artifacts. this paper presents a study on a computational means to address the problem. initially, a complex frequency response of water vapor is modeled from a spectroscopic catalog. using a deconvolution technique, together with fine tuning of the strength of each resonance, parts of the water-vapor response are removed from a measured t-ray signal, with minimal signal distortion.", "date_create": "2007-10-18", "area": "cs.ce physics.comp-ph", "authors": ["withayachumnankul", "fischer", "mickan", "abbott"]}, {"idpaper": "0710.3621", "title": "numerical removal of water-vapor effects from thz-tds measurements", "abstract": "one source of disturbance in a pulsed t-ray signal is attributed to ambient water vapor. water molecules in the gas phase selectively absorb t-rays at discrete frequencies corresponding to their molecular rotational transitions. this results in prominent resonances spread over the t-ray spectrum, and in the time domain the t-ray signal is observed as fluctuations after the main pulse. these effects are generally undesired, since they may mask critical spectroscopic data. so, ambient water vapor is commonly removed from the t-ray path by using a closed chamber during the measurement. yet, in some applications a closed chamber is not applicable. this situation, therefore, motivates the need for another method to reduce these unwanted artifacts. this paper presents a study on a computational means to address the problem. initially, a complex frequency response of water vapor is modeled from a spectroscopic catalog. using a deconvolution technique, together with fine tuning of the strength of each resonance, parts of the water-vapor response are removed from a measured t-ray signal, with minimal signal distortion.", "date_create": "2007-10-18", "area": "cs.ce physics.comp-ph", "authors": ["withayachumnankul", "fischer", "mickan", "abbott"]}, {"idpaper": "0710.3804", "title": "random subcubes as a toy model for constraint satisfaction problems", "abstract": "we present an exactly solvable random-subcube model inspired by the structure of hard constraint satisfaction and optimization problems. our model reproduces the structure of the solution space of the random k-satisfiability and k-coloring problems, and undergoes the same phase transitions as these problems. the comparison becomes quantitative in the large-k limit. distance properties, as well the x-satisfiability threshold, are studied. the model is also generalized to define a continuous energy landscape useful for studying several aspects of glassy dynamics.", "date_create": "2007-10-19", "area": "cs.cc cond-mat.dis-nn", "authors": ["mora", "zdeborova"]}, {"idpaper": "0710.3888", "title": "cooperative multi-cell networks: impact of limited-capacity backhaul and   inter-users links", "abstract": "cooperative technology is expected to have a great impact on the performance of cellular or, more generally, infrastructure networks. both multicell processing (cooperation among base stations) and relaying (cooperation at the user level) are currently being investigated. in this presentation, recent results regarding the performance of multicell processing and user cooperation under the assumption of limited-capacity interbase station and inter-user links, respectively, are reviewed. the survey focuses on related results derived for non-fading uplink and downlink channels of simple cellular system models. the analytical treatment, facilitated by these simple setups, enhances the insight into the limitations imposed by limited-capacity constraints on the gains achievable by cooperative techniques.", "date_create": "2007-10-20", "area": "cs.it math.it", "authors": ["shamai", "somekh", "simeone", "sanderovich", "zaidel", "poor"]}, {"idpaper": "0710.3888", "title": "cooperative multi-cell networks: impact of limited-capacity backhaul and   inter-users links", "abstract": "cooperative technology is expected to have a great impact on the performance of cellular or, more generally, infrastructure networks. both multicell processing (cooperation among base stations) and relaying (cooperation at the user level) are currently being investigated. in this presentation, recent results regarding the performance of multicell processing and user cooperation under the assumption of limited-capacity interbase station and inter-user links, respectively, are reviewed. the survey focuses on related results derived for non-fading uplink and downlink channels of simple cellular system models. the analytical treatment, facilitated by these simple setups, enhances the insight into the limitations imposed by limited-capacity constraints on the gains achievable by cooperative techniques.", "date_create": "2007-10-20", "area": "cs.it math.it", "authors": ["shamai", "somekh", "simeone", "sanderovich", "zaidel", "poor"]}, {"idpaper": "0710.3901", "title": "simple, linear-time modular decomposition", "abstract": "modular decomposition is fundamental for many important problems in algorithmic graph theory including transitive orientation, the recognition of several classes of graphs, and certain combinatorial optimization problems. accordingly, there has been a drive towards a practical, linear-time algorithm for the problem. despite considerable effort, such an algorithm has remained elusive. the linear-time algorithms to date are impractical and of mainly theoretical interest. in this paper we present the first simple, linear-time algorithm to compute the modular decomposition tree of an undirected graph. the breakthrough comes by combining the best elements of two different approaches to the problem.", "date_create": "2007-10-20", "area": "cs.dm", "authors": ["tedder", "corneil", "habib", "paul"]}, {"idpaper": "0710.3901", "title": "simple, linear-time modular decomposition", "abstract": "modular decomposition is fundamental for many important problems in algorithmic graph theory including transitive orientation, the recognition of several classes of graphs, and certain combinatorial optimization problems. accordingly, there has been a drive towards a practical, linear-time algorithm for the problem. despite considerable effort, such an algorithm has remained elusive. the linear-time algorithms to date are impractical and of mainly theoretical interest. in this paper we present the first simple, linear-time algorithm to compute the modular decomposition tree of an undirected graph. the breakthrough comes by combining the best elements of two different approaches to the problem.", "date_create": "2007-10-20", "area": "cs.dm", "authors": ["tedder", "corneil", "habib", "paul"]}, {"idpaper": "0710.4187", "title": "universal coding for correlated sources with complementary delivery", "abstract": "this paper deals with a universal coding problem for a certain kind of multiterminal source coding system that we call the complementary delivery coding system. in this system, messages from two correlated sources are jointly encoded, and each decoder has access to one of the two messages to enable it to reproduce the other message. both fixed-to-fixed length and fixed-to-variable length lossless coding schemes are considered. explicit constructions of universal codes and bounds of the error probabilities are clarified via type-theoretical and graph-theoretical analyses. [[keywords]] multiterminal source coding, complementary delivery, universal coding, types of sequences, bipartite graphs", "date_create": "2007-10-23", "area": "cs.it math.it", "authors": ["kimura", "uyematsu", "kuzuoka"]}, {"idpaper": "0710.4410", "title": "a multi-level blocking distinct degree factorization algorithm", "abstract": "we give a new algorithm for performing the distinct-degree factorization of a polynomial p(x) over gf(2), using a multi-level blocking strategy. the coarsest level of blocking replaces gcd computations by multiplications, as suggested by pollard (1975), von zur gathen and shoup (1992), and others. the novelty of our approach is that a finer level of blocking replaces multiplications by squarings, which speeds up the computation in gf(2)[x]/p(x) of certain interval polynomials when p(x) is sparse. as an application we give a fast algorithm to search for all irreducible trinomials x^r + x^s + 1 of degree r over gf(2), while producing a certificate that can be checked in less time than the full search. naive algorithms cost o(r^2) per trinomial, thus o(r^3) to search over all trinomials of given degree r. under a plausible assumption about the distribution of factors of trinomials, the new algorithm has complexity o(r^2 (log r)^{3/2}(log log r)^{1/2}) for the search over all trinomials of degree r. our implementation achieves a speedup of greater than a factor of 560 over the naive algorithm in the case r = 24036583 (a mersenne exponent). using our program, we have found two new primitive trinomials of degree 24036583 over gf(2) (the previous record degree was 6972593).", "date_create": "2007-10-24", "area": "cs.ds", "authors": ["brent", "zimmermann"]}, {"idpaper": "0710.4629", "title": "space-efficient bounded model checking", "abstract": "current algorithms for bounded model checking use sat methods for checking satisfiability of boolean formulae. these methods suffer from the potential memory explosion problem. methods based on the validity of quantified boolean formulae (qbf) allow an exponentially more succinct representation of formulae to be checked, because no \"unrolling\" of the transition relation is required. these methods have not been widely used, because of the lack of an efficient decision procedure for qbf. we evaluate the usage of qbf in bounded model checking (bmc), using general-purpose sat and qbf solvers. we develop a special-purpose decision procedure for qbf used in bmc, and compare our technique with the methods using general-purpose sat and qbf solvers on real-life industrial benchmarks.", "date_create": "2007-10-25", "area": "cs.lo", "authors": ["katz", "hanna", "dershowitz"]}, {"idpaper": "0710.4633", "title": "nano-sim: a step wise equivalent conductance based statistical simulator   for nanotechnology circuit design", "abstract": "new nanotechnology based devices are replacing cmos devices to overcome cmos technology's scaling limitations. however, many such devices exhibit non-monotonic i-v characteristics and uncertain properties which lead to the negative differential resistance (ndr) problem and the chaotic performance. this paper proposes a new circuit simulation approach that can effectively simulate nanotechnology devices with uncertain input sources and negative differential resistance (ndr) problem. the experimental results show a 20-30 times speedup comparing with existing simulators.", "date_create": "2007-10-25", "area": "cs.pf", "authors": ["sukhwani", "padmanabhan", "wang"]}, {"idpaper": "0710.4637", "title": "the accidental detection index as a fault ordering heuristic for   full-scan circuits", "abstract": "we investigate a new fault ordering heuristic for test generation in full-scan circuits. the heuristic is referred to as the accidental detection index. it associates a value adi (f) with every circuit fault f. the heuristic estimates the number of faults that will be detected by a test generated for f. fault ordering is done such that a fault with a higher accidental detection index appears earlier in the ordered fault set and targeted earlier during test generation. this order is effective for generating compact test sets, and for obtaining a test set with a steep fault coverage curve. such a test set has several applications. we present experimental results to demonstrate the effectiveness of the heuristic.", "date_create": "2007-10-25", "area": "cs.oh", "authors": ["pomeranz", "reddy"]}, {"idpaper": "0710.4637", "title": "the accidental detection index as a fault ordering heuristic for   full-scan circuits", "abstract": "we investigate a new fault ordering heuristic for test generation in full-scan circuits. the heuristic is referred to as the accidental detection index. it associates a value adi (f) with every circuit fault f. the heuristic estimates the number of faults that will be detected by a test generated for f. fault ordering is done such that a fault with a higher accidental detection index appears earlier in the ordered fault set and targeted earlier during test generation. this order is effective for generating compact test sets, and for obtaining a test set with a steep fault coverage curve. such a test set has several applications. we present experimental results to demonstrate the effectiveness of the heuristic.", "date_create": "2007-10-25", "area": "cs.oh", "authors": ["pomeranz", "reddy"]}, {"idpaper": "0710.4645", "title": "at-speed logic bist for ip cores", "abstract": "this paper describes a flexible logic bist scheme that features high fault coverage achieved by fault-simulation guided test point insertion, real at-speed test capability for multi-clock designs without clock frequency manipulation, and easy physical implementation due to the use of a low-speed se signal. application results of this scheme to two widely used ip cores are also reported.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["cheon", "lee", "wang", "wen", "hsu", "cho", "park", "chao", "wu"]}, {"idpaper": "0710.4654", "title": "modeling interconnect variability using efficient parametric model order   reduction", "abstract": "assessing ic manufacturing process fluctuations and their impacts on ic interconnect performance has become unavoidable for modern dsm designs. however, the construction of parametric interconnect models is often hampered by the rapid increase in computational cost and model complexity. in this paper we present an efficient yet accurate parametric model order reduction algorithm for addressing the variability of ic interconnect performance. the efficiency of the approach lies in a novel combination of low-rank matrix approximation and multi-parameter moment matching. the complexity of the proposed parametric model order reduction is as low as that of a standard krylov subspace method when applied to a nominal system. under the projection-based framework, our algorithm also preserves the passivity of the resulting parametric models.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["li", "liu", "li", "pileggi", "nassif"]}, {"idpaper": "0710.4660", "title": "thermal-aware task allocation and scheduling for embedded systems", "abstract": "temperature affects not only the reliability but also the performance, power, and cost of the embedded system. this paper proposes a thermal-aware task allocation and scheduling algorithm for embedded systems. the algorithm is used as a sub-routine for hardware/software co-synthesis to reduce the peak temperature and achieve a thermally even distribution while meeting real time constraints. the paper investigates both power-aware and thermal-aware approaches to task allocation and scheduling. the experimental results show that the thermal-aware approach outperforms the power-aware schemes in terms of maximal and average temperature reductions. to the best of our knowledge, this is the first task allocation and scheduling algorithm that takes temperature into consideration.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["hung", "xie", "vijaykrishnan", "kandemir", "irwin"]}, {"idpaper": "0710.4665", "title": "new perspectives and opportunities from the wild west of microelectronic   biochips", "abstract": "application of microelectronic to bioanalysis is an emerging field which holds great promise. from the standpoint of electronic and system design, biochips imply a radical change of perspective, since new, completely different constraints emerge while other usual constraints can be relaxed. while electronic parts of the system can rely on the usual established design-flow, fluidic and packaging design, calls for a new approach which relies significantly on experiments. we hereby make some general considerations based on our experience in the development of biochips for cell analysis.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["manaresi", "medoro", "abonnenc", "auger", "vulto", "romani", "altomare", "tartagni", "guerrieri"]}, {"idpaper": "0710.4665", "title": "new perspectives and opportunities from the wild west of microelectronic   biochips", "abstract": "application of microelectronic to bioanalysis is an emerging field which holds great promise. from the standpoint of electronic and system design, biochips imply a radical change of perspective, since new, completely different constraints emerge while other usual constraints can be relaxed. while electronic parts of the system can rely on the usual established design-flow, fluidic and packaging design, calls for a new approach which relies significantly on experiments. we hereby make some general considerations based on our experience in the development of biochips for cell analysis.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["manaresi", "medoro", "abonnenc", "auger", "vulto", "romani", "altomare", "tartagni", "guerrieri"]}, {"idpaper": "0710.4678", "title": "cmos-based biosensor arrays", "abstract": "cmos-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. examples for molecule- and cell-based approaches and related circuit design issues are discussed.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["thewes", "paulus", "schienle", "hofmann", "frey", "brederlow", "augustyniak", "jenkner", "eversmann", "schindler-bauer", "atzesberger", "holzapfl", "beer", "haneder", "hanke"]}, {"idpaper": "0710.4678", "title": "cmos-based biosensor arrays", "abstract": "cmos-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. examples for molecule- and cell-based approaches and related circuit design issues are discussed.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["thewes", "paulus", "schienle", "hofmann", "frey", "brederlow", "augustyniak", "jenkner", "eversmann", "schindler-bauer", "atzesberger", "holzapfl", "beer", "haneder", "hanke"]}, {"idpaper": "0710.4678", "title": "cmos-based biosensor arrays", "abstract": "cmos-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. examples for molecule- and cell-based approaches and related circuit design issues are discussed.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["thewes", "paulus", "schienle", "hofmann", "frey", "brederlow", "augustyniak", "jenkner", "eversmann", "schindler-bauer", "atzesberger", "holzapfl", "beer", "haneder", "hanke"]}, {"idpaper": "0710.4678", "title": "cmos-based biosensor arrays", "abstract": "cmos-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. examples for molecule- and cell-based approaches and related circuit design issues are discussed.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["thewes", "paulus", "schienle", "hofmann", "frey", "brederlow", "augustyniak", "jenkner", "eversmann", "schindler-bauer", "atzesberger", "holzapfl", "beer", "haneder", "hanke"]}, {"idpaper": "0710.4681", "title": "a quality-of-service mechanism for interconnection networks in   system-on-chips", "abstract": "as moore's law continues to fuel the ability to build ever increasingly complex system-on-chips (socs), achieving performance goals is rising as a critical challenge to completing designs. in particular, the system interconnect must efficiently service a diverse set of data flows with widely ranging quality-of-service (qos) requirements. however, the known solutions for off-chip interconnects such as large-scale networks are not necessarily applicable to the on-chip environment. latency and memory constraints for on-chip interconnects are quite different from larger-scale interconnects. this paper introduces a novel on-chip interconnect arbitration scheme. we show how this scheme can be distributed across a chip for high-speed implementation. we compare the performance of the arbitration scheme with other known interconnect arbitration schemes. existing schemes typically focus heavily on either low latency of service for some initiators, or alternatively on guaranteed bandwidth delivery for other initiators. our scheme allows service latency on some initiators to be traded off smoothly against jitter bounds on other initiators, while still delivering bandwidth guarantees. this scheme is a subset of the qos controls that are available in the sonicsmx? (smx) product.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["weber", "chou", "swarbrick", "wingard"]}, {"idpaper": "0710.4689", "title": "functional equivalence checking for verification of algebraic   transformations on array-intensive source code", "abstract": "development of energy and performance-efficient embedded software is increasingly relying on application of complex transformations on the critical parts of the source code. designers applying such nontrivial source code transformations are often faced with the problem of ensuring functional equivalence of the original and transformed programs. currently they have to rely on incomplete and time-consuming simulation. formal automatic verification of the transformed program against the original is instead desirable. this calls for equivalence checking tools similar to the ones available for comparing digital circuits. we present such a tool to compare array-intensive programs related through a combination of important global transformations like expression propagations, loop and algebraic transformations. when the transformed program fails to pass the equivalence check, the tool provides specific feedback on the possible locations of errors.", "date_create": "2007-10-25", "area": "cs.lo", "authors": ["shashidhar", "bruynooghe", "catthoor", "janssens"]}, {"idpaper": "0710.4690", "title": "rip: an efficient hybrid repeater insertion scheme for low power", "abstract": "this paper presents a novel repeater insertion algorithm for interconnect power minimization. the novelty of our approach is in the judicious integration of an analytical solver and a dynamic programming based method. specifically, the analytical solver chooses a concise repeater library and a small set of repeater location candidates such that the dynamic programming algorithm can be performed fast with little degradation of the solution quality. in comparison with previously reported repeater insertion schemes, within comparable runtimes, our approach achieves up to 37% higher power savings. moreover, for the same design quality, our scheme attains a speedup of two orders of magnitude.", "date_create": "2007-10-25", "area": "cs.oh", "authors": ["liu", "peng", "papaefthymiou"]}, {"idpaper": "0710.4692", "title": "cantilever-based biosensors in cmos technology", "abstract": "single-chip cmos-based biosensors that feature microcantilevers as transducer elements are presented. the cantilevers are functionalized for the capturing of specific analytes, e.g., proteins or dna. the binding of the analyte changes the mechanical properties of the cantilevers such as surface stress and resonant frequency, which can be detected by an integrated wheatstone bridge. the monolithic integrated readout allows for a high signal-to-noise ratio, lowers the sensitivity to external interference and enables autonomous device operation.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["kirstein", "li", "zimmermann", "vancura", "volden", "song", "lichtenberg", "hierlemannn"]}, {"idpaper": "0710.4693", "title": "memory testing under different stress conditions: an industrial   evaluation", "abstract": "this paper presents the effectiveness of various stress conditions (mainly voltage and frequency) on detecting the resistive shorts and open defects in deep sub-micron embedded memories in an industrial environment. simulation studies on very-low voltage, high voltage and at-speed testing show the need of the stress conditions for high quality products; i.e., low defect-per-million (dpm) level, which is driving the semiconductor market today. the above test conditions have been validated to screen out bad devices on real silicon (a test-chip) built on cmos 0.18 um technology. ifa (inductive fault analysis) based simulation technique leads to an efficient fault coverage and dpm estimator, which helps the customers upfront to make decisions on test algorithm implementations under different stress conditions in order to reduce the number of test escapes.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["majhi", "azimane", "gronthoud", "lousberg", "eichenberger", "bowen"]}, {"idpaper": "0710.4709", "title": "analog and digital circuit design in 65 nm cmos: end of the road?", "abstract": "this special session adresses the problems that designers face when implementing analog and digital circuits in nanometer technologies. an introductory embedded tutorial will give an overview of the design problems at hand : the leakage power and process variability and their implications for digital circuits and memories, and the reducing supply voltages, the design productivity and signal integrity problems for embedded analog blocks. next, a panel of experts from both industrial semiconductor houses and design companies, eda vendors and research institutes will present and discuss with the audience their opinions on whether the design road ends at marker \"65nm\" or not.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["gielen", "dehaene", "christie", "draxelmayr", "janssens", "maex", "vucurevich"]}, {"idpaper": "0710.4722", "title": "designer-driven topology optimization for pipelined analog to digital   converters", "abstract": "this paper suggests a practical \"hybrid\" synthesis methodology which integrates designer-derived analytical models for system-level description with simulation-based models at the circuit level. we show how to optimize stage-resolution to minimize the power in a pipelined adc. exploration (via detailed synthesis) of several adc configurations is used to show that a 4-3-2... resolution distribution uses the least power for a 13-bit 40 msps converter in a 0.25 $\\mu$m cmos process.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["chien", "chen", "lou", "ma", "rutenbar", "mukherjee"]}, {"idpaper": "0710.4722", "title": "designer-driven topology optimization for pipelined analog to digital   converters", "abstract": "this paper suggests a practical \"hybrid\" synthesis methodology which integrates designer-derived analytical models for system-level description with simulation-based models at the circuit level. we show how to optimize stage-resolution to minimize the power in a pipelined adc. exploration (via detailed synthesis) of several adc configurations is used to show that a 4-3-2... resolution distribution uses the least power for a 13-bit 40 msps converter in a 0.25 $\\mu$m cmos process.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["chien", "chen", "lou", "ma", "rutenbar", "mukherjee"]}, {"idpaper": "0710.4724", "title": "systematic figure of merit computation for the design of pipeline adc", "abstract": "the emerging concept of soc-ams leads to research new top-down methodologies to aid systems designers in sizing analog and mixed devices. this work applies this idea to the high-level optimization of pipeline adc. considering a given technology, it consists in comparing different configurations according to their imperfections and their architectures without fft computation or time-consuming simulations. the final selection is based on a figure of merit.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["barrandon", "crand", "houzet"]}, {"idpaper": "0710.4731", "title": "leakage-aware interconnect for on-chip network", "abstract": "on-chip networks have been proposed as the interconnect fabric for future systems-on-chip and multi-processors on chip. power is one of the main constraints of these systems and interconnect consumes a significant portion of the power budget. in this paper, we propose four leakage-aware interconnect schemes. our schemes achieve 10.13%~63.57% active leakage savings and 12.35%~95.96% standby leakage savings across schemes while the delay penalty ranges from 0% to 4.69%.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["tsai", "narayaynan", "xie", "irwin"]}, {"idpaper": "0710.4732", "title": "energy efficiency of the ieee 802.15.4 standard in dense wireless   microsensor networks: modeling and improvement perspectives", "abstract": "wireless microsensor networks, which have been the topic of intensive research in recent years, are now emerging in industrial applications. an important milestone in this transition has been the release of the ieee 802.15.4 standard that specifies interoperable wireless physical and medium access control layers targeted to sensor node radios. in this paper, we evaluate the potential of an 802.15.4 radio for use in an ultra low power sensor node operating in a dense network. starting from measurements carried out on the off-the-shelf radio, effective radio activation and link adaptation policies are derived. it is shown that, in a typical sensor network scenario, the average power per node can be reduced down to 211m mm mw. next, the energy consumption breakdown between the different phases of a packet transmission is presented, indicating which part of the transceiver architecture can most effectively be optimized in order to further reduce the radio power, enabling self-powered wireless microsensor networks.", "date_create": "2007-10-25", "area": "cs.ni", "authors": ["bougard", "catthoor", "daly", "chandrakasan", "dehaene"]}, {"idpaper": "0710.4733", "title": "smart temperature sensor for thermal testing of cell-based ics", "abstract": "in this paper we present a simple and efficient built-in temperature sensor for thermal monitoring of standard-cell based vlsi circuits. the proposed smart temperature sensor uses a ring-oscillator composed of complex gates instead of inverters to optimize their linearity. simulation results from a 0.18$\\mu$m cmos technology show that the non-linearity error of the sensor can be reduced when an adequate set of standard logic gates is selected.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["bota", "rosales", "rossello", "segura"]}, {"idpaper": "0710.4743", "title": "efficient solution of language equations using partitioned   representations", "abstract": "a class of discrete event synthesis problems can be reduced to solving language equations f . x &sube; s, where f is the fixed component and s the specification. sequential synthesis deals with fsms when the automata for f and s are prefix closed, and are naturally represented by multi-level networks with latches. for this special case, we present an efficient computation, using partitioned representations, of the most general prefix-closed solution of the above class of language equations. the transition and the output relations of the fsms for f and s in their partitioned form are represented by the sets of output and next state functions of the corresponding networks. experimentally, we show that using partitioned representations is much faster than using monolithic representations, as well as applicable to larger problem instances.", "date_create": "2007-10-25", "area": "cs.lo", "authors": ["mishchenko", "brayton", "jiang", "villa", "yevtushenko"]}, {"idpaper": "0710.4746", "title": "rtk-spec tron: a simulation model of an itron based rtos kernel in   systemc", "abstract": "this paper presents the methodology and the modeling constructs we have developed to capture the real time aspects of rtos simulation models in a system level design language (sldl) like systemc. we describe these constructs and show how they are used to build a simulation model of an rtos kernel targeting the $\\mu$-itron os specification standard.", "date_create": "2007-10-25", "area": "cs.os", "authors": ["hassan", "sakanushi", "takeuchi", "imai"]}, {"idpaper": "0710.4747", "title": "an efficient transparent test scheme for embedded word-oriented memories", "abstract": "memory cores are usually the densest portion with the smallest feature size in system-on-chip (soc) designs. the reliability of memory cores thus has heavy impact on the reliability of socs. transparent test is one of useful technique for improving the reliability of memories during life time. this paper presents a systematic algorithm used for transforming a bit-oriented march test into a transparent word-oriented march test. the transformed transparent march test has shorter test complexity compared with that proposed in the previous works [theory of transparent bist for rams, a transparent online memory test for simultaneous detection of functional faults and soft errors in memories]. for example, if a memory with 32-bit words is tested with march c-, time complexity of the transparent word-oriented test transformed by the proposed scheme is only about 56% or 19% time complexity of the transparent word-oriented test converted by the scheme reported in [theory of transparent bist for rams] or [a transparent online memory test for simultaneous detection of functional faults and soft errors in memories], respectively.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["li", "tseng", "wey"]}, {"idpaper": "0710.4761", "title": "low-cost multi-gigahertz test systems using cmos fpgas and pecl", "abstract": "this paper describes two research projects that develop new low-cost techniques for testing devices with multiple high-speed (2 to 5 gbps) signals. each project uses commercially available components to keep costs low, yet achieves performance characteristics comparable to (and in some ways exceeding) more expensive ate. a common cmos fpga-based logic core provides flexibility, adaptability, and communication with controlling computers while customized positive emitter-coupled logic (pecl) achieves multi-gigahertz data rates with about $\\pm$25ps timing accuracy.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["keezer", "gray", "majid", "taher"]}, {"idpaper": "0710.4761", "title": "low-cost multi-gigahertz test systems using cmos fpgas and pecl", "abstract": "this paper describes two research projects that develop new low-cost techniques for testing devices with multiple high-speed (2 to 5 gbps) signals. each project uses commercially available components to keep costs low, yet achieves performance characteristics comparable to (and in some ways exceeding) more expensive ate. a common cmos fpga-based logic core provides flexibility, adaptability, and communication with controlling computers while customized positive emitter-coupled logic (pecl) achieves multi-gigahertz data rates with about $\\pm$25ps timing accuracy.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["keezer", "gray", "majid", "taher"]}, {"idpaper": "0710.4762", "title": "area-efficient selective multi-threshold cmos design methodology for   standby leakage power reduction", "abstract": "this paper presents a design flow for an improved selective multi-threshold(selective-mt) circuit. the selective-mt circuit is improved so that plural mt-cells can share one switch transistor. we propose the design methodology from rtl(register transfer level) to final layout with optimizing switch transistor structure.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["kitahara", "kawabe", "minami", "seta", "furusawa"]}, {"idpaper": "0710.4803", "title": "hardware engines for bus encryption: a survey of existing techniques", "abstract": "the widening spectrum of applications and services provided by portable and embedded devices bring a new dimension of concerns in security. most of those embedded systems (pay-tv, pdas, mobile phones, etc...) make use of external memory. as a result, the main problem is that data and instructions are constantly exchanged between memory (ram) and cpu in clear form on the bus. this memory may contain confidential data like commercial software or private contents, which either the end-user or the content provider is willing to protect. the goal of this paper is to clearly describe the problem of processor-memory bus communications in this regard and the existing techniques applied to secure the communication channel through encryption - performance overheads implied by those solutions will be extensively discussed in this paper.", "date_create": "2007-10-25", "area": "cs.cr", "authors": ["elbaz", "torres", "sassatelli", "guillemin", "anguille", "bardouillet", "buatois", "rigaud"]}, {"idpaper": "0710.4803", "title": "hardware engines for bus encryption: a survey of existing techniques", "abstract": "the widening spectrum of applications and services provided by portable and embedded devices bring a new dimension of concerns in security. most of those embedded systems (pay-tv, pdas, mobile phones, etc...) make use of external memory. as a result, the main problem is that data and instructions are constantly exchanged between memory (ram) and cpu in clear form on the bus. this memory may contain confidential data like commercial software or private contents, which either the end-user or the content provider is willing to protect. the goal of this paper is to clearly describe the problem of processor-memory bus communications in this regard and the existing techniques applied to secure the communication channel through encryption - performance overheads implied by those solutions will be extensively discussed in this paper.", "date_create": "2007-10-25", "area": "cs.cr", "authors": ["elbaz", "torres", "sassatelli", "guillemin", "anguille", "bardouillet", "buatois", "rigaud"]}, {"idpaper": "0710.4812", "title": "area and throughput trade-offs in the design of pipelined discrete   wavelet transform architectures", "abstract": "the jpeg2000 standard defines the discrete wavelet transform (dwt) as a linear space-to-frequency transform of the image domain in an irreversible compression. this irreversible discrete wavelet transform is implemented by fir filter using 9/7 daubechies coefficients or a lifting scheme of factorizated coefficients from 9/7 daubechies coefficients. this work investigates the tradeoffs between area, power and data throughput (or operating frequency) of several implementations of the discrete wavelet transform using the lifting scheme in various pipeline designs. this paper shows the results of five different architectures synthesized and simulated in fpgas. it concludes that the descriptions with pipelined operators provide the best area-power-operating frequency trade-off over non-pipelined operators descriptions. those descriptions require around 40% more hardware to increase the maximum operating frequency up to 100% and reduce power consumption to less than 50%. starting from behavioral hdl descriptions provide the best area-power-operating frequency trade-off, improving hardware cost and maximum operating frequency around 30% in comparison to structural descriptions for the same power requirement.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["silva", "bampi"]}, {"idpaper": "0710.4814", "title": "picoarray technology: the tool's story", "abstract": "this paper briefly describes the picoarray? architecture, and in particular the deterministic internal communication fabric. the methods that have been developed for debugging and verifying systems using devices from the picoarray family are explained. in order to maximize the computational ability of these devices, hardware debugging support has been kept to a minimum and the methods and tools developed to take this into account.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["duller", "towner", "panesar", "gray", "robbins"]}, {"idpaper": "0710.4820", "title": "isegen: generation of high-quality instruction set extensions by   iterative improvement", "abstract": "customization of processor architectures through instruction set extensions (ises) is an effective way to meet the growing performance demands of embedded applications. a high-quality ise generation approach needs to obtain results close to those achieved by experienced designers, particularly for complex applications that exhibit regularity: expert designers are able to exploit manually such regularity in the data flow graphs to generate high-quality ises. in this paper, we present isegen, an approach that identifies high-quality ises by iterative improvement following the basic principles of the well-known kernighan-lin (k-l) min-cut heuristic. experimental results on a number of mediabench, eembc and cryptographic applications show that our approach matches the quality of the optimal solution obtained by exhaustive search. we also show that our isegen technique is on average 20x faster than a genetic formulation that generates equivalent solutions. furthermore, the ises identified by our technique exhibit 35% more speedup than the genetic solution on a large cryptographic application (aes) by effectively exploiting its regular structure.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["biswas", "banerjee", "dutt", "pozzi", "ienne"]}, {"idpaper": "0710.4823", "title": "a coprocessor for accelerating visual information processing", "abstract": "visual information processing will play an increasingly important role in future electronics systems. in many applications, e.g. video surveillance cameras, data throughput of microprocessors is not sufficient and power consumption is too high. instruction profiling on a typical test algorithm has shown that pixel address calculations are the dominant operations to be optimized. therefore addresslib, a structured scheme for pixel addressing was developed, that can be accelerated by addressengine, a coprocessor for visual information processing. in this paper, the architectural design of addressengine is described, which in the first step supports a subset of the addresslib. dataflow and memory organization are optimized during architectural design. addressengine was implemented in a fpga and was tested with mpeg-7 global motion estimation algorithm. results on processing speed and circuit complexity are given and compared to a pure software implementation. the next step will be the support for the full addresslib, including segment addressing. an outlook on further investigations on dynamic reconfiguration capabilities is given.", "date_create": "2007-10-25", "area": "cs.mm", "authors": ["stechele", "carcel", "herrmann", "simon"]}, {"idpaper": "0710.4823", "title": "a coprocessor for accelerating visual information processing", "abstract": "visual information processing will play an increasingly important role in future electronics systems. in many applications, e.g. video surveillance cameras, data throughput of microprocessors is not sufficient and power consumption is too high. instruction profiling on a typical test algorithm has shown that pixel address calculations are the dominant operations to be optimized. therefore addresslib, a structured scheme for pixel addressing was developed, that can be accelerated by addressengine, a coprocessor for visual information processing. in this paper, the architectural design of addressengine is described, which in the first step supports a subset of the addresslib. dataflow and memory organization are optimized during architectural design. addressengine was implemented in a fpga and was tested with mpeg-7 global motion estimation algorithm. results on processing speed and circuit complexity are given and compared to a pure software implementation. the next step will be the support for the full addresslib, including segment addressing. an outlook on further investigations on dynamic reconfiguration capabilities is given.", "date_create": "2007-10-25", "area": "cs.mm", "authors": ["stechele", "carcel", "herrmann", "simon"]}, {"idpaper": "0710.4826", "title": "the integration of on-line monitoring and reconfiguration functions   using edaa - european design and automation association1149.4 into a safety   critical automotive electronic control unit", "abstract": "this paper presents an innovative application of edaa - european design and automation association 1149.4 and the integrated diagnostic reconfiguration (idr) as tools for the implementation of an embedded test solution for an automotive electronic control unit implemented as a fully integrated mixed signal system. the paper described how the test architecture can be used for fault avoidance with results from a hardware prototype presented. the paper concludes that fault avoidance can be integrated into mixed signal electronic systems to handle key failure modes.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["jeffrey", "cutajar", "prosser", "lickess", "richardson", "riches"]}, {"idpaper": "0710.4826", "title": "the integration of on-line monitoring and reconfiguration functions   using edaa - european design and automation association1149.4 into a safety   critical automotive electronic control unit", "abstract": "this paper presents an innovative application of edaa - european design and automation association 1149.4 and the integrated diagnostic reconfiguration (idr) as tools for the implementation of an embedded test solution for an automotive electronic control unit implemented as a fully integrated mixed signal system. the paper described how the test architecture can be used for fault avoidance with results from a hardware prototype presented. the paper concludes that fault avoidance can be integrated into mixed signal electronic systems to handle key failure modes.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["jeffrey", "cutajar", "prosser", "lickess", "richardson", "riches"]}, {"idpaper": "0710.4827", "title": "debug support, calibration and emulation for multiple processor and   powertrain control socs", "abstract": "the introduction of complex socs with multiple processor cores presents new development challenges, such that development support is now a decisive factor when choosing a system-on-chip (soc). the presented developments support strategy addresses the challenges using both architecture and technology approaches. the multi-core debug support (mcds) architecture provides flexible triggering using cross triggers and a multiple core break and suspend switch. temporal trace ordering is guaranteed down to cycle level by on-chip time stamping. the package sized-ice (psi) approach is a novel method of including trace buffers, overlay memories, processing resources and communication interfaces without changing device behavior. psi requires no external emulation box, as the debug host interfaces directly with the soc using a standard interface.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["mayer", "siebert", "mcdonald-maier"]}, {"idpaper": "0710.4829", "title": "automode - model-based development of automotive software", "abstract": "this paper describes first results from the automode (automotive model-based development) project. the overall goal of the project is to develop an integrated methodology for model-based development of automotive control software, based on problem-specific design notations with an explicit formal foundation. based on the existing autofocus framework, a tool prototype is being developed in order to illustrate and validate the key elements of our approach.", "date_create": "2007-10-25", "area": "cs.se", "authors": ["ziegenbein", "braun", "freund", "bauer", "romberg", "schatz"]}, {"idpaper": "0710.4829", "title": "automode - model-based development of automotive software", "abstract": "this paper describes first results from the automode (automotive model-based development) project. the overall goal of the project is to develop an integrated methodology for model-based development of automotive control software, based on problem-specific design notations with an explicit formal foundation. based on the existing autofocus framework, a tool prototype is being developed in order to illustrate and validate the key elements of our approach.", "date_create": "2007-10-25", "area": "cs.se", "authors": ["ziegenbein", "braun", "freund", "bauer", "romberg", "schatz"]}, {"idpaper": "0710.4838", "title": "a 6bit, 1.2gsps low-power flash-adc in 0.13$\\mu$m digital cmos", "abstract": "a 6bit flash-adc with 1.2gsps, wide analog bandwidth and low power, realized in a standard digital 0.13 $\\mu$m cmos copper technology is presented. employing capacitive interpolation gives various advantages when designing for low power: no need for a reference resistor ladder, implicit sample-and-hold operation, no edge effects in the interpolation network (as compared to resistive interpolation), and a very low input capacitance of only 400ff, which leads to an easily drivable analog converter interface. operating at 1.2gsps the adc achieves an effective resolution bandwidth (erbw) of 700mhz, while consuming 160mw of power. at 600msps we achieve an erbw of 600mhz with only 90mw power consumption, both from a 1.5v supply. this corresponds to outstanding figure-of-merit numbers (fom) of 2.2 and 1.5pj/convstep, respectively. the module area is 0.12mm^2.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["sandner", "clara", "santner", "hartig", "kuttner"]}, {"idpaper": "0710.4839", "title": "a 97mw 110ms/s 12b pipeline adc implemented in 0.18$\\mu$m digital cmos", "abstract": "a 12 bit pipeline adc fabricated in a 0.18 $\\mu$m pure digital cmos technology is presented. its nominal conversion rate is 110ms/s and the nominal supply voltage is 1.8v. the effective number of bits is 10.4 when a 10mhz input signal with 2v_{p-p} signal swing is applied. the occupied silicon area is 0.86mm^2 and the power consumption equals 97mw. a switched capacitor bias current circuit scale the bias current automatically with the conversion rate, which gives scaleable power consumption and full performance of the adc from 20 to 140ms/s.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["andersen", "briskemyr", "telsto", "bjornsen", "bonnerud", "hernes", "moldsvor"]}, {"idpaper": "0710.4844", "title": "a partitioning methodology for accelerating applications in hybrid   reconfigurable platforms", "abstract": "in this paper, we propose a methodology for partitioning and mapping computational intensive applications in reconfigurable hardware blocks of different granularity. a generic hybrid reconfigurable architecture is considered so as the methodology can be applicable to a large number of heterogeneous reconfigurable platforms. the methodology mainly consists of two stages, the analysis and the mapping of the application onto fine and coarse-grain hardware resources. a prototype framework consisting of analysis, partitioning and mapping tools has been also developed. for the coarse-grain reconfigurable hardware, we use our previous-developed high-performance coarse-grain data-path. in this work, the methodology is validated using two real-world applications, an ofdm transmitter and a jpeg encoder. in the case of the ofdm transmitter, a maximum clock cycles decrease of 82% relative to the ones in an all fine-grain mapping solution is achieved. the corresponding performance improvement for the jpeg is 43%.", "date_create": "2007-10-25", "area": "cs.ar", "authors": ["galanis", "milidonis", "theodoridis", "soudris", "goutis"]}, {"idpaper": "0710.4846", "title": "an integrated design and verification methodology for reconfigurable   multimedia systems", "abstract": "recently a lot of multimedia applications are emerging on portable appliances. they require both the flexibility of upgradeable devices (traditionally software based) and a powerful computing engine (typically hardware). in this context, programmable hw and dynamic reconfiguration allow novel approaches to the migration of algorithms from sw to hw. thus, in the frame of the symbad project, we propose an industrial design flow for reconfigurable soc's. the goal of symbad consists of developing a system level design platform for hardware and software soc systems including formal and semi-formal verification techniques.", "date_create": "2007-10-25", "area": "cs.mm cs.lo", "authors": ["borgatti", "capello", "rossi", "lambert", "moussa", "fummi", "pravadelli"]}, {"idpaper": "0711.0114", "title": "geometric spanners with small chromatic number", "abstract": "given an integer $k \\geq 2$, we consider the problem of computing the smallest real number $t(k)$ such that for each set $p$ of points in the plane, there exists a $t(k)$-spanner for $p$ that has chromatic number at most $k$. we prove that $t(2) = 3$, $t(3) = 2$, $t(4) = \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$. we also show that for any $\\epsilon >0$, there exists a $(1+\\epsilon)t(k)$-spanner for $p$ that has $o(|p|)$ edges and chromatic number at most $k$. finally, we consider an on-line variant of the problem where the points of $p$ are given one after another, and the color of a point must be assigned at the moment the point is given. in this setting, we prove that $t(2) = 3$, $t(3) = 1+ \\sqrt{3}$, $t(4) = 1+ \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$.", "date_create": "2007-11-01", "area": "cs.cg", "authors": ["bose", "carmi", "couture", "maheshwari", "smid", "zeh"]}, {"idpaper": "0711.0114", "title": "geometric spanners with small chromatic number", "abstract": "given an integer $k \\geq 2$, we consider the problem of computing the smallest real number $t(k)$ such that for each set $p$ of points in the plane, there exists a $t(k)$-spanner for $p$ that has chromatic number at most $k$. we prove that $t(2) = 3$, $t(3) = 2$, $t(4) = \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$. we also show that for any $\\epsilon >0$, there exists a $(1+\\epsilon)t(k)$-spanner for $p$ that has $o(|p|)$ edges and chromatic number at most $k$. finally, we consider an on-line variant of the problem where the points of $p$ are given one after another, and the color of a point must be assigned at the moment the point is given. in this setting, we prove that $t(2) = 3$, $t(3) = 1+ \\sqrt{3}$, $t(4) = 1+ \\sqrt{2}$, and give upper and lower bounds on $t(k)$ for $k>4$.", "date_create": "2007-11-01", "area": "cs.cg", "authors": ["bose", "carmi", "couture", "maheshwari", "smid", "zeh"]}, {"idpaper": "0711.0251", "title": "faster algorithms for online topological ordering", "abstract": "we present two algorithms for maintaining the topological order of a directed acyclic graph with n vertices, under an online edge insertion sequence of m edges. efficient algorithms for online topological ordering have many applications, including online cycle detection, which is to discover the first edge that introduces a cycle under an arbitrary sequence of edge insertions in a directed graph. in this paper we present efficient algorithms for the online topological ordering problem.   we first present a simple algorithm with running time o(n^{5/2}) for the online topological ordering problem. this is the current fastest algorithm for this problem on dense graphs, i.e., when m > n^{5/3}. we then present an algorithm with running time o((m + nlog n)\\sqrt{m}); this is more efficient for sparse graphs. our results yield an improved upper bound of o(min(n^{5/2}, (m + nlog n)sqrt{m})) for the online topological ordering problem.", "date_create": "2007-11-02", "area": "cs.ds", "authors": ["kavitha", "mathew"]}, {"idpaper": "0711.0325", "title": "self-organising management of grid environments", "abstract": "this paper presents basic concepts, architectural principles and algorithms for efficient resource and security management in cluster computing environments and the grid. the work presented in this paper is funded by btexact and the epsrc project so-grm (gr/s21939).", "date_create": "2007-11-02", "area": "cs.dc", "authors": ["liabotis", "prnjat", "olukemi", "ching", "lazarevic", "sacks", "fisher", "mckee"]}, {"idpaper": "0711.0325", "title": "self-organising management of grid environments", "abstract": "this paper presents basic concepts, architectural principles and algorithms for efficient resource and security management in cluster computing environments and the grid. the work presented in this paper is funded by btexact and the epsrc project so-grm (gr/s21939).", "date_create": "2007-11-02", "area": "cs.dc", "authors": ["liabotis", "prnjat", "olukemi", "ching", "lazarevic", "sacks", "fisher", "mckee"]}, {"idpaper": "0711.0325", "title": "self-organising management of grid environments", "abstract": "this paper presents basic concepts, architectural principles and algorithms for efficient resource and security management in cluster computing environments and the grid. the work presented in this paper is funded by btexact and the epsrc project so-grm (gr/s21939).", "date_create": "2007-11-02", "area": "cs.dc", "authors": ["liabotis", "prnjat", "olukemi", "ching", "lazarevic", "sacks", "fisher", "mckee"]}, {"idpaper": "0711.1055", "title": "simple recursive games", "abstract": "we define the class of \"simple recursive games\". a simple recursive game is defined as a simple stochastic game (a notion due to anne condon), except that we allow arbitrary real payoffs but disallow moves of chance. we study the complexity of solving simple recursive games and obtain an almost-linear time comparison-based algorithm for computing an equilibrium of such a game. the existence of a linear time comparison-based algorithm remains an open problem.", "date_create": "2007-11-07", "area": "cs.gt cs.ds", "authors": ["andersson", "hansen", "miltersen", "sorensen"]}, {"idpaper": "0711.1682", "title": "data structures for mergeable trees", "abstract": "motivated by an application in computational topology, we consider a novel variant of the problem of efficiently maintaining dynamic rooted trees. this variant requires merging two paths in a single operation. in contrast to the standard problem, in which only one tree arc changes at a time, a single merge operation can change many arcs. in spite of this, we develop a data structure that supports merges on an n-node forest in o(log^2 n) amortized time and all other standard tree operations in o(log n) time (amortized, worst-case, or randomized depending on the underlying data structure). for the special case that occurs in the motivating application, in which arbitrary arc deletions (cuts) are not allowed, we give a data structure with an o(log n) time bound per operation. this is asymptotically optimal under certain assumptions. for the even-more special case in which both cuts and parent queries are disallowed, we give an alternative o(log n)-time solution that uses standard dynamic trees as a black box. this solution also applies to the motivating application. our methods use previous work on dynamic trees in various ways, but the analysis of each algorithm requires novel ideas. we also investigate lower bounds for the problem under various assumptions.", "date_create": "2007-11-11", "area": "cs.ds", "authors": ["georgiadis", "kaplan", "shafrir", "tarjan", "werneck"]}, {"idpaper": "0711.2087", "title": "query evaluation and optimization in the semantic web", "abstract": "we address the problem of answering web ontology queries efficiently. an ontology is formalized as a deductive ontology base (dob), a deductive database that comprises the ontology's inference axioms and facts. a cost-based query optimization technique for dob is presented. a hybrid cost model is proposed to estimate the cost and cardinality of basic and inferred facts. cardinality and cost of inferred facts are estimated using an adaptive sampling technique, while techniques of traditional relational cost models are used for estimating the cost of basic facts and conjunctive ontology queries. finally, we implement a dynamic-programming optimization algorithm to identify query evaluation plans that minimize the number of intermediate inferred facts. we modeled a subset of the web ontology language owl lite as a dob, and performed an experimental study to analyze the predictive capacity of our cost model and the benefits of the query optimization technique. our study has been conducted over synthetic and real-world owl ontologies, and shows that the techniques are accurate and improve query performance. to appear in theory and practice of logic programming (tplp).", "date_create": "2007-11-13", "area": "cs.db cs.lo", "authors": ["ruckhaus", "ruiz", "vidal"]}, {"idpaper": "0711.2155", "title": "guarded hybrid knowledge bases", "abstract": "recently, there has been a lot of interest in the integration of description logics and rules on the semantic web.we define guarded hybrid knowledge bases (or g-hybrid knowledge bases) as knowledge bases that consist of a description logic knowledge base and a guarded logic program, similar to the dl+log knowledge bases from (rosati 2006). g-hybrid knowledge bases enable an integration of description logics and logic programming where, unlike in other approaches, variables in the rules of a guarded program do not need to appear in positive non-dl atoms of the body, i.e. dl atoms can act as guards as well. decidability of satisfiability checking of g-hybrid knowledge bases is shown for the particular dl dlro, which is close to owl dl, by a reduction to guarded programs under the open answer set semantics. moreover, we show 2-exptime-completeness for satisfiability checking of such g-hybrid knowledge bases. finally, we discuss advantages and disadvantages of our approach compared with dl+log knowledge bases.", "date_create": "2007-11-14", "area": "cs.lo", "authors": ["heymans", "de bruijn", "predoiu", "feier", "van nieuwenborgh"]}, {"idpaper": "0711.3277", "title": "3-d self-assembled soi mems: an example of multiphysics simulation", "abstract": "mems devices are typical systems where multiphysics simulations are unavoidable. in this work, we present possible applications of 3-d self-assembled soi (silicon-on-insulator) mems such as, for instance, thermal actuators and flow sensors. the numerical simulations of these microsystems are presented. structural and thermal parts have to be strongly coupled for correctly describing the fabrication process and for simulating the behavior of these 3-d soi mems.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["mendez", "louis", "paquay", "de vincenzo", "klapka", "rochus", "iker", "andr\u00e9", "raskin"]}, {"idpaper": "0711.3279", "title": "influence of the feedback filter on the response of the pulsed digital   oscillator", "abstract": "this paper introduces a new feedback topology for the pulsed digital oscillator (pdo) and compares it to the classical topology. the `classic' or single feedback topology, introduced in previous works, shows a strong behavior dependence on the damping losses in the mems resonator. a new double feedback topology is introduced here in order to help solving this problem. comparative discrete-time simulations and preliminary experimental measurements have been carried out for both topologies, showing how the new double feedback topology may increase pdo performance for some frequency ranges.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["dominguez", "pons", "ricart", "juillard", "colinet"]}, {"idpaper": "0711.3289", "title": "electromechanical reliability testing of three-axial silicon force   sensors", "abstract": "this paper reports on the systematic electromechanical characterization of a new three-axial force sensor used in dimensional metrology of micro components. the siliconbased sensor system consists of piezoresistive mechanicalstress transducers integrated in thin membrane hinges supporting a suspended flexible cross structure. the mechanical behavior of the fragile micromechanical structure isanalyzed for both static and dynamic load cases. this work demonstrates that the silicon microstructure withstands static forces of 1.16n applied orthogonally to the front-side of the structure. a statistical weibull analysis of the measured data shows that these values are significantly reduced if the normal force is applied to the back of the sensor. improvements of the sensor system design for future development cycles are derived from the measurement results.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["spinner", "bartholomeyczik", "becker", "doelle", "paul", "polian", "roth", "seitz", "ruther"]}, {"idpaper": "0711.3289", "title": "electromechanical reliability testing of three-axial silicon force   sensors", "abstract": "this paper reports on the systematic electromechanical characterization of a new three-axial force sensor used in dimensional metrology of micro components. the siliconbased sensor system consists of piezoresistive mechanicalstress transducers integrated in thin membrane hinges supporting a suspended flexible cross structure. the mechanical behavior of the fragile micromechanical structure isanalyzed for both static and dynamic load cases. this work demonstrates that the silicon microstructure withstands static forces of 1.16n applied orthogonally to the front-side of the structure. a statistical weibull analysis of the measured data shows that these values are significantly reduced if the normal force is applied to the back of the sensor. improvements of the sensor system design for future development cycles are derived from the measurement results.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["spinner", "bartholomeyczik", "becker", "doelle", "paul", "polian", "roth", "seitz", "ruther"]}, {"idpaper": "0711.3290", "title": "an active chaotic micromixer integrating thermal actuation associating   pdms and silicon microtechnology", "abstract": "due to scaling laws, in microfluidic, flows are laminar. consequently, mixing between two liquids is mainly obtained by natural diffusion which may take a long time or equivalently requires centimetre length channels. to reduce time and length for mixing, it is possible to generate chaotic-like flows either by modifying the channel geometry or by creating an external perturbation of the flow. in this paper, an active micromixer is presented consisting on thermal actuation with heating resistors. in order to disturb the liquid flow, an oscillating transverse flow is generated by heating the liquid. depending on the value of boiling point, either bubble expansion or volumetric dilation controlled the transverse flow amplitude. a chaotic like mixing is then induced under particular conditions depending on volume expansion, liquid velocity, frequency of actuation... this solution presents the advantage to achieve mixing in a very short time (1s) and along a short channel distance (channel width). it can also be integrated in a more complex device due to actuator integration with microfluidics.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["fran\u00e7ais", "jullien", "rousseau", "poulichet", "desportes", "chouai", "lefevre", "delaire"]}, {"idpaper": "0711.3292", "title": "a silicon-based micro gas turbine engine for power generation", "abstract": "this paper reports on our research in developing a micro power generation system based on gas turbine engine and piezoelectric converter. the micro gas turbine engine consists of a micro combustor, a turbine and a centrifugal compressor. comprehensive simulation has been implemented to optimal the component design. we have successfully demonstrated a silicon-based micro combustor, which consists of seven layers of silicon structures. a hairpin-shaped design is applied to the fuel/air recirculation channel. the micro combustor can sustain a stable combustion with an exit temperature as high as 1600 k. we have also successfully developed a micro turbine device, which is equipped with enhanced micro air-bearings and driven by compressed air. a rotation speed of 15,000 rpm has been demonstrated during lab test. in this paper, we will introduce our research results major in the development of micro combustor and micro turbine test device.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["shan", "wang", "maeda", "sun", "wu", "hua"]}, {"idpaper": "0711.3294", "title": "energy conversion using new thermoelectric generator", "abstract": "during recent years, microelectronics helped to develop complex and varied technologies. it appears that many of these technologies can be applied successfully to realize seebeck micro generators: photolithography and deposition methods allow to elaborate thin thermoelectric structures at the micro-scale level. our goal is to scavenge energy by developing a miniature power source for operating electronic components. first bi and sb micro-devices on silicon glass substrate have been manufactured with an area of 1cm2 including more than one hundred junctions. each step of process fabrication has been optimized: photolithography, deposition process, anneals conditions and metallic connections. different device structures have been realized with different micro-line dimensions. each devices performance will be reviewed and discussed in function of their design structure.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["savelli", "plissonnier", "bablet", "salvi", "fournier"]}, {"idpaper": "0711.3305", "title": "measurement technique for elastic and mechanical properties of   polycrystalline silicon-germanium films using surface acoustic waves and   projection masks", "abstract": "using rayleigh surface acoustic waves (saw), the young's modulus, the density and the thickness of polycrystalline silicon-germanium (sige) films deposited on silicon and sio2 were measured, in excellent agreement with theory. the dispersion curve of the propagating saw is calculated with a boundary element method (bem)-model based on green's functions. the propagating saw is generated with a nanosecond laser in a narrowband scheme projecting stripes from a mask on the surface of the sample. for this purpose a glass mask and a liquid crystal display (lcd) mask are used. the slope of the saw is then measured using a probe beam setup. from the wavelength of the mask and the frequency of the measured saw, the dispersion curve is determined point by point. fitting the bem-model to the measured nonlinear dispersion curve provides several physical parameters simultaneously. in the present work this is demonstrated for the young's modulus, the density and the thickness of sige films. the results from the narrowband scheme measurement are in excellent agreement with separated measurements of the thickness (profilometer), the density (balance) and the young's modulus (nanoindenter).", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["bennis", "leinenbach", "raudzis", "m\u00fcller-fiedler", "kronm\u00fcller"]}, {"idpaper": "0711.3309", "title": "optimization of piezoelectric electrical generators powered by random   vibrations", "abstract": "this paper compares the performances of a vibrationpowered electrical generators using pzt piezoelectric ceramic associated to two different power conditioning circuits. a new approach of the piezoelectric power conversion based on a nonlinear voltage processing is presented and implemented with a particular power conditioning circuit topology. theoretical predictions and experimental results show that the nonlinear processing technique may increase the power harvested by a factor up to 4 compared to the standard optimization technique. properties of this new technique are analyzed in particular in the case of broadband, random vibrations, and compared to those of the standard interface.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["lefeuvre", "badel", "richard", "petit", "guyomar"]}, {"idpaper": "0711.3309", "title": "optimization of piezoelectric electrical generators powered by random   vibrations", "abstract": "this paper compares the performances of a vibrationpowered electrical generators using pzt piezoelectric ceramic associated to two different power conditioning circuits. a new approach of the piezoelectric power conversion based on a nonlinear voltage processing is presented and implemented with a particular power conditioning circuit topology. theoretical predictions and experimental results show that the nonlinear processing technique may increase the power harvested by a factor up to 4 compared to the standard optimization technique. properties of this new technique are analyzed in particular in the case of broadband, random vibrations, and compared to those of the standard interface.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["lefeuvre", "badel", "richard", "petit", "guyomar"]}, {"idpaper": "0711.3314", "title": "macro and micro scale electromagnetic kinetic energy harvesting   generators", "abstract": "this paper is concerned with generators that harvest electrical energy from the kinetic energy present in the sensor nodes environment. these generators have the potential to replace or augment battery power which has a limited lifetime and requires periodic replacement which limits the placement and application of the sensor node.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["beeby", "tudor", "torah", "koukharenko", "roberts", "o'donnell", "roy"]}, {"idpaper": "0711.3321", "title": "electrostatic actuators operating in liquid environment : suppression of   pull-in instability and dynamic response", "abstract": "this paper presents results about fabrication and operation of electrostatic actuators in liquids with various permittivities. in the static mode, we provide experimental and theoretical demonstration that the pull-in effect can be shifted beyond one third of the initial gap and even be eliminated when electrostatic actuators are operated in liquids. this should benefit to applications in microfluidics requiring either binary state actuation (e.g. pumps, valves) or continuous displacements over the whole gap (e.g. microtweezers). in dynamic mode, actuators like micro-cantilevers present a great interest for atomic force microscopy (afm) in liquids. as this application requires a good understanding of the cantilever resonance frequency and q-factor, an analytical modeling in liquid environment has been established. the theoretically derived curves are validated by experimental results using a nitride encapsulated cantilever with integrated electrostatic actuation. electrode potential screening and undesirable electrochemistry in dielectric liquids are counteracted using ac-voltages. both experimental and theoretical results should prove useful in micro-cantilever design for afm in liquids.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["rollier", "faucher", "legrand", "collard", "buchaillot"]}, {"idpaper": "0711.3321", "title": "electrostatic actuators operating in liquid environment : suppression of   pull-in instability and dynamic response", "abstract": "this paper presents results about fabrication and operation of electrostatic actuators in liquids with various permittivities. in the static mode, we provide experimental and theoretical demonstration that the pull-in effect can be shifted beyond one third of the initial gap and even be eliminated when electrostatic actuators are operated in liquids. this should benefit to applications in microfluidics requiring either binary state actuation (e.g. pumps, valves) or continuous displacements over the whole gap (e.g. microtweezers). in dynamic mode, actuators like micro-cantilevers present a great interest for atomic force microscopy (afm) in liquids. as this application requires a good understanding of the cantilever resonance frequency and q-factor, an analytical modeling in liquid environment has been established. the theoretically derived curves are validated by experimental results using a nitride encapsulated cantilever with integrated electrostatic actuation. electrode potential screening and undesirable electrochemistry in dielectric liquids are counteracted using ac-voltages. both experimental and theoretical results should prove useful in micro-cantilever design for afm in liquids.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["rollier", "faucher", "legrand", "collard", "buchaillot"]}, {"idpaper": "0711.3326", "title": "characterization of flexible rf microcoil dedicated to surface mri", "abstract": "in magnetic resonance imaging (mri), to achieve sufficient signal to noise ratio (snr), the electrical performance of the rf coil is critical. we developed a device (microcoil) based on the original concept of monolithic resonator. this paper presents the used fabrication process based on micromoulding. the dielectric substrates are flexible thin films of polymer, which allow the microcoil to be form fitted to none-plane surface. electrical characterizations of the rf coils are first performed and results are compared to the attempted values. proton mri of a saline phantom using a flexible rf coil of 15 mm in diameter is performed. when the coil is conformed to the phantom surface, a snr gain up to 2 is achieved as compared to identical but planar rf coil. finally, the flexible coil is used in vivo to perform mri with high spatial resolution on a mouse using a small animal dedicated scanner operating at in a 2.35 t.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["woytasik", "ginefri", "raynaud", "poirier-quinot", "dufour-gergam", "grandchamp", "darrasse", "robert", "gilles", "martincic", "girard"]}, {"idpaper": "0711.3326", "title": "characterization of flexible rf microcoil dedicated to surface mri", "abstract": "in magnetic resonance imaging (mri), to achieve sufficient signal to noise ratio (snr), the electrical performance of the rf coil is critical. we developed a device (microcoil) based on the original concept of monolithic resonator. this paper presents the used fabrication process based on micromoulding. the dielectric substrates are flexible thin films of polymer, which allow the microcoil to be form fitted to none-plane surface. electrical characterizations of the rf coils are first performed and results are compared to the attempted values. proton mri of a saline phantom using a flexible rf coil of 15 mm in diameter is performed. when the coil is conformed to the phantom surface, a snr gain up to 2 is achieved as compared to identical but planar rf coil. finally, the flexible coil is used in vivo to perform mri with high spatial resolution on a mouse using a small animal dedicated scanner operating at in a 2.35 t.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["woytasik", "ginefri", "raynaud", "poirier-quinot", "dufour-gergam", "grandchamp", "darrasse", "robert", "gilles", "martincic", "girard"]}, {"idpaper": "0711.3326", "title": "characterization of flexible rf microcoil dedicated to surface mri", "abstract": "in magnetic resonance imaging (mri), to achieve sufficient signal to noise ratio (snr), the electrical performance of the rf coil is critical. we developed a device (microcoil) based on the original concept of monolithic resonator. this paper presents the used fabrication process based on micromoulding. the dielectric substrates are flexible thin films of polymer, which allow the microcoil to be form fitted to none-plane surface. electrical characterizations of the rf coils are first performed and results are compared to the attempted values. proton mri of a saline phantom using a flexible rf coil of 15 mm in diameter is performed. when the coil is conformed to the phantom surface, a snr gain up to 2 is achieved as compared to identical but planar rf coil. finally, the flexible coil is used in vivo to perform mri with high spatial resolution on a mouse using a small animal dedicated scanner operating at in a 2.35 t.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["woytasik", "ginefri", "raynaud", "poirier-quinot", "dufour-gergam", "grandchamp", "darrasse", "robert", "gilles", "martincic", "girard"]}, {"idpaper": "0711.3333", "title": "process issues for a multi-layer microelectrofluidic platform", "abstract": "we report on the development of some process capabilities for a polymer-based, multi-layer microelectrofluidic platform, namely: the hot embossing process, metallization on polymer and polymer bonding. hot embossing experiments were conducted to look at the effects of load applied, embossing temperature and embossing time on the fidelity of line arrays representing micro channels. the results revealed that the embossing temperature is a more sensitive parameter than the others due to its large effect on the polymer material's viscoelastic properties. dynamic mechanical analysis (dma) on polymethyl methacrylate (pmma) revealed a steep glass transition over a 20 oc range, with the material losing more than 95 % of its storage modulus. the data explained the hot embossing results which showed large change in the embossed channel dimensions when the temperature is within the glass transition range. it was demonstrated that the micro-printing of silver epoxy is a possible low-cost technique in the mass production of disposable lab chips. an interconnecting network of electrical traces was fabricated in the form of a four-layer pmma-based device. a four pmma layer device with interconnecting microfluidic channels was also fabricated and tested.", "date_create": "2007-11-21", "area": "cs.oh", "authors": ["ng", "wang", "tjeung", "de rooij"]}, {"idpaper": "0711.3419", "title": "translating owl and semantic web rules into prolog: moving toward   description logic programs", "abstract": "to appear in theory and practice of logic programming (tplp), 2008.   we are researching the interaction between the rule and the ontology layers of the semantic web, by comparing two options: 1) using owl and its rule extension swrl to develop an integrated ontology/rule language, and 2) layering rules on top of an ontology with ruleml and owl. toward this end, we are developing the sworier system, which enables efficient automated reasoning on ontologies and rules, by translating all of them into prolog and adding a set of general rules that properly capture the semantics of owl. we have also enabled the user to make dynamic changes on the fly, at run time. this work addresses several of the concerns expressed in previous work, such as negation, complementary classes, disjunctive heads, and cardinality, and it discusses alternative approaches for dealing with inconsistencies in the knowledge base. in addition, for efficiency, we implemented techniques called extensionalization, avoiding reanalysis, and code minimization.", "date_create": "2007-11-21", "area": "cs.ai", "authors": ["samuel", "obrst", "stoutenberg", "fox", "franklin", "johnson", "laskey", "nichols", "lopez", "peterson"]}, {"idpaper": "0711.3591", "title": "an estimation of distribution algorithm with intelligent local search   for rule-based nurse rostering", "abstract": "this paper proposes a new memetic evolutionary algorithm to achieve explicit learning in rule-based nurse rostering, which involves applying a set of heuristic rules for each nurse's assignment. the main framework of the algorithm is an estimation of distribution algorithm, in which an ant-miner methodology improves the individual solutions produced in each generation. unlike our previous work (where learning is implicit), the learning in the memetic estimation of distribution algorithm is explicit, i.e. we are able to identify building blocks directly. the overall approach learns by building a probabilistic model, i.e. an estimation of the probability distribution of individual nurse-rule pairs that are used to construct schedules. the local search processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive higher rewards. a challenging real world nurse rostering problem is used as the test problem. computational results show that the proposed approach outperforms most existing approaches. it is suggested that the learning methodologies suggested in this paper may be applied to other scheduling problems where schedules are built systematically according to specific rules", "date_create": "2007-11-22", "area": "cs.ne cs.ce", "authors": ["aickelin", "burke", "li"]}, {"idpaper": "0711.4516", "title": "fluoroscopy-based navigation system in spine surgery", "abstract": "the variability in width, height, and spatial orientation of a spinal pedicle makes pedicle screw insertion a delicate operation. the aim of the current paper is to describe a computer-assisted surgical navigation system based on fluoroscopic x-ray image calibration and three-dimensional optical localizers in order to reduce radiation exposure while increasing accuracy and reliability of the surgical procedure for pedicle screw insertion. instrumentation using transpedicular screw fixation was performed: in a first group, a conventional surgical procedure was carried out with 26 patients (138 screws); in a second group, a navigated surgical procedure (virtual fluoroscopy) was performed with 26 patients (140 screws). evaluation of screw placement in every case was done by using plain x-rays and post-operative computer tomography scan. a 5 per cent cortex penetration (7 of 140 pedicle screws) occurred for the computer-assisted group. a 13 per cent penetration (18 of 138 pedicle screws) occurred for the non computer-assisted group. the radiation running time for each vertebra level (two screws) reached 3.5 s on average in the computer-assisted group and 11.5 s on average in the non computer-assisted group. the operative time for two screws on the same vertebra level reaches 10 min on average in the non computer-assisted group and 11.9 min on average in the computer-assisted group. the fluoroscopy-based (two-dimensional) navigation system for pedicle screw insertion is a safe and reliable procedure for surgery in the lower thoracic and lumbar spine.", "date_create": "2007-11-28", "area": "cs.oh", "authors": ["merloz", "troccaz", "vouaillat", "vasile", "tonetti", "eid", "plaweski"]}, {"idpaper": "0711.4516", "title": "fluoroscopy-based navigation system in spine surgery", "abstract": "the variability in width, height, and spatial orientation of a spinal pedicle makes pedicle screw insertion a delicate operation. the aim of the current paper is to describe a computer-assisted surgical navigation system based on fluoroscopic x-ray image calibration and three-dimensional optical localizers in order to reduce radiation exposure while increasing accuracy and reliability of the surgical procedure for pedicle screw insertion. instrumentation using transpedicular screw fixation was performed: in a first group, a conventional surgical procedure was carried out with 26 patients (138 screws); in a second group, a navigated surgical procedure (virtual fluoroscopy) was performed with 26 patients (140 screws). evaluation of screw placement in every case was done by using plain x-rays and post-operative computer tomography scan. a 5 per cent cortex penetration (7 of 140 pedicle screws) occurred for the computer-assisted group. a 13 per cent penetration (18 of 138 pedicle screws) occurred for the non computer-assisted group. the radiation running time for each vertebra level (two screws) reached 3.5 s on average in the computer-assisted group and 11.5 s on average in the non computer-assisted group. the operative time for two screws on the same vertebra level reaches 10 min on average in the non computer-assisted group and 11.9 min on average in the computer-assisted group. the fluoroscopy-based (two-dimensional) navigation system for pedicle screw insertion is a safe and reliable procedure for surgery in the lower thoracic and lumbar spine.", "date_create": "2007-11-28", "area": "cs.oh", "authors": ["merloz", "troccaz", "vouaillat", "vasile", "tonetti", "eid", "plaweski"]}, {"idpaper": "0711.4516", "title": "fluoroscopy-based navigation system in spine surgery", "abstract": "the variability in width, height, and spatial orientation of a spinal pedicle makes pedicle screw insertion a delicate operation. the aim of the current paper is to describe a computer-assisted surgical navigation system based on fluoroscopic x-ray image calibration and three-dimensional optical localizers in order to reduce radiation exposure while increasing accuracy and reliability of the surgical procedure for pedicle screw insertion. instrumentation using transpedicular screw fixation was performed: in a first group, a conventional surgical procedure was carried out with 26 patients (138 screws); in a second group, a navigated surgical procedure (virtual fluoroscopy) was performed with 26 patients (140 screws). evaluation of screw placement in every case was done by using plain x-rays and post-operative computer tomography scan. a 5 per cent cortex penetration (7 of 140 pedicle screws) occurred for the computer-assisted group. a 13 per cent penetration (18 of 138 pedicle screws) occurred for the non computer-assisted group. the radiation running time for each vertebra level (two screws) reached 3.5 s on average in the computer-assisted group and 11.5 s on average in the non computer-assisted group. the operative time for two screws on the same vertebra level reaches 10 min on average in the non computer-assisted group and 11.9 min on average in the computer-assisted group. the fluoroscopy-based (two-dimensional) navigation system for pedicle screw insertion is a safe and reliable procedure for surgery in the lower thoracic and lumbar spine.", "date_create": "2007-11-28", "area": "cs.oh", "authors": ["merloz", "troccaz", "vouaillat", "vasile", "tonetti", "eid", "plaweski"]}, {"idpaper": "0711.4516", "title": "fluoroscopy-based navigation system in spine surgery", "abstract": "the variability in width, height, and spatial orientation of a spinal pedicle makes pedicle screw insertion a delicate operation. the aim of the current paper is to describe a computer-assisted surgical navigation system based on fluoroscopic x-ray image calibration and three-dimensional optical localizers in order to reduce radiation exposure while increasing accuracy and reliability of the surgical procedure for pedicle screw insertion. instrumentation using transpedicular screw fixation was performed: in a first group, a conventional surgical procedure was carried out with 26 patients (138 screws); in a second group, a navigated surgical procedure (virtual fluoroscopy) was performed with 26 patients (140 screws). evaluation of screw placement in every case was done by using plain x-rays and post-operative computer tomography scan. a 5 per cent cortex penetration (7 of 140 pedicle screws) occurred for the computer-assisted group. a 13 per cent penetration (18 of 138 pedicle screws) occurred for the non computer-assisted group. the radiation running time for each vertebra level (two screws) reached 3.5 s on average in the computer-assisted group and 11.5 s on average in the non computer-assisted group. the operative time for two screws on the same vertebra level reaches 10 min on average in the non computer-assisted group and 11.9 min on average in the computer-assisted group. the fluoroscopy-based (two-dimensional) navigation system for pedicle screw insertion is a safe and reliable procedure for surgery in the lower thoracic and lumbar spine.", "date_create": "2007-11-28", "area": "cs.oh", "authors": ["merloz", "troccaz", "vouaillat", "vasile", "tonetti", "eid", "plaweski"]}, {"idpaper": "0711.4523", "title": "robot-based tele-echography: clinical evaluation of the ter system in   abdominal aortic exploration", "abstract": "objective: the ter system is a robot-based tele-echography system allowing remote ultrasound examination. the specialist moves a mock-up of the ultrasound probe at the master site, and the robot reproduces the movements of the real probe, which sends back ultrasound images and force feedback. this tool could be used to perform ultrasound examinations in small health care centers or from isolated sites. the objective of this study was to prove, under real conditions, the feasibility and reliability of the ter system in detecting abdominal aortic and iliac aneurysms. methods: fifty-eight patients were included in 2 centers in brest and grenoble, france. the remote examination was compared with the reference standard, the bedside examination, for aorta and iliac artery diameter measurement, detection and description of aneurysms, detection of atheromatosis, the duration of the examination, and acceptability. results: all aneurysms (8) were detected by both techniques as intramural thrombosis and extension to the iliac arteries. the interobserver correlation coefficient was 0.982 (p < .0001) for aortic diameters. the rate of concordance between 2 operators in evaluating atheromatosis was 84% +/- 11% (95% confidence interval). conclusions: our study on 58 patients suggests that the ter system could be a reliable, acceptable, and effective robot-based system for performing remote abdominal aortic ultrasound examinations. research is continuing to improve the equipment for general abdominal use.", "date_create": "2007-11-28", "area": "cs.oh", "authors": ["martinelli", "bosson", "bressollette", "pelissier", "boidard", "troccaz", "cinquin"]}, {"idpaper": "0711.4562", "title": "near-deterministic inference of as relationships", "abstract": "the discovery of autonomous systems (ases) interconnections and the inference of their commercial type-of-relationships (tor) has been extensively studied during the last few years. the main motivation is to accurately calculate as-level paths and to provide better topological view of the internet. an inherent problem in current algorithms is their extensive use of heuristics. such heuristics incur unbounded errors which are spread over all inferred relationships. we propose a near-deterministic algorithm for solving the tor inference problem. our algorithm uses as input the internet core, which is a dense sub-graph of top-level ases. we test several methods for creating such a core and demonstrate the robustness of the algorithm to the core's size and density, the inference period, and errors in the core.   we evaluate our algorithm using as-level paths collected from routeviews bgp paths and dimes traceroute measurements. our proposed algorithm deterministically infers over 95% of the approximately 58,000 as topology links. the inference becomes stable when using a week worth of data and as little as 20 ases in the core. the algorithm infers 2-3 times more peer-to-peer relationships in edges discovered only by dimes than in routeviews edges, validating the dimes promise to discover periphery as edges.", "date_create": "2007-11-28", "area": "cs.ni", "authors": ["shavitt", "shir", "weinsberg"]}, {"idpaper": "0711.4944", "title": "development of miniaturized light endoscope-holder robot for   laparoscopic surgery", "abstract": "purpose: we have conducted experiments with an innovatively designed robot endoscope holder for laparoscopic surgery that is small and low cost. materials and methods: a compact light endoscope robot (ler) that is placed on the patient's skin and can be used with the patient in the lateral or dorsal supine position was tested on cadavers and laboratory pigs in order to allow successive modifications. the current control system is based on voice recognition. the range of vision is 360 degrees with an angle of 160 degrees . twenty-three procedures were performed. results: the tests made it possible to advance the prototype on a variety of aspects, including reliability, steadiness, ergonomics, and dimensions. the ease of installation of the robot, which takes only 5 minutes, and the easy handling made it possible for 21 of the 23 procedures to be performed without an assistant. conclusion: the ler is a camera holder guided by the surgeon's voice that can eliminate the need for an assistant during laparoscopic surgery. the ease of installation and manufacture should make it an effective and inexpensive system for use on patients in the lateral and dorsal supine positions. randomized clinical trials will soon validate a new version of this robot prior to marketing.", "date_create": "2007-11-30", "area": "cs.oh", "authors": ["long", "cinquin", "troccaz", "voros", "descotes", "berkelman", "letoublon", "rambeaud"]}, {"idpaper": "0711.4944", "title": "development of miniaturized light endoscope-holder robot for   laparoscopic surgery", "abstract": "purpose: we have conducted experiments with an innovatively designed robot endoscope holder for laparoscopic surgery that is small and low cost. materials and methods: a compact light endoscope robot (ler) that is placed on the patient's skin and can be used with the patient in the lateral or dorsal supine position was tested on cadavers and laboratory pigs in order to allow successive modifications. the current control system is based on voice recognition. the range of vision is 360 degrees with an angle of 160 degrees . twenty-three procedures were performed. results: the tests made it possible to advance the prototype on a variety of aspects, including reliability, steadiness, ergonomics, and dimensions. the ease of installation of the robot, which takes only 5 minutes, and the easy handling made it possible for 21 of the 23 procedures to be performed without an assistant. conclusion: the ler is a camera holder guided by the surgeon's voice that can eliminate the need for an assistant during laparoscopic surgery. the ease of installation and manufacture should make it an effective and inexpensive system for use on patients in the lateral and dorsal supine positions. randomized clinical trials will soon validate a new version of this robot prior to marketing.", "date_create": "2007-11-30", "area": "cs.oh", "authors": ["long", "cinquin", "troccaz", "voros", "descotes", "berkelman", "letoublon", "rambeaud"]}, {"idpaper": "0712.0769", "title": "towards 3d ultrasound image based soft tissue tracking: a transrectal   ultrasound prostate image alignment system", "abstract": "the emergence of real-time 3d ultrasound (us) makes it possible to consider image-based tracking of subcutaneous soft tissue targets for computer guided diagnosis and therapy. we propose a 3d transrectal us based tracking system for precise prostate biopsy sample localisation. the aim is to improve sample distribution, to enable targeting of unsampled regions for repeated biopsies, and to make post-interventional quality controls possible. since the patient is not immobilized, since the prostate is mobile and due to the fact that probe movements are only constrained by the rectum during biopsy acquisition, the tracking system must be able to estimate rigid transformations that are beyond the capture range of common image similarity measures. we propose a fast and robust multi-resolution attribute-vector registration approach that combines global and local optimization methods to solve this problem. global optimization is performed on a probe movement model that reduces the dimensionality of the search space and thus renders optimization efficient. the method was tested on 237 prostate volumes acquired from 14 different patients for 3d to 3d and 3d to orthogonal 2d slices registration. the 3d-3d version of the algorithm converged correctly in 96.7% of all cases in 6.5s with an accuracy of 1.41mm (r.m.s.) and 3.84mm (max). the 3d to slices method yielded a success rate of 88.9% in 2.3s with an accuracy of 1.37mm (r.m.s.) and 4.3mm (max).", "date_create": "2007-12-05", "area": "cs.oh physics.med-ph", "authors": ["baumann", "mozer", "daanen", "troccaz"]}, {"idpaper": "0712.0811", "title": "the fast fibonacci decompression algorithm", "abstract": "data compression has been widely applied in many data processing areas. compression methods use variable-size codes with the shorter codes assigned to symbols or groups of symbols that appear in the data frequently. fibonacci coding, as a representative of these codes, is used for compressing small numbers. time consumption of a decompression algorithm is not usually as important as the time of a compression algorithm. however, efficiency of the decompression may be a critical issue in some cases. for example, a real-time compression of tree data structures follows this issue. tree's pages are decompressed during every reading from a secondary storage into the main memory. in this case, the efficiency of a decompression algorithm is extremely important. we have developed a fast fibonacci decompression for this purpose. our approach is up to $3.5\\times$ faster than the original implementation.", "date_create": "2007-12-05", "area": "cs.pf cs.oh", "authors": ["baca", "snasel", "platos", "kratky", "el-qawasmeh"]}, {"idpaper": "0712.1189", "title": "implementation, compilation, optimization of object-oriented languages,   programs and systems - report on the workshop icooolps'2007 at ecoop'07", "abstract": "icooolps'2007 was the second edition of the ecoop-icooolps workshop. icooolps intends to bring researchers and practitioners both from academia and industry together, with a spirit of openness, to try and identify and begin to address the numerous and very varied issues of optimization. after a first successful edition, this second one put a stronger emphasis on exchanges and discussions amongst the participants, progressing on the bases set last year in nantes. the workshop attendance was a success, since the 30-people limit we had set was reached about 2 weeks before the workshop itself. some of the discussions (e.g. annotations) were so successful that they would required even more time than we were able to dedicate to them. that's one area we plan to further improve for the next edition.", "date_create": "2007-12-07", "area": "cs.pl cs.se", "authors": ["zendra", "jul", "ducournau", "gagnon", "jones", "krintz", "mulet", "vitek"]}, {"idpaper": "0712.1869", "title": "two-connected graphs with prescribed three-connected components", "abstract": "we adapt the classical 3-decomposition of any 2-connected graph to the case of simple graphs (no loops or multiple edges). by analogy with the block-cutpoint tree of a connected graph, we deduce from this decomposition a bicolored tree tc(g) associated with any 2-connected graph g, whose white vertices are the 3-components of g (3-connected components or polygons) and whose black vertices are bonds linking together these 3-components, arising from separating pairs of vertices of g. two fundamental relationships on graphs and networks follow from this construction. the first one is a dissymmetry theorem which leads to the expression of the class b=b(f) of 2-connected graphs, all of whose 3-connected components belong to a given class f of 3-connected graphs, in terms of various rootings of b. the second one is a functional equation which characterizes the corresponding class r=r(f) of two-pole networks all of whose 3-connected components are in f. all the rootings of b are then expressed in terms of f and r. there follow corresponding identities for all the associated series, in particular the edge index series. numerous enumerative consequences are discussed.", "date_create": "2007-12-12", "area": "math.co cs.dm", "authors": ["gagarin", "labelle", "leroux", "walsh"]}, {"idpaper": "0712.1869", "title": "two-connected graphs with prescribed three-connected components", "abstract": "we adapt the classical 3-decomposition of any 2-connected graph to the case of simple graphs (no loops or multiple edges). by analogy with the block-cutpoint tree of a connected graph, we deduce from this decomposition a bicolored tree tc(g) associated with any 2-connected graph g, whose white vertices are the 3-components of g (3-connected components or polygons) and whose black vertices are bonds linking together these 3-components, arising from separating pairs of vertices of g. two fundamental relationships on graphs and networks follow from this construction. the first one is a dissymmetry theorem which leads to the expression of the class b=b(f) of 2-connected graphs, all of whose 3-connected components belong to a given class f of 3-connected graphs, in terms of various rootings of b. the second one is a functional equation which characterizes the corresponding class r=r(f) of two-pole networks all of whose 3-connected components are in f. all the rootings of b are then expressed in terms of f and r. there follow corresponding identities for all the associated series, in particular the edge index series. numerous enumerative consequences are discussed.", "date_create": "2007-12-12", "area": "math.co cs.dm", "authors": ["gagarin", "labelle", "leroux", "walsh"]}, {"idpaper": "0712.1869", "title": "two-connected graphs with prescribed three-connected components", "abstract": "we adapt the classical 3-decomposition of any 2-connected graph to the case of simple graphs (no loops or multiple edges). by analogy with the block-cutpoint tree of a connected graph, we deduce from this decomposition a bicolored tree tc(g) associated with any 2-connected graph g, whose white vertices are the 3-components of g (3-connected components or polygons) and whose black vertices are bonds linking together these 3-components, arising from separating pairs of vertices of g. two fundamental relationships on graphs and networks follow from this construction. the first one is a dissymmetry theorem which leads to the expression of the class b=b(f) of 2-connected graphs, all of whose 3-connected components belong to a given class f of 3-connected graphs, in terms of various rootings of b. the second one is a functional equation which characterizes the corresponding class r=r(f) of two-pole networks all of whose 3-connected components are in f. all the rootings of b are then expressed in terms of f and r. there follow corresponding identities for all the associated series, in particular the edge index series. numerous enumerative consequences are discussed.", "date_create": "2007-12-12", "area": "math.co cs.dm", "authors": ["gagarin", "labelle", "leroux", "walsh"]}, {"idpaper": "0712.1869", "title": "two-connected graphs with prescribed three-connected components", "abstract": "we adapt the classical 3-decomposition of any 2-connected graph to the case of simple graphs (no loops or multiple edges). by analogy with the block-cutpoint tree of a connected graph, we deduce from this decomposition a bicolored tree tc(g) associated with any 2-connected graph g, whose white vertices are the 3-components of g (3-connected components or polygons) and whose black vertices are bonds linking together these 3-components, arising from separating pairs of vertices of g. two fundamental relationships on graphs and networks follow from this construction. the first one is a dissymmetry theorem which leads to the expression of the class b=b(f) of 2-connected graphs, all of whose 3-connected components belong to a given class f of 3-connected graphs, in terms of various rootings of b. the second one is a functional equation which characterizes the corresponding class r=r(f) of two-pole networks all of whose 3-connected components are in f. all the rootings of b are then expressed in terms of f and r. there follow corresponding identities for all the associated series, in particular the edge index series. numerous enumerative consequences are discussed.", "date_create": "2007-12-12", "area": "math.co cs.dm", "authors": ["gagarin", "labelle", "leroux", "walsh"]}, {"idpaper": "0712.2094", "title": "hinged dissections exist", "abstract": "we prove that any finite collection of polygons of equal area has a common hinged dissection. that is, for any such collection of polygons there exists a chain of polygons hinged at vertices that can be folded in the plane continuously without self-intersection to form any polygon in the collection. this result settles the open problem about the existence of hinged dissections between pairs of polygons that goes back implicitly to 1864 and has been studied extensively in the past ten years. our result generalizes and indeed builds upon the result from 1814 that polygons have common dissections (without hinges). we also extend our common dissection result to edge-hinged dissections of solid 3d polyhedra that have a common (unhinged) dissection, as determined by dehn's 1900 solution to hilbert's third problem. our proofs are constructive, giving explicit algorithms in all cases. for a constant number of planar polygons, both the number of pieces and running time required by our construction are pseudopolynomial. this bound is the best possible, even for unhinged dissections. hinged dissections have possible applications to reconfigurable robotics, programmable matter, and nanomanufacturing.", "date_create": "2007-12-12", "area": "cs.cg", "authors": ["abbott", "abel", "charlton", "demaine", "demaine", "kominers"]}, {"idpaper": "0712.2094", "title": "hinged dissections exist", "abstract": "we prove that any finite collection of polygons of equal area has a common hinged dissection. that is, for any such collection of polygons there exists a chain of polygons hinged at vertices that can be folded in the plane continuously without self-intersection to form any polygon in the collection. this result settles the open problem about the existence of hinged dissections between pairs of polygons that goes back implicitly to 1864 and has been studied extensively in the past ten years. our result generalizes and indeed builds upon the result from 1814 that polygons have common dissections (without hinges). we also extend our common dissection result to edge-hinged dissections of solid 3d polyhedra that have a common (unhinged) dissection, as determined by dehn's 1900 solution to hilbert's third problem. our proofs are constructive, giving explicit algorithms in all cases. for a constant number of planar polygons, both the number of pieces and running time required by our construction are pseudopolynomial. this bound is the best possible, even for unhinged dissections. hinged dissections have possible applications to reconfigurable robotics, programmable matter, and nanomanufacturing.", "date_create": "2007-12-12", "area": "cs.cg", "authors": ["abbott", "abel", "charlton", "demaine", "demaine", "kominers"]}, {"idpaper": "0712.2099", "title": "mri/trus data fusion for brachytherapy", "abstract": "background: prostate brachytherapy consists in placing radioactive seeds for tumour destruction under transrectal ultrasound imaging (trus) control. it requires prostate delineation from the images for dose planning. because ultrasound imaging is patient- and operator-dependent, we have proposed to fuse mri data to trus data to make image processing more reliable. the technical accuracy of this approach has already been evaluated. methods: we present work in progress concerning the evaluation of the approach from the dosimetry viewpoint. the objective is to determine what impact this system may have on the treatment of the patient. dose planning is performed from initial trus prostate contours and evaluated on contours modified by data fusion. results: for the eight patients included, we demonstrate that trus prostate volume is most often underestimated and that dose is overestimated in a correlated way. however, dose constraints are still verified for those eight patients. conclusions: this confirms our initial hypothesis.", "date_create": "2007-12-13", "area": "cs.oh", "authors": ["daanen", "gastaldo", "giraud", "fourneret", "descotes", "bolla", "collomb", "troccaz"]}, {"idpaper": "0712.2099", "title": "mri/trus data fusion for brachytherapy", "abstract": "background: prostate brachytherapy consists in placing radioactive seeds for tumour destruction under transrectal ultrasound imaging (trus) control. it requires prostate delineation from the images for dose planning. because ultrasound imaging is patient- and operator-dependent, we have proposed to fuse mri data to trus data to make image processing more reliable. the technical accuracy of this approach has already been evaluated. methods: we present work in progress concerning the evaluation of the approach from the dosimetry viewpoint. the objective is to determine what impact this system may have on the treatment of the patient. dose planning is performed from initial trus prostate contours and evaluated on contours modified by data fusion. results: for the eight patients included, we demonstrate that trus prostate volume is most often underestimated and that dose is overestimated in a correlated way. however, dose constraints are still verified for those eight patients. conclusions: this confirms our initial hypothesis.", "date_create": "2007-12-13", "area": "cs.oh", "authors": ["daanen", "gastaldo", "giraud", "fourneret", "descotes", "bolla", "collomb", "troccaz"]}, {"idpaper": "0712.2099", "title": "mri/trus data fusion for brachytherapy", "abstract": "background: prostate brachytherapy consists in placing radioactive seeds for tumour destruction under transrectal ultrasound imaging (trus) control. it requires prostate delineation from the images for dose planning. because ultrasound imaging is patient- and operator-dependent, we have proposed to fuse mri data to trus data to make image processing more reliable. the technical accuracy of this approach has already been evaluated. methods: we present work in progress concerning the evaluation of the approach from the dosimetry viewpoint. the objective is to determine what impact this system may have on the treatment of the patient. dose planning is performed from initial trus prostate contours and evaluated on contours modified by data fusion. results: for the eight patients included, we demonstrate that trus prostate volume is most often underestimated and that dose is overestimated in a correlated way. however, dose constraints are still verified for those eight patients. conclusions: this confirms our initial hypothesis.", "date_create": "2007-12-13", "area": "cs.oh", "authors": ["daanen", "gastaldo", "giraud", "fourneret", "descotes", "bolla", "collomb", "troccaz"]}, {"idpaper": "0712.2099", "title": "mri/trus data fusion for brachytherapy", "abstract": "background: prostate brachytherapy consists in placing radioactive seeds for tumour destruction under transrectal ultrasound imaging (trus) control. it requires prostate delineation from the images for dose planning. because ultrasound imaging is patient- and operator-dependent, we have proposed to fuse mri data to trus data to make image processing more reliable. the technical accuracy of this approach has already been evaluated. methods: we present work in progress concerning the evaluation of the approach from the dosimetry viewpoint. the objective is to determine what impact this system may have on the treatment of the patient. dose planning is performed from initial trus prostate contours and evaluated on contours modified by data fusion. results: for the eight patients included, we demonstrate that trus prostate volume is most often underestimated and that dose is overestimated in a correlated way. however, dose constraints are still verified for those eight patients. conclusions: this confirms our initial hypothesis.", "date_create": "2007-12-13", "area": "cs.oh", "authors": ["daanen", "gastaldo", "giraud", "fourneret", "descotes", "bolla", "collomb", "troccaz"]}, {"idpaper": "0712.2100", "title": "medical image computing and computer-aided medical interventions applied   to soft tissues. work in progress in urology", "abstract": "until recently, computer-aided medical interventions (cami) and medical robotics have focused on rigid and non deformable anatomical structures. nowadays, special attention is paid to soft tissues, raising complex issues due to their mobility and deformation. mini-invasive digestive surgery was probably one of the first fields where soft tissues were handled through the development of simulators, tracking of anatomical structures and specific assistance robots. however, other clinical domains, for instance urology, are concerned. indeed, laparoscopic surgery, new tumour destruction techniques (e.g. hifu, radiofrequency, or cryoablation), increasingly early detection of cancer, and use of interventional and diagnostic imaging modalities, recently opened new challenges to the urologist and scientists involved in cami. this resulted in the last five years in a very significant increase of research and developments of computer-aided urology systems. in this paper, we propose a description of the main problems related to computer-aided diagnostic and therapy of soft tissues and give a survey of the different types of assistance offered to the urologist: robotization, image fusion, surgical navigation. both research projects and operational industrial systems are discussed.", "date_create": "2007-12-13", "area": "cs.oh cs.ro", "authors": ["troccaz", "baumann", "berkelman", "cinquin", "daanen", "leroy", "marchal", "payan", "promayon", "voros", "bart", "bolla", "chartier-kastler", "descotes", "dusserre", "giraud", "long", "moalic", "mozer"]}, {"idpaper": "0712.2100", "title": "medical image computing and computer-aided medical interventions applied   to soft tissues. work in progress in urology", "abstract": "until recently, computer-aided medical interventions (cami) and medical robotics have focused on rigid and non deformable anatomical structures. nowadays, special attention is paid to soft tissues, raising complex issues due to their mobility and deformation. mini-invasive digestive surgery was probably one of the first fields where soft tissues were handled through the development of simulators, tracking of anatomical structures and specific assistance robots. however, other clinical domains, for instance urology, are concerned. indeed, laparoscopic surgery, new tumour destruction techniques (e.g. hifu, radiofrequency, or cryoablation), increasingly early detection of cancer, and use of interventional and diagnostic imaging modalities, recently opened new challenges to the urologist and scientists involved in cami. this resulted in the last five years in a very significant increase of research and developments of computer-aided urology systems. in this paper, we propose a description of the main problems related to computer-aided diagnostic and therapy of soft tissues and give a survey of the different types of assistance offered to the urologist: robotization, image fusion, surgical navigation. both research projects and operational industrial systems are discussed.", "date_create": "2007-12-13", "area": "cs.oh cs.ro", "authors": ["troccaz", "baumann", "berkelman", "cinquin", "daanen", "leroy", "marchal", "payan", "promayon", "voros", "bart", "bolla", "chartier-kastler", "descotes", "dusserre", "giraud", "long", "moalic", "mozer"]}, {"idpaper": "0712.2100", "title": "medical image computing and computer-aided medical interventions applied   to soft tissues. work in progress in urology", "abstract": "until recently, computer-aided medical interventions (cami) and medical robotics have focused on rigid and non deformable anatomical structures. nowadays, special attention is paid to soft tissues, raising complex issues due to their mobility and deformation. mini-invasive digestive surgery was probably one of the first fields where soft tissues were handled through the development of simulators, tracking of anatomical structures and specific assistance robots. however, other clinical domains, for instance urology, are concerned. indeed, laparoscopic surgery, new tumour destruction techniques (e.g. hifu, radiofrequency, or cryoablation), increasingly early detection of cancer, and use of interventional and diagnostic imaging modalities, recently opened new challenges to the urologist and scientists involved in cami. this resulted in the last five years in a very significant increase of research and developments of computer-aided urology systems. in this paper, we propose a description of the main problems related to computer-aided diagnostic and therapy of soft tissues and give a survey of the different types of assistance offered to the urologist: robotization, image fusion, surgical navigation. both research projects and operational industrial systems are discussed.", "date_create": "2007-12-13", "area": "cs.oh cs.ro", "authors": ["troccaz", "baumann", "berkelman", "cinquin", "daanen", "leroy", "marchal", "payan", "promayon", "voros", "bart", "bolla", "chartier-kastler", "descotes", "dusserre", "giraud", "long", "moalic", "mozer"]}, {"idpaper": "0712.2262", "title": "the earth system grid: supporting the next generation of climate   modeling research", "abstract": "understanding the earth's climate system and how it might be changing is a preeminent scientific challenge. global climate models are used to simulate past, present, and future climates, and experiments are executed continuously on an array of distributed supercomputers. the resulting data archive, spread over several sites, currently contains upwards of 100 tb of simulation data and is growing rapidly. looking toward mid-decade and beyond, we must anticipate and prepare for distributed climate research data holdings of many petabytes. the earth system grid (esg) is a collaborative interdisciplinary project aimed at addressing the challenge of enabling management, discovery, access, and analysis of these critically important datasets in a distributed and heterogeneous computational environment. the problem is fundamentally a grid problem. building upon the globus toolkit and a variety of other technologies, esg is developing an environment that addresses authentication, authorization for data access, large-scale data transport and management, services and abstractions for high-performance remote data access, mechanisms for scalable data replication, cataloging with rich semantic and syntactic information, data discovery, distributed monitoring, and web-based portals for using the system.", "date_create": "2007-12-13", "area": "cs.ce cs.dc cs.ni", "authors": ["bernholdt", "bharathi", "brown", "chanchio", "chen", "chervenak", "cinquini", "drach", "foster", "fox", "garcia", "kesselman", "markel", "middleton", "nefedova", "pouchard", "shoshani", "sim", "strand", "williams"]}, {"idpaper": "0712.2302", "title": "data access optimizations for highly threaded multi-core cpus with   multiple memory controllers", "abstract": "processor and system architectures that feature multiple memory controllers are prone to show bottlenecks and erratic performance numbers on codes with regular access patterns. although such effects are well known in the form of cache thrashing and aliasing conflicts, they become more severe when memory access is involved. using the new sun ultrasparc t2 processor as a prototypical multi-core design, we analyze performance patterns in low-level and application benchmarks and show ways to circumvent bottlenecks by careful data layout and padding.", "date_create": "2007-12-14", "area": "cs.dc cs.pf", "authors": ["hager", "zeiser", "wellein"]}, {"idpaper": "0712.2661", "title": "algorithms for generating convex sets in acyclic digraphs", "abstract": "a set $x$ of vertices of an acyclic digraph $d$ is convex if $x\\neq \\emptyset$ and there is no directed path between vertices of $x$ which contains a vertex not in $x$. a set $x$ is connected if $x\\neq \\emptyset$ and the underlying undirected graph of the subgraph of $d$ induced by $x$ is connected. connected convex sets and convex sets of acyclic digraphs are of interest in the area of modern embedded processor technology. we construct an algorithm $\\cal a$ for enumeration of all connected convex sets of an acyclic digraph $d$ of order $n$. the time complexity of $\\cal a$ is $o(n\\cdot cc(d))$, where $cc(d)$ is the number of connected convex sets in $d$. we also give an optimal algorithm for enumeration of all (not just connected) convex sets of an acyclic digraph $d$ of order $n$. in computational experiments we demonstrate that our algorithms outperform the best algorithms in the literature.   using the same approach as for $\\cal a$, we design an algorithm for generating all connected sets of a connected undirected graph $g$. the complexity of the algorithm is $o(n\\cdot c(g)),$ where $n$ is the order of $g$ and $c(g)$ is the number of connected sets of $g.$ the previously reported algorithm for connected set enumeration is of running time $o(mn\\cdot c(g))$, where $m$ is the number of edges in $g.$", "date_create": "2007-12-17", "area": "cs.dm cs.ds", "authors": ["balister", "gerke", "gutin", "johnstone", "reddington", "scott", "soleimanfallah", "yeo"]}, {"idpaper": "0712.3084", "title": "proxy signature scheme with effective revocation using bilinear pairings", "abstract": "we present a proxy signature scheme using bilinear pairings that provides effective proxy revocation. the scheme uses a binding-blinding technique to avoid secure channel requirements in the key issuance stage. with this technique, the signer receives a partial private key from a trusted authority and unblinds it to get his private key, in turn, overcomes the key escrow problem which is a constraint in most of the pairing-based proxy signature schemes. the scheme fulfills the necessary security requirements of proxy signature and resists other possible threats.", "date_create": "2007-12-18", "area": "cs.cr", "authors": ["das", "saxena", "phatak"]}, {"idpaper": "0712.3298", "title": "clairlib documentation v1.03", "abstract": "the clair library is intended to simplify a number of generic tasks in natural language processing (nlp), information retrieval (ir), and network analysis. its architecture also allows for external software to be plugged in with very little effort. functionality native to clairlib includes tokenization, summarization, lexrank, biased lexrank, document clustering, document indexing, pagerank, biased pagerank, web graph analysis, network generation, power law distribution analysis, network analysis (clustering coefficient, degree distribution plotting, average shortest path, diameter, triangles, shortest path matrices, connected components), cosine similarity, random walks on graphs, statistics (distributions, tests), tf, idf, community finding.", "date_create": "2007-12-19", "area": "cs.ir cs.cl", "authors": ["radev", "hodges", "fader", "joseph", "gerrish", "schaller", "deperi", "gibson"]}, {"idpaper": "0712.3568", "title": "a partition-based relaxation for steiner trees", "abstract": "the steiner tree problem is a classical np-hard optimization problem with a wide range of practical applications. in an instance of this problem, we are given an undirected graph g=(v,e), a set of terminals r, and non-negative costs c_e for all edges e in e. any tree that contains all terminals is called a steiner tree; the goal is to find a minimum-cost steiner tree. the nodes v r are called steiner nodes.   the best approximation algorithm known for the steiner tree problem is due to robins and zelikovsky (siam j. discrete math, 2005); their greedy algorithm achieves a performance guarantee of 1+(ln 3)/2 ~ 1.55. the best known linear (lp)-based algorithm, on the other hand, is due to goemans and bertsimas (math. programming, 1993) and achieves an approximation ratio of 2-2/|r|. in this paper we establish a link between greedy and lp-based approaches by showing that robins and zelikovsky's algorithm has a natural primal-dual interpretation with respect to a novel partition-based linear programming relaxation. we also exhibit surprising connections between the new formulation and existing lps and we show that the new lp is stronger than the bidirected cut formulation.   an instance is b-quasi-bipartite if each connected component of g r has at most b vertices. we show that robins' and zelikovsky's algorithm has an approximation ratio better than 1+(ln 3)/2 for such instances, and we prove that the integrality gap of our lp is between 8/7 and (2b+1)/(b+1).", "date_create": "2007-12-20", "area": "cs.ds", "authors": ["konemann", "pritchard", "tan"]}, {"idpaper": "0712.3568", "title": "a partition-based relaxation for steiner trees", "abstract": "the steiner tree problem is a classical np-hard optimization problem with a wide range of practical applications. in an instance of this problem, we are given an undirected graph g=(v,e), a set of terminals r, and non-negative costs c_e for all edges e in e. any tree that contains all terminals is called a steiner tree; the goal is to find a minimum-cost steiner tree. the nodes v r are called steiner nodes.   the best approximation algorithm known for the steiner tree problem is due to robins and zelikovsky (siam j. discrete math, 2005); their greedy algorithm achieves a performance guarantee of 1+(ln 3)/2 ~ 1.55. the best known linear (lp)-based algorithm, on the other hand, is due to goemans and bertsimas (math. programming, 1993) and achieves an approximation ratio of 2-2/|r|. in this paper we establish a link between greedy and lp-based approaches by showing that robins and zelikovsky's algorithm has a natural primal-dual interpretation with respect to a novel partition-based linear programming relaxation. we also exhibit surprising connections between the new formulation and existing lps and we show that the new lp is stronger than the bidirected cut formulation.   an instance is b-quasi-bipartite if each connected component of g r has at most b vertices. we show that robins' and zelikovsky's algorithm has an approximation ratio better than 1+(ln 3)/2 for such instances, and we prove that the integrality gap of our lp is between 8/7 and (2b+1)/(b+1).", "date_create": "2007-12-20", "area": "cs.ds", "authors": ["konemann", "pritchard", "tan"]}, {"idpaper": "0712.3830", "title": "tchr: a framework for tabled clp", "abstract": "tabled constraint logic programming is a powerful execution mechanism for dealing with constraint logic programming without worrying about fixpoint computation. various applications, e.g in the fields of program analysis and model checking, have been proposed. unfortunately, a high-level system for developing new applications is lacking, and programmers are forced to resort to complicated ad hoc solutions.   this papers presents tchr, a high-level framework for tabled constraint logic programming. it integrates in a light-weight manner constraint handling rules (chr), a high-level language for constraint solvers, with tabled logic programming. the framework is easily instantiated with new application-specific constraint domains. various high-level operations can be instantiated to control performance. in particular, we propose a novel, generalized technique for compacting answer sets.", "date_create": "2007-12-26", "area": "cs.pl", "authors": ["schrijvers", "demoen", "warren"]}, {"idpaper": "0712.3980", "title": "distributed slicing in dynamic systems", "abstract": "peer to peer (p2p) systems are moving from application specific architectures to a generic service oriented design philosophy. this raises interesting problems in connection with providing useful p2p middleware services capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. the slicing service, has been proposed to allow for an automatic partitioning of p2p networks into groups (slices) that represent a controllable amount of some resource and that are also relatively homogeneous with respect to that resource. in this paper we propose two gossip-based algorithms to solve the distributed slicing problem. the first algorithm speeds up an existing algorithm sorting a set of uniform random numbers. the second algorithm statistically approximates the rank of nodes in the ordering. the scalability, efficiency and resilience to dynamics of both algorithms rely on their gossip-based models. these algorithms are proved viable theoretically and experimentally.", "date_create": "2007-12-26", "area": "cs.dc", "authors": ["fernandez", "gramoli", "jimenez", "kermarrec", "raynal"]}, {"idpaper": "0801.0931", "title": "the asymptotic bit error probability of ldpc codes for the binary   erasure channel with finite iteration number", "abstract": "we consider communication over the binary erasure channel (bec) using low-density parity-check (ldpc) code and belief propagation (bp) decoding. the bit error probability for infinite block length is known by density evolution and it is well known that a difference between the bit error probability at finite iteration number for finite block length $n$ and for infinite block length is asymptotically $\\alpha/n$, where $\\alpha$ is a specific constant depending on the degree distribution, the iteration number and the erasure probability. our main result is to derive an efficient algorithm for calculating $\\alpha$ for regular ensembles. the approximation using $\\alpha$ is accurate for $(2,r)$-regular ensembles even in small block length.", "date_create": "2008-01-07", "area": "cs.it math.it", "authors": ["mori", "kasai", "shibuya", "sakaniwa"]}, {"idpaper": "0801.1210", "title": "increasing gp computing power via volunteer computing", "abstract": "this paper describes how it is possible to increase gp computing power via volunteer computing (vc) using the boinc framework. two experiments using well-known gp tools -lil-gp & ecj- are performed in order to demonstrate the benefit of using vc in terms of computing power and speed up. finally we present an extension of the model where any gp tool or framework can be used inside boinc regardless of its programming language, complexity or required operating system.", "date_create": "2008-01-08", "area": "cs.dc", "authors": ["gonzalez", "de vega", "trujillo", "olague", "de la o", "cardenas", "araujo", "castillo", "sharman"]}, {"idpaper": "0801.1364", "title": "an algorithm to compute the nearest point in the lattice $a_{n}^*$", "abstract": "the lattice $a_n^*$ is an important lattice because of its covering properties in low dimensions. clarkson \\cite{clarkson1999:anstar} described an algorithm to compute the nearest lattice point in $a_n^*$ that requires $o(n\\log{n})$ arithmetic operations. in this paper, we describe a new algorithm. while the complexity is still $o(n\\log{n})$, it is significantly simpler to describe and verify. in practice, we find that the new algorithm also runs faster.", "date_create": "2008-01-09", "area": "cs.it math.it", "authors": ["mckilliam", "clarkson", "quinn"]}, {"idpaper": "0801.1630", "title": "computational solutions for today's navy", "abstract": "new methods are being employed to meet the navy's changing software-development environment.", "date_create": "2008-01-10", "area": "cs.ma cs.gl", "authors": ["bentrem", "sample", "harris"]}, {"idpaper": "0801.1630", "title": "computational solutions for today's navy", "abstract": "new methods are being employed to meet the navy's changing software-development environment.", "date_create": "2008-01-10", "area": "cs.ma cs.gl", "authors": ["bentrem", "sample", "harris"]}, {"idpaper": "0801.1766", "title": "a family of counter examples to an approach to graph isomorphism", "abstract": "we give a family of counter examples showing that the two sequences of polytopes $\\phi_{n,n}$ and $\\psi_{n,n}$ are different. these polytopes were defined recently by s. friedland in an attempt at a polynomial time algorithm for graph isomorphism.", "date_create": "2008-01-11", "area": "cs.cc cs.dm", "authors": ["cai", "lu", "xia"]}, {"idpaper": "0801.4129", "title": "scaling laws and techniques in decentralized processing of interfered   gaussian channels", "abstract": "the scaling laws of the achievable communication rates and the corresponding upper bounds of distributed reception in the presence of an interfering signal are investigated. the scheme includes one transmitter communicating to a remote destination via two relays, which forward messages to the remote destination through reliable links with finite capacities. the relays receive the transmission along with some unknown interference. we focus on three common settings for distributed reception, wherein the scaling laws of the capacity (the pre-log as the power of the transmitter and the interference are taken to infinity) are completely characterized. it is shown in most cases that in order to overcome the interference, a definite amount of information about the interference needs to be forwarded along with the desired message, to the destination. it is exemplified in one scenario that the cut-set upper bound is strictly loose. the results are derived using the cut-set along with a new bounding technique, which relies on multi letter expressions. furthermore, lattices are found to be a useful communication technique in this setting, and are used to characterize the scaling laws of achievable rates.", "date_create": "2008-01-27", "area": "cs.it math.it", "authors": ["sanderovich", "peleg", "shamai"]}, {"idpaper": "0801.4305", "title": "risk-seeking versus risk-avoiding investments in noisy periodic   environments", "abstract": "we study the performance of various agent strategies in an artificial investment scenario. agents are equipped with a budget, $x(t)$, and at each time step invest a particular fraction, $q(t)$, of their budget. the return on investment (roi), $r(t)$, is characterized by a periodic function with different types and levels of noise. risk-avoiding agents choose their fraction $q(t)$ proportional to the expected positive roi, while risk-seeking agents always choose a maximum value $q_{max}$ if they predict the roi to be positive (\"everything on red\"). in addition to these different strategies, agents have different capabilities to predict the future $r(t)$, dependent on their internal complexity. here, we compare 'zero-intelligent' agents using technical analysis (such as moving least squares) with agents using reinforcement learning or genetic algorithms to predict $r(t)$. the performance of agents is measured by their average budget growth after a certain number of time steps. we present results of extensive computer simulations, which show that, for our given artificial environment, (i) the risk-seeking strategy outperforms the risk-avoiding one, and (ii) the genetic algorithm was able to find this optimal strategy itself, and thus outperforms other prediction approaches considered.", "date_create": "2008-01-28", "area": "q-fin.pm cs.ce physics.soc-ph", "authors": ["barrientos", "walter", "schweitzer"]}, {"idpaper": "0801.4355", "title": "ter: a robot for remote ultrasonic examination: experimental evaluations", "abstract": "this chapter:   o motivates the clinical use of robotic tele-echography   o introduces the ter system   o describes technical and clinical evaluations performed with ter", "date_create": "2008-01-28", "area": "cs.oh cs.ro", "authors": ["banihachemi", "boidard", "bosson", "bressollette", "bricault", "cinquin", "ferretti", "marchal", "martinelli", "moreau-gaudry", "pelissier", "roux", "saragaglia", "thorel", "troccaz", "vilchis"]}, {"idpaper": "0801.4405", "title": "a locked orthogonal tree", "abstract": "we give a counterexample to a conjecture of poon [poo06] that any orthogonal tree in two dimensions can always be flattened by a continuous motion that preserves edge lengths and avoids self-intersection. we show our example is locked by extending results on strongly locked self-touching linkages due to connelly, demaine and rote [cdr02] to allow zero-length edges as defined in [adg07], which may be of independent interest. our results also yield a locked tree with only eleven edges, which is the smallest known example of a locked tree.", "date_create": "2008-01-28", "area": "cs.cg", "authors": ["charlton", "demaine", "demaine", "price", "tu"]}, {"idpaper": "0802.1026", "title": "an empirical study of cache-oblivious priority queues and their   application to the shortest path problem", "abstract": "in recent years the cache-oblivious model of external memory computation has provided an attractive theoretical basis for the analysis of algorithms on massive datasets. much progress has been made in discovering algorithms that are asymptotically optimal or near optimal. however, to date there are still relatively few successful experimental studies. in this paper we compare two different cache-oblivious priority queues based on the funnel and bucket heap and apply them to the single source shortest path problem on graphs with positive edge weights. our results show that when ram is limited and data is swapping to external storage, the cache-oblivious priority queues achieve orders of magnitude speedups over standard internal memory techniques. however, for the single source shortest path problem both on simulated and real world graph data, these speedups are markedly lower due to the time required to access the graph adjacency list itself.", "date_create": "2008-02-07", "area": "cs.ds cs.se", "authors": ["sach", "clifford"]}, {"idpaper": "0802.1555", "title": "constructing linear codes with good joint spectra", "abstract": "the problem of finding good linear codes for joint source-channel coding (jscc) is investigated in this paper. by the code-spectrum approach, it has been proved in the authors' previous paper that a good linear code for the authors' jscc scheme is a code with a good joint spectrum, so the main task in this paper is to construct linear codes with good joint spectra. first, the code-spectrum approach is developed further to facilitate the calculation of spectra. second, some general principles for constructing good linear codes are presented. finally, we propose an explicit construction of linear codes with good joint spectra based on low density parity check (ldpc) codes and low density generator matrix (ldgm) codes.", "date_create": "2008-02-11", "area": "cs.it math.it", "authors": ["yang", "chen", "honold", "zhang", "qiu"]}, {"idpaper": "0802.2201", "title": "reconstruction of eye movements during blinks", "abstract": "in eye movement research in reading, the amount of data plays a crucial role for the validation of results. a methodological problem for the analysis of the eye movement in reading are blinks, when readers close their eyes. blinking rate increases with increasing reading time, resulting in high data losses, especially for older adults or reading impaired subjects. we present a method, based on the symbolic sequence dynamics of the eye movements, that reconstructs the horizontal position of the eyes while the reader blinks. the method makes use of an observed fact that the movements of the eyes before closing or after opening contain information about the eyes movements during blinks. test results indicate that our reconstruction method is superior to methods that use simpler interpolation approaches. in addition, analyses of the reconstructed data show no significant deviation from the usual behavior observed in readers.", "date_create": "2008-02-15", "area": "cs.sc", "authors": ["baptista", "bohn", "kliegl", "engbert", "kurths"]}, {"idpaper": "0802.2428", "title": "sign language tutoring tool", "abstract": "in this project, we have developed a sign language tutor that lets users learn isolated signs by watching recorded videos and by trying the same signs. the system records the user's video and analyses it. if the sign is recognized, both verbal and animated feedback is given to the user. the system is able to recognize complex signs that involve both hand gestures and head movements and expressions. our performance tests yield a 99% recognition rate on signs involving only manual gestures and 85% recognition rate on signs that involve both manual and non manual components, such as head movement and facial expressions.", "date_create": "2008-02-18", "area": "cs.lg cs.hc", "authors": ["aran", "ari", "benoit", "carrillo", "fanard", "campr", "akarun", "caplier", "rombaut", "sankur"]}, {"idpaper": "0802.2836", "title": "minimizing flow time in the wireless gathering problem", "abstract": "we address the problem of efficient data gathering in a wireless network through multi-hop communication. we focus on the objective of minimizing the maximum flow time of a data packet. we prove that no polynomial time algorithm for this problem can have approximation ratio less than $\\omega(m^{1/3)$ when $m$ packets have to be transmitted, unless $p = np$. we then use resource augmentation to assess the performance of a fifo-like strategy. we prove that this strategy is 5-speed optimal, i.e., its cost remains within the optimal cost if we allow the algorithm to transmit data at a speed 5 times higher than that of the optimal solution we compare to.", "date_create": "2008-02-20", "area": "cs.ds cs.ni", "authors": ["bonifaci", "korteweg", "marchetti-spaccamela", "stougie"]}, {"idpaper": "0802.2836", "title": "minimizing flow time in the wireless gathering problem", "abstract": "we address the problem of efficient data gathering in a wireless network through multi-hop communication. we focus on the objective of minimizing the maximum flow time of a data packet. we prove that no polynomial time algorithm for this problem can have approximation ratio less than $\\omega(m^{1/3)$ when $m$ packets have to be transmitted, unless $p = np$. we then use resource augmentation to assess the performance of a fifo-like strategy. we prove that this strategy is 5-speed optimal, i.e., its cost remains within the optimal cost if we allow the algorithm to transmit data at a speed 5 times higher than that of the optimal solution we compare to.", "date_create": "2008-02-20", "area": "cs.ds cs.ni", "authors": ["bonifaci", "korteweg", "marchetti-spaccamela", "stougie"]}, {"idpaper": "0802.2843", "title": "sublinear communication protocols for multi-party pointer jumping and a   related lower bound", "abstract": "we study the one-way number-on-the-forehead (nof) communication complexity of the $k$-layer pointer jumping problem with $n$ vertices per layer. this classic problem, which has connections to many aspects of complexity theory, has seen a recent burst of research activity, seemingly preparing the ground for an $\\omega(n)$ lower bound, for constant $k$. our first result is a surprising sublinear -- i.e., $o(n)$ -- upper bound for the problem that holds for $k \\ge 3$, dashing hopes for such a lower bound. a closer look at the protocol achieving the upper bound shows that all but one of the players involved are collapsing, i.e., their messages depend only on the composition of the layers ahead of them. we consider protocols for the pointer jumping problem where all players are collapsing. our second result shows that a strong $n - o(\\log n)$ lower bound does hold in this case. our third result is another upper bound showing that nontrivial protocols for (a non-boolean version of) pointer jumping are possible even when all players are collapsing. our lower bound result uses a novel proof technique, different from those of earlier lower bounds that had an information-theoretic flavor. we hope this is useful in further study of the problem.", "date_create": "2008-02-20", "area": "cs.cc cs.ds", "authors": ["brody", "chakrabarti"]}, {"idpaper": "0802.2867", "title": "fixed parameter polynomial time algorithms for maximum agreement and   compatible supertrees", "abstract": "consider a set of labels $l$ and a set of trees ${\\mathcal t} = \\{{\\mathcal t}^{(1), {\\mathcal t}^{(2), ..., {\\mathcal t}^{(k) \\$ where each tree ${\\mathcal t}^{(i)$ is distinctly leaf-labeled by some subset of $l$. one fundamental problem is to find the biggest tree (denoted as supertree) to represent $\\mathcal t}$ which minimizes the disagreements with the trees in ${\\mathcal t}$ under certain criteria. this problem finds applications in phylogenetics, database, and data mining. in this paper, we focus on two particular supertree problems, namely, the maximum agreement supertree problem (masp) and the maximum compatible supertree problem (mcsp). these two problems are known to be np-hard for $k \\geq 3$. this paper gives the first polynomial time algorithms for both masp and mcsp when both $k$ and the maximum degree $d$ of the trees are constant.", "date_create": "2008-02-20", "area": "cs.ds", "authors": ["hoang", "sung"]}, {"idpaper": "0802.3047", "title": "step-up converter for electromagnetic vibrational energy scavenger", "abstract": "this paper introduces a voltage multiplier (vm) circuit which can step up a minimum voltage of 150 mv (peak). the operation and characteristics of this converter circuit are described. the voltage multiplier circuit is also tested with micro and macro scale electromagnetic vibrational generators and the effect of the vm on the optimum load conditions of the electromagnetic generator is presented. the measured results show that 85% efficiency can be achieved from this vm circuit at a power level of 18 ?w.", "date_create": "2008-02-21", "area": "cs.oh", "authors": ["saha", "o'donnell", "godsell", "carlioz", "wang", "mccloskey", "beeby", "tudor", "torah"]}, {"idpaper": "0802.3055", "title": "parameter identification of pressure sensors by static and dynamic   measurements", "abstract": "fast identification methods of pressure sensors are investigated. with regard to a complete accurate sensor parameter identification two different measurement methods are combined. the approach consists on one hand in performing static measurements - an applied pressure results in a membrane deformation measured interferometrically and the corresponding output voltage. on the other hand optical measurements of the modal responses of the sensor membranes are performed. this information is used in an inverse identification algorithm to identify geometrical and material parameters based on a fe model. the number of parameters to be identified is thereby generally limited only by the number of measurable modal frequencies. a quantitative evaluation of the identification results permits furthermore the classification of processing errors like etching errors. algorithms and identification results for membrane thickness, intrinsic stress and output voltage will be discussed in this contribution on the basis of the parameter identification of relative pressure sensors.", "date_create": "2008-02-21", "area": "cs.oh", "authors": ["michael", "kurth", "klattenhoff", "geissler", "hering"]}, {"idpaper": "0802.3075", "title": "solving functional reliability issue for an optical electrostatic switch", "abstract": "in this paper, we report the advantage of using ac actuating signal for driving mems actuators instead of dc voltages. the study is based upon micro mirror devices used in digital mode for optical switching operation. when the pull-in effect is used, charge injection occurs when the micro mirror is maintained in the deflected position. to avoid this effect, a geometrical solution is to realize grounded landing electrodes which are electro-statically separated from the control electrodes. another solution is the use of ac signal which eliminates charge injection particularly if a bipolar signal is used. long term experiments have demonstrated the reliability of such a signal command to avoid injection of electric charges.", "date_create": "2008-02-21", "area": "cs.oh", "authors": ["camon", "ganibal", "rapahoz", "trzmiel", "pisella", "martinez", "gilbert", "valette"]}, {"idpaper": "0802.3081", "title": "a high-q microwave mems resonator", "abstract": "a high-q microwave (k band) mems resonator is presented, which empolys substrate integrated waveguide (siw) and micromachined via-hole arrays by icp process. nonradiation dielectric waveguide (nrd) is formed by metal filled via-hole arrays and grounded planes. the three dimensional (3d) high resistivity silicon substrate filled cavity resonator is fed by current probes using cpw line. this monolithic resonator results in low cost, high performance and easy integration with planar cicuits. the measured quality factor is beyond 180 and the resonance frequency is 21ghz.it shows a good agreement with the simulation results. the chip size is only 4.7mm x 4.6mm x 0.5mm. finally, as an example of applications, a filter using two siw resonators is designed.", "date_create": "2008-02-21", "area": "cs.oh", "authors": ["jian", "yuanwei", "yong", "chen", "shixing"]}, {"idpaper": "0802.3086", "title": "selection of high strength encapsulant for mems devices undergoing high   pressure packaging", "abstract": "deflection behavior of several encapsulant materials under uniform pressure was studied to determine the best encapsulant for mems device. encapsulation is needed to protect movable parts of mems devices during high pressure transfer molded packaging process. the selected encapsulant material has to have surface deflection of less than 5 ?m under 100 atm vertical loading. deflection was simulated using coventorware ver.2005 software and verified with calculation results obtained using shell bending theory. screening design was used to construct a systematic approach for selecting the best encapsulant material and thickness under uniform pressure up to 100 atm. materials considered for this study were polyimide, parylene c and carbon based epoxy resin. it was observed that carbon based epoxy resin has deflection of less than 5 ?m for all thickness and pressure variations. parylene c is acceptable and polyimide is unsuitable as high strength encapsulant. carbon based epoxy resin is considered the best encapsulation material for mems under high pressure packaging process due to its high strength.", "date_create": "2008-02-21", "area": "cs.oh", "authors": ["hamzah", "husaini", "husaini", "majlis", "ahmad"]}, {"idpaper": "0802.3253", "title": "on the capacity and design of limited feedback multiuser mimo uplinks", "abstract": "the theory of multiple-input multiple-output (mimo) technology has been well-developed to increase fading channel capacity over single-input single-output (siso) systems. this capacity gain can often be leveraged by utilizing channel state information at the transmitter and the receiver. users make use of this channel state information for transmit signal adaptation. in this correspondence, we derive the capacity region for the mimo multiple access channel (mimo mac) when partial channel state information is available at the transmitters, where we assume a synchronous mimo multiuser uplink. the partial channel state information feedback has a cardinality constraint and is fed back from the basestation to the users using a limited rate feedback channel. using this feedback information, we propose a finite codebook design method to maximize sum-rate. in this correspondence, the codebook is a set of transmit signal covariance matrices. we also derive the capacity region and codebook design methods in the case that the covariance matrix is rank-one (i.e., beamforming). this is motivated by the fact that beamforming is optimal in certain conditions. the simulation results show that when the number of feedback bits increases, the capacity also increases. even with a small number of feedback bits, the performance of the proposed system is close to an optimal solution with the full feedback.", "date_create": "2008-02-21", "area": "cs.it cs.mm math.it", "authors": ["kim", "love"]}, {"idpaper": "0803.0194", "title": "acquisition accuracy evaluation in visual inspection systems - a   practical approach", "abstract": "this paper draws a proposal of a set of parameters and methods for accuracy evaluation of visual inspection systems. the case of a monochrome board is treated, but practically all conclusions and methods may be extended for colour acquisition. basically, the proposed parameters are grouped in five sets as follows:internal noise;video adc cuantisation parameters;analogue processing section parameters;dominant frequencies;synchronisation (lock-in) accuracy. on basis of this set of parameters was developed a software environment, in conjunction with a test signal generator that allows the \"test\" images. the paper also presents conclusions of evaluation for two types of video acquisition boards", "date_create": "2008-03-03", "area": "cs.cv cs.mm", "authors": ["arsinte", "miron"]}, {"idpaper": "0803.0316", "title": "staged self-assembly:nanomanufacture of arbitrary shapes with o(1) glues", "abstract": "we introduce staged self-assembly of wang tiles, where tiles can be added dynamically in sequence and where intermediate constructions can be stored for later mixing. this model and its various constraints and performance measures are motivated by a practical nanofabrication scenario through protein-based bioengineering. staging allows us to break through the traditional lower bounds in tile self-assembly by encoding the shape in the staging algorithm instead of the tiles. all of our results are based on the practical assumption that only a constant number of glues, and thus only a constant number of tiles, can be engineered, as each new glue type requires significant biochemical research and experiments. under this assumption, traditional tile self-assembly cannot even manufacture an n*n square; in contrast, we show how staged assembly enables manufacture of arbitrary orthogonal shapes in a variety of precise formulations of the model.", "date_create": "2008-03-03", "area": "cs.cg", "authors": ["demaine", "demaine", "fekete", "ishaque", "rafalin", "schweller", "souvaine"]}, {"idpaper": "0803.0316", "title": "staged self-assembly:nanomanufacture of arbitrary shapes with o(1) glues", "abstract": "we introduce staged self-assembly of wang tiles, where tiles can be added dynamically in sequence and where intermediate constructions can be stored for later mixing. this model and its various constraints and performance measures are motivated by a practical nanofabrication scenario through protein-based bioengineering. staging allows us to break through the traditional lower bounds in tile self-assembly by encoding the shape in the staging algorithm instead of the tiles. all of our results are based on the practical assumption that only a constant number of glues, and thus only a constant number of tiles, can be engineered, as each new glue type requires significant biochemical research and experiments. under this assumption, traditional tile self-assembly cannot even manufacture an n*n square; in contrast, we show how staged assembly enables manufacture of arbitrary orthogonal shapes in a variety of precise formulations of the model.", "date_create": "2008-03-03", "area": "cs.cg", "authors": ["demaine", "demaine", "fekete", "ishaque", "rafalin", "schweller", "souvaine"]}, {"idpaper": "0803.0473", "title": "stream sampling for variance-optimal estimation of subset sums", "abstract": "from a high volume stream of weighted items, we want to maintain a generic sample of a certain limited size $k$ that we can later use to estimate the total weight of arbitrary subsets. this is the classic context of on-line reservoir sampling, thinking of the generic sample as a reservoir. we present an efficient reservoir sampling scheme, $\\varoptk$, that dominates all previous schemes in terms of estimation quality.   $\\varoptk$ provides {\\em variance optimal unbiased estimation of subset sums}. more precisely, if we have seen $n$ items of the stream, then for {\\em any} subset size $m$, our scheme based on $k$ samples minimizes the average variance over all subsets of size $m$. in fact, the optimality is against any off-line scheme with $k$ samples tailored for the concrete set of items seen. in addition to optimal average variance, our scheme provides tighter worst-case bounds on the variance of {\\em particular} subsets than previously possible. it is efficient, handling each new item of the stream in $o(\\log k)$ time. finally, it is particularly well suited for combination of samples from different streams in a distributed setting.", "date_create": "2008-03-04", "area": "cs.ds", "authors": ["cohen", "duffield", "kaplan", "lund", "thorup"]}, {"idpaper": "0803.1220", "title": "22-step collisions for sha-2", "abstract": "in this note, we provide the first 22-step collisions for sha-256 and sha-512. detailed technique of generating these collisions will be provided in the next revision of this note.", "date_create": "2008-03-08", "area": "cs.cr", "authors": ["sanadhya", "sarkar"]}, {"idpaper": "0803.1321", "title": "treewidth computation and extremal combinatorics", "abstract": "for a given graph g and integers b,f >= 0, let s be a subset of vertices of g of size b+1 such that the subgraph of g induced by s is connected and s can be separated from other vertices of g by removing f vertices. we prove that every graph on n vertices contains at most n\\binom{b+f}{b} such vertex subsets. this result from extremal combinatorics appears to be very useful in the design of several enumeration and exact algorithms. in particular, we use it to provide algorithms that for a given n-vertex graph g - compute the treewidth of g in time o(1.7549^n) by making use of exponential space and in time o(2.6151^n) and polynomial space; - decide in time o(({\\frac{2n+k+1}{3})^{k+1}\\cdot kn^6}) if the treewidth of g is at most k; - list all minimal separators of g in time o(1.6181^n) and all potential maximal cliques of g in time o(1.7549^n). this significantly improves previous algorithms for these problems.", "date_create": "2008-03-09", "area": "cs.ds", "authors": ["fomin", "villanger"]}, {"idpaper": "0803.1500", "title": "ncore: architecture and implementation of a flexible, collaborative   digital library", "abstract": "ncore is an open source architecture and software platform for creating flexible, collaborative digital libraries. ncore was developed by the national science digital library (nsdl) project, and it serves as the central technical infrastructure for nsdl. ncore consists of a central fedora-based digital repository, a specific data model, an api, and a set of backend services and frontend tools that create a new model for collaborative, contributory digital libraries. this paper describes ncore, presents and analyzes its architecture, tools and services; and reports on the experience of nsdl in building and operating a major digital library on it over the past year and the experience of the digital library for earth systems education in porting their existing digital library and tools to the ncore platform.", "date_create": "2008-03-10", "area": "cs.dl", "authors": ["krafft", "birkland", "cramer"]}, {"idpaper": "0803.1596", "title": "using intelligent agents to understand organisational behaviour", "abstract": "this paper introduces two ongoing research projects which seek to apply computer modelling techniques in order to simulate human behaviour within organisations. previous research in other disciplines has suggested that complex social behaviours are governed by relatively simple rules which, when identified, can be used to accurately model such processes using computer technology. the broad objective of our research is to develop a similar capability within organisational psychology.", "date_create": "2008-03-11", "area": "cs.ne cs.ma", "authors": ["celia", "clegg", "robinson", "siebers", "aickelin", "sprigg"]}, {"idpaper": "0803.1596", "title": "using intelligent agents to understand organisational behaviour", "abstract": "this paper introduces two ongoing research projects which seek to apply computer modelling techniques in order to simulate human behaviour within organisations. previous research in other disciplines has suggested that complex social behaviours are governed by relatively simple rules which, when identified, can be used to accurately model such processes using computer technology. the broad objective of our research is to develop a similar capability within organisational psychology.", "date_create": "2008-03-11", "area": "cs.ne cs.ma", "authors": ["celia", "clegg", "robinson", "siebers", "aickelin", "sprigg"]}, {"idpaper": "0803.1723", "title": "estimation of available bandwidth and measurement infrastructure for   russian segment of internet", "abstract": "in paper the method for estimation of available bandwidth is supposed which does not demand the advanced utilities. our method is based on the measurement of network delay $d$ for packets of different sizes $w$. the simple expression for available bandwidth $b_{av} =(w_2-w_1)/(d_2-d_1)$ is substantiated. for the experimental testing the measurement infrastructure for russian segment of internet was installed in framework of rfbr grant 06-07-89074.", "date_create": "2008-03-12", "area": "cs.ni cs.pf", "authors": ["platonov", "sidelnikov", "strizhov", "sukhov"]}, {"idpaper": "0803.1748", "title": "a computational framework for the near elimination of spreadsheet risk", "abstract": "we present risk integrated's enterprise spreadsheet platform (esp), a technical approach to the near-elimination of spreadsheet risk in the enterprise computing environment, whilst maintaining the full flexibility of spreadsheets for modelling complex financial structures and processes. in its basic mode of use, the system comprises a secure and robust centralised spreadsheet management framework. in advanced mode, the system can be viewed as a robust computational framework whereby users can \"submit jobs\" to the spreadsheet, and retrieve the results from the computations, but with no direct access to the underlying spreadsheet. an example application, monte carlo simulation, is presented to highlight the benefits of this approach with regard to mitigating spreadsheet risk in complex, mission-critical, financial calculations.", "date_create": "2008-03-12", "area": "cs.se cs.cy", "authors": ["jafry", "sidoroff", "chi"]}, {"idpaper": "0803.1944", "title": "early experiences in traffic engineering exploiting path diversity: a   practical approach", "abstract": "recent literature has proved that stable dynamic routing algorithms have solid theoretical foundation that makes them suitable to be implemented in a real protocol, and used in practice in many different operational network contexts. such algorithms inherit much of the properties of congestion controllers implementing one of the possible combination of aqm/ecn schemes at nodes and flow control at sources. in this paper we propose a linear program formulation of the multi-commodity flow problem with congestion control, under max-min fairness, comprising demands with or without exogenous peak rates. our evaluations of the gain, using path diversity, in scenarios as intra-domain traffic engineering and wireless mesh networks encourages real implementations, especially in presence of hot spots demands and non uniform traffic matrices. we propose a flow aware perspective of the subject by using a natural multi-path extension to current congestion controllers and show its performance with respect to current proposals. since flow aware architectures exploiting path diversity are feasible, scalable, robust and nearly optimal in presence of flows with exogenous peak rates, we claim that our solution rethinked in the context of realistic traffic assumptions performs as better as an optimal approach with all the additional benefits of the flow aware paradigm.", "date_create": "2008-03-13", "area": "cs.ni", "authors": ["muscariello", "perino"]}, {"idpaper": "0803.2220", "title": "the anatomy of mitos web search engine", "abstract": "engineering a web search engine offering effective and efficient information retrieval is a challenging task. this document presents our experiences from designing and developing a web search engine offering a wide spectrum of functionalities and we report some interesting experimental results. a rather peculiar design choice of the engine is that its index is based on a dbms, while some of the distinctive functionalities that are offered include advanced greek language stemming, real time result clustering, and advanced link analysis techniques (also for spam page detection).", "date_create": "2008-03-14", "area": "cs.ir", "authors": ["papadakos", "vasiliadis", "theoharis", "armenatzoglou", "kopidaki", "marketakis", "daskalakis", "karamaroudis", "linardakis", "makrydakis", "papathanasiou", "sardis", "tsialiamanis", "troullinou", "vandikas", "velegrakis", "tzitzikas"]}, {"idpaper": "0803.2559", "title": "logical queries over views: decidability and expressiveness", "abstract": "we study the problem of deciding satisfiability of first order logic queries over views, our aim being to delimit the boundary between the decidable and the undecidable fragments of this language. views currently occupy a central place in database research, due to their role in applications such as information integration and data warehousing. our main result is the identification of a decidable class of first order queries over unary conjunctive views that generalises the decidability of the classical class of first order sentences over unary relations, known as the lowenheim class. we then demonstrate how various extensions of this class lead to undecidability and also provide some expressivity results. besides its theoretical interest, our new decidable class is potentially interesting for use in applications such as deciding implication of complex dependencies, analysis of a restricted class of active database rules, and ontology reasoning.", "date_create": "2008-03-17", "area": "cs.lo cs.db", "authors": ["bailey", "dong", "to"]}, {"idpaper": "0803.2967", "title": "building better nurse scheduling algorithms", "abstract": "the aim of this research is twofold: firstly, to model and solve a complex nurse scheduling problem with an integer programming formulation and evolutionary algorithms. secondly, to detail a novel statistical method of comparing and hence building better scheduling algorithms by identifying successful algorithm modifications. the comparison method captures the results of algorithms in a single figure that can then be compared using traditional statistical techniques. thus, the proposed method of comparing algorithms is an objective procedure designed to assist in the process of improving an algorithm. this is achieved even when some results are non-numeric or missing due to infeasibility. the final algorithm outperforms all previous evolutionary algorithms, which relied on human expertise for modification.", "date_create": "2008-03-20", "area": "cs.ne cs.ce", "authors": ["aickelin", "white"]}, {"idpaper": "0803.2967", "title": "building better nurse scheduling algorithms", "abstract": "the aim of this research is twofold: firstly, to model and solve a complex nurse scheduling problem with an integer programming formulation and evolutionary algorithms. secondly, to detail a novel statistical method of comparing and hence building better scheduling algorithms by identifying successful algorithm modifications. the comparison method captures the results of algorithms in a single figure that can then be compared using traditional statistical techniques. thus, the proposed method of comparing algorithms is an objective procedure designed to assist in the process of improving an algorithm. this is achieved even when some results are non-numeric or missing due to infeasibility. the final algorithm outperforms all previous evolutionary algorithms, which relied on human expertise for modification.", "date_create": "2008-03-20", "area": "cs.ne cs.ce", "authors": ["aickelin", "white"]}, {"idpaper": "0803.3850", "title": "state estimation over wireless channels using multiple sensors:   asymptotic behaviour and optimal power allocation", "abstract": "this paper considers state estimation of linear systems using analog amplify and forwarding with multiple sensors, for both multiple access and orthogonal access schemes. optimal state estimation can be achieved at the fusion center using a time varying kalman filter. we show that in many situations, the estimation error covariance decays at a rate of $1/m$ when the number of sensors $m$ is large. we consider optimal allocation of transmission powers that 1) minimizes the sum power usage subject to an error covariance constraint and 2) minimizes the error covariance subject to a sum power constraint. in the case of fading channels with channel state information the optimization problems are solved using a greedy approach, while for fading channels without channel state information but with channel statistics available a sub-optimal linear estimator is derived.", "date_create": "2008-03-26", "area": "cs.it math.it", "authors": ["leong", "dey", "evans"]}, {"idpaper": "0804.0556", "title": "rubberedge: reducing clutching by combining position and rate control   with elastic feedback", "abstract": "position control devices enable precise selection, but significant clutching degrades performance. clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. elastic rate control eliminates clutching completely, but can make precise selection difficult. we show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. unlike previous work, our rubberedge technique eliminates trajectory and velocity discontinuities. we derive predictive models for position control with clutching and hybrid control, and present a prototype rubberedge position-rate control device including initial user feedback.", "date_create": "2008-04-03", "area": "cs.hc", "authors": ["casiez", "vogel", "pan", "chaillou"]}, {"idpaper": "0804.0556", "title": "rubberedge: reducing clutching by combining position and rate control   with elastic feedback", "abstract": "position control devices enable precise selection, but significant clutching degrades performance. clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. elastic rate control eliminates clutching completely, but can make precise selection difficult. we show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. unlike previous work, our rubberedge technique eliminates trajectory and velocity discontinuities. we derive predictive models for position control with clutching and hybrid control, and present a prototype rubberedge position-rate control device including initial user feedback.", "date_create": "2008-04-03", "area": "cs.hc", "authors": ["casiez", "vogel", "pan", "chaillou"]}, {"idpaper": "0804.0573", "title": "an artificial immune system as a recommender system for web sites", "abstract": "artificial immune systems have been used successfully to build recommender systems for film databases. in this research, an attempt is made to extend this idea to web site recommendation. a collection of more than 1000 individuals web profiles (alternatively called preferences / favourites / bookmarks file) will be used. urls will be classified using the dmoz (directory mozilla) database of the open directory project as our ontology. this will then be used as the data for the artificial immune systems rather than the actual addresses. the first attempt will involve using a simple classification code number coupled with the number of pages within that classification code. however, this implementation does not make use of the hierarchical tree-like structure of dmoz. consideration will then be given to the construction of a similarity measure for web profiles that makes use of this hierarchical information to build a better-informed artificial immune system.", "date_create": "2008-04-03", "area": "cs.ne cs.ai", "authors": ["morrison", "aickelin"]}, {"idpaper": "0804.0573", "title": "an artificial immune system as a recommender system for web sites", "abstract": "artificial immune systems have been used successfully to build recommender systems for film databases. in this research, an attempt is made to extend this idea to web site recommendation. a collection of more than 1000 individuals web profiles (alternatively called preferences / favourites / bookmarks file) will be used. urls will be classified using the dmoz (directory mozilla) database of the open directory project as our ontology. this will then be used as the data for the artificial immune systems rather than the actual addresses. the first attempt will involve using a simple classification code number coupled with the number of pages within that classification code. however, this implementation does not make use of the hierarchical tree-like structure of dmoz. consideration will then be given to the construction of a similarity measure for web profiles that makes use of this hierarchical information to build a better-informed artificial immune system.", "date_create": "2008-04-03", "area": "cs.ne cs.ai", "authors": ["morrison", "aickelin"]}, {"idpaper": "0804.1653", "title": "nonextensive generalizations of the jensen-shannon divergence", "abstract": "convexity is a key concept in information theory, namely via the many implications of jensen's inequality, such as the non-negativity of the kullback-leibler divergence (kld). jensen's inequality also underlies the concept of jensen-shannon divergence (jsd), which is a symmetrized and smoothed version of the kld. this paper introduces new jsd-type divergences, by extending its two building blocks: convexity and shannon's entropy. in particular, a new concept of q-convexity is introduced and shown to satisfy a jensen's q-inequality. based on this jensen's q-inequality, the jensen-tsallis q-difference is built, which is a nonextensive generalization of the jsd, based on tsallis entropies. finally, the jensen-tsallis q-difference is charaterized in terms of convexity and extrema.", "date_create": "2008-04-10", "area": "cs.it math.it math.st stat.th", "authors": ["martins", "aguiar", "figueiredo"]}, {"idpaper": "0804.2288", "title": "parimutuel betting on permutations", "abstract": "we focus on a permutation betting market under parimutuel call auction model where traders bet on the final ranking of n candidates. we present a proportional betting mechanism for this market. our mechanism allows the traders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards them proportionally to the number of pairs that appear in the final outcome. we show that market organizer's decision problem for this mechanism can be formulated as a convex program of polynomial size. more importantly, the formulation yields a set of n x n unique marginal prices that are sufficient to price the bets in this mechanism, and are computable in polynomial-time. the marginal prices reflect the traders' beliefs about the marginal distributions over outcomes. we also propose techniques to compute the joint distribution over n! permutations from these marginal distributions. we show that using a maximum entropy criterion, we can obtain a concise parametric form (with only n x n parameters) for the joint distribution which is defined over an exponentially large state space. we then present an approximation algorithm for computing the parameters of this distribution. in fact, the algorithm addresses the generic problem of finding the maximum entropy distribution over permutations that has a given mean, and may be of independent interest.", "date_create": "2008-04-14", "area": "cs.gt cs.cc cs.ds cs.ma", "authors": ["agrawal", "wang", "ye"]}, {"idpaper": "0804.2701", "title": "information resources in high-energy physics: surveying the present   landscape and charting the future course", "abstract": "access to previous results is of paramount importance in the scientific process. recent progress in information management focuses on building e-infrastructures for the optimization of the research workflow, through both policy-driven and user-pulled dynamics. for decades, high-energy physics (hep) has pioneered innovative solutions in the field of information management and dissemination. in light of a transforming information environment, it is important to assess the current usage of information resources by researchers and hep provides a unique test-bed for this assessment. a survey of about 10% of practitioners in the field reveals usage trends and information needs. community-based services, such as the pioneering arxiv and spires systems, largely answer the need of the scientists, with a limited but increasing fraction of younger users relying on google. commercial services offered by publishers or database vendors are essentially unused in the field. the survey offers an insight into the most important features that users require to optimize their research workflow. these results inform the future evolution of information management in hep and, as these researchers are traditionally ``early adopters'' of innovation in scholarly communication, can inspire developments of disciplinary repositories serving other communities.", "date_create": "2008-04-16", "area": "cs.dl", "authors": ["gentil-beccot", "mele", "holtkamp", "o'connell", "brooks"]}, {"idpaper": "0804.4391", "title": "a lower bound on the bayesian mse based on the optimal bias function", "abstract": "a lower bound on the minimum mean-squared error (mse) in a bayesian estimation problem is proposed in this paper. this bound utilizes a well-known connection to the deterministic estimation setting. using the prior distribution, the bias function which minimizes the cramer-rao bound can be determined, resulting in a lower bound on the bayesian mse. the bound is developed for the general case of a vector parameter with an arbitrary probability distribution, and is shown to be asymptotically tight in both the high and low signal-to-noise ratio regimes. a numerical study demonstrates several cases in which the proposed technique is both simpler to compute and tighter than alternative methods.", "date_create": "2008-04-28", "area": "cs.it math.it", "authors": ["ben-haim", "eldar"]}, {"idpaper": "0804.4466", "title": "free distance bounds for protograph-based regular ldpc convolutional   codes", "abstract": "in this paper asymptotic methods are used to form lower bounds on the free distance to constraint length ratio of several ensembles of regular, asymptotically good, protograph-based ldpc convolutional codes. in particular, we show that the free distance to constraint length ratio of the regular ldpc convolutional codes exceeds that of the minimum distance to block length ratio of the corresponding ldpc block codes.", "date_create": "2008-04-28", "area": "cs.it math.it", "authors": ["mitchell", "pusane", "goertz", "costello"]}, {"idpaper": "0804.4523", "title": "a non-distillability criterion for secret correlations", "abstract": "within entanglement theory there are criteria which certify that some quantum states cannot be distilled into pure entanglement. an example is the positive partial transposition criterion. here we present, for the first time, the analogous thing for secret correlations. we introduce a computable criterion which certifies that a probability distribution between two honest parties and an eavesdropper cannot be (asymptotically) distilled into a secret key. the existence of non-distillable correlations with positive secrecy cost, also known as bound information, is an open question. this criterion may be the key for finding bound information. however, if it turns out that this criterion does not detect bound information, then, a very interesting consequence follows: any distribution with positive secrecy cost can increase the secrecy content of another distribution. in other words, all correlations with positive secrecy cost constitute a useful resource.", "date_create": "2008-04-28", "area": "quant-ph cs.cr", "authors": ["masanes", "winter"]}, {"idpaper": "0804.4666", "title": "combining geometry and combinatorics: a unified approach to sparse   signal recovery", "abstract": "there are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. the geometric approach starts with a geometric constraint on the measurement matrix and then uses linear programming to decode information about the signal from its measurements. the combinatorial approach constructs the measurement matrix and a combinatorial decoding algorithm to match. we present a unified approach to these two classes of sparse signal recovery algorithms.   the unifying elements are the adjacency matrices of high-quality unbalanced expanders. we generalize the notion of restricted isometry property (rip), crucial to compressed sensing results for signal recovery, from the euclidean norm to the l_p norm for p about 1, and then show that unbalanced expanders are essentially equivalent to rip-p matrices.   from known deterministic constructions for such matrices, we obtain new deterministic measurement matrix constructions and algorithms for signal recovery which, compared to previous deterministic algorithms, are superior in either the number of measurements or in noise tolerance.", "date_create": "2008-04-29", "area": "cs.dm cs.ds cs.na", "authors": ["berinde", "gilbert", "indyk", "karloff", "strauss"]}, {"idpaper": "0804.4749", "title": "study of improving nano-contouring performance by employing   cross-coupling controller", "abstract": "for the tracking stage path planning, we design a two-axis cross-coupling control system which uses the pi controller to compensate the contour error between axes. in this paper, the stage adoptive is designed by our laboratory (precision machine center of national formosa university). the cross-coupling controller calculates the actuating signal of each axis by combining multi-axes position error. hence, the cross-coupling controller improves the stage tracking ability and decreases the contour error. the experiments show excellent stage motion. this finding confirms that the proposed method is a powerful and efficient tool for improving stage tracking ability. also found were the stages tracking to minimize contour error of two types circular to approximately 25nm.", "date_create": "2008-04-30", "area": "cs.ro", "authors": ["jywe", "chen", "wang", "liu", "jwo", "teng", "hsieh"]}, {"idpaper": "0804.4865", "title": "characterizing video responses in social networks", "abstract": "video sharing sites, such as youtube, use video responses to enhance the social interactions among their users. the video response feature allows users to interact and converse through video, by creating a video sequence that begins with an opening video and followed by video responses from other users. our characterization is over 3.4 million videos and 400,000 video responses collected from youtube during a 7-day period. we first analyze the characteristics of the video responses, such as popularity, duration, and geography. we then examine the social networks that emerge from the video response interactions.", "date_create": "2008-04-30", "area": "cs.mm cs.cy cs.hc", "authors": ["benevenuto", "duarte", "rodrigues", "almeida", "almeida", "ross"]}, {"idpaper": "0804.4865", "title": "characterizing video responses in social networks", "abstract": "video sharing sites, such as youtube, use video responses to enhance the social interactions among their users. the video response feature allows users to interact and converse through video, by creating a video sequence that begins with an opening video and followed by video responses from other users. our characterization is over 3.4 million videos and 400,000 video responses collected from youtube during a 7-day period. we first analyze the characteristics of the video responses, such as popularity, duration, and geography. we then examine the social networks that emerge from the video response interactions.", "date_create": "2008-04-30", "area": "cs.mm cs.cy cs.hc", "authors": ["benevenuto", "duarte", "rodrigues", "almeida", "almeida", "ross"]}, {"idpaper": "0805.0861", "title": "uv direct-writing of metals on polyimide", "abstract": "conductive micro-patterned copper tracks were fabricated by uv direct-writing of a nanoparticle silver seed layer followed by selective electroless copper deposition. silver ions were first incorporated into a hydrolyzed polyimide surface layer by wet chemical treatment. a photoreactive polymer coating, methoxy poly(ethylene glycol) (mpeg) was coated on top of the substrate prior to uv irradiation. electrons released through the interaction between the mpeg molecules and uv photons allowed the reduction of the silver ions across the mpeg/doped polyimide interface. the resultant silver seed layer has a cluster morphology which is suitable for the initiation of electroless plating. initial results showed that the deposited copper tracks were in good agreement with the track width on the photomask and laser direct-writing can also fabricate smaller line width metal tracks with good accuracy. the facile fabrication presented here can be carried out in air, at atmospheric pressure, and on contoured surfaces.", "date_create": "2008-05-07", "area": "cs.oh", "authors": ["ng", "desmulliez", "mccarthy", "suyal", "prior", "hand"]}, {"idpaper": "0805.0868", "title": "manufacturing of a micro probe using supersonic aided electrolysis   process", "abstract": "in this paper, a practical micromachining technology was applied for the fabrication of a micro probe using a complex nontraditional machining process. a series process was combined to machine tungsten carbide rods from original dimension. the original dimension of tungsten carbide rods was 3mm ; the rods were ground to a fixed-dimension of 50 micrometers using precision grinding machine in first step. and then, the rod could be machined to a middle-dimension of 20 micrometers by electrolysis. a final desired micro dimension can be achieved using supersonic aided electrolysis. high-aspect-ratio of micro tungsten carbide rod was easily obtained by this process. surface roughness of the sample with supersonic aided agitation was compared with that with no agitation in electrolysis. the machined surface of the sample is very smooth due to ionized particles of anode could be removed by supersonic aided agitation during electrolysis. deep micro holes can also be achieved by the machined high-aspect-rati tungsten carbide rod using edm process. a micro probe of a ball shape at the end was processed by proposed supersonic aided electrolysis machining process.", "date_create": "2008-05-07", "area": "cs.oh", "authors": ["shyu", "weng", "ho"]}, {"idpaper": "0805.0904", "title": "micromachined inclinometer based on fluid convection", "abstract": "this paper presents a numerical simulation and experimental results of a one-dimensional thermal inclinometer with the cavity filled of gas and liquid. the sensor principle consists of one heating resistor placed between two detectors. when the resistor is electrically powered, it creates a symmetrical temperature profile inside a micromachined silicon cavity. by applying a tilt to the sensor, the profile shifts in the same direction of the sensible axis corresponding to the horizontal one to one. the temperature profile and the sensitivity according to the co2 gas and mineral oil sae50 have been studied using numerical resolution of fluid dynamics equations with the computational fluid dynamics (cfd) software package fluent v6.2. we have shown that the sensitivity of liquid sensors is higher than the gas sensors one. by using micromachined silicon technique, a thermal inclinometer with one pair of detectors placed at 300 um from the heater has been made. experimental measurements corroborate with the numerical simulation.", "date_create": "2008-05-07", "area": "cs.oh", "authors": ["crespy", "courteaud", "combette", "boyer", "giani", "foucaran"]}, {"idpaper": "0805.1226", "title": "spectrum allocation in two-tier networks", "abstract": "two-tier networks, comprising a conventional cellular network overlaid with shorter range hotspots (e.g. femtocells, distributed antennas, or wired relays), offer an economically viable way to improve cellular system capacity. the capacity-limiting factor in such networks is interference. the cross-tier interference between macrocells and femtocells can suffocate the capacity due to the near-far problem, so in practice hotspots should use a different frequency channel than the potentially nearby high-power macrocell users. centralized or coordinated frequency planning, which is difficult and inefficient even in conventional cellular networks, is all but impossible in a two-tier network. this paper proposes and analyzes an optimum decentralized spectrum allocation policy for two-tier networks that employ frequency division multiple access (including ofdma). the proposed allocation is optimal in terms of area spectral efficiency (ase), and is subjected to a sensible quality of service (qos) requirement, which guarantees that both macrocell and femtocell users attain at least a prescribed data rate. results show the dependence of this allocation on the qos requirement, hotspot density and the co-channel interference from the macrocell and surrounding femtocells. design interpretations of this result are provided.", "date_create": "2008-05-08", "area": "cs.ni", "authors": ["chandrasekhar", "andrews"]}, {"idpaper": "0805.1489", "title": "modeling and verifying a broad array of network properties", "abstract": "motivated by widely observed examples in nature, society and software, where groups of already related nodes arrive together and attach to an existing network, we consider network growth via sequential attachment of linked node groups, or graphlets. we analyze the simplest case, attachment of the three node v-graphlet, where, with probability alpha, we attach a peripheral node of the graphlet, and with probability (1-alpha), we attach the central node. our analytical results and simulations show that tuning alpha produces a wide range in degree distribution and degree assortativity, achieving assortativity values that capture a diverse set of many real-world systems. we introduce a fifteen-dimensional attribute vector derived from seven well-known network properties, which enables comprehensive comparison between any two networks. principal component analysis (pca) of this attribute vector space shows a significantly larger coverage potential of real-world network properties by a simple extension of the above model when compared against a classic model of network growth.", "date_create": "2008-05-12", "area": "cond-mat.stat-mech cs.se q-bio.qm stat.ap", "authors": ["filkov", "saul", "roy", "d'souza", "devanbu"]}, {"idpaper": "0805.1877", "title": "perfect tag identification protocol in rfid networks", "abstract": "radio frequency identification (rfid) systems are becoming more and more popular in the field of ubiquitous computing, in particular for objects identification. an rfid system is composed by one or more readers and a number of tags. one of the main issues in an rfid network is the fast and reliable identification of all tags in the reader range. the reader issues some queries, and tags properly answer. then, the reader must identify the tags from such answers. this is crucial for most applications. since the transmission medium is shared, the typical problem to be faced is a mac-like one, i.e. to avoid or limit the number of tags transmission collisions. we propose a protocol which, under some assumptions about transmission techniques, always achieves a 100% perfomance. it is based on a proper recursive splitting of the concurrent tags sets, until all tags have been identified. the other approaches present in literature have performances of about 42% in the average at most. the counterpart is a more sophisticated hardware to be deployed in the manufacture of low cost tags.", "date_create": "2008-05-13", "area": "cs.ni", "authors": ["bonuccelli", "lonetti", "martelli"]}, {"idpaper": "0805.3217", "title": "statistical region-based active contours with exponential family   observations", "abstract": "in this paper, we focus on statistical region-based active contour models where image features (e.g. intensity) are random variables whose distribution belongs to some parametric family (e.g. exponential) rather than confining ourselves to the special gaussian case. using shape derivation tools, our effort focuses on constructing a general expression for the derivative of the energy (with respect to a domain) and derive the corresponding evolution speed. a general result is stated within the framework of multi-parameter exponential family. more particularly, when using maximum likelihood estimators, the evolution speed has a closed-form expression that depends simply on the probability density function, while complicating additive terms appear when using other estimators, e.g. moments method. experimental results on both synthesized and real images demonstrate the applicability of our approach.", "date_create": "2008-05-21", "area": "cs.cv", "authors": ["lecellier", "jehan-besson", "fadili", "aubert", "revenu"]}, {"idpaper": "0805.4606", "title": "community detection using a measure of global influence", "abstract": "the growing popularity of online social networks has provided researchers with access to large amount of social network data. this, coupled with the ever increasing computation speed, storage capacity and data mining capabilities, led to the renewal of interest in automatic community detection methods. surprisingly, there is no universally accepted definition of the community. one frequently used definition states that ``communities, that have more and/or better-connected `internal edges' connecting members of the set than `cut edges' connecting the set to the rest of the world''[leskovec et al. 20008]. this definition inspired the modularity-maximization class of community detection algorithms, which look for regions of the network that have higher than expected density of edges within them. we introduce an alternative definition which states that a community is composed of individuals who have more influence on others within the community than on those outside of it. we present a mathematical formulation of influence, define an influence-based modularity metric, and show how to use it to partition the network into communities. we evaluated our approach on the standard data sets used in literature, and found that it often outperforms the edge-based modularity algorithm.", "date_create": "2008-05-29", "area": "cs.cy", "authors": ["ghosh", "lerman"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1006", "title": "the vo-neural project: recent developments and some applications", "abstract": "vo-neural is the natural evolution of the astroneural project which was started in 1994 with the aim to implement a suite of neural tools for data mining in astronomical massive data sets. at a difference with its ancestor, which was implemented under matlab, vo-neural is written in c++, object oriented, and it is specifically tailored to work in distributed computing architectures. we discuss the current status of implementation of vo-neural, present an application to the classification of active galactic nuclei, and outline the ongoing work to improve the functionalities of the package.", "date_create": "2008-06-05", "area": "astro-ph cs.ce", "authors": ["brescia", "cavuoti", "d'angelo", "d'abrusco", "deniskina", "garofalo", "laurino", "longo", "nocella", "skordovski"]}, {"idpaper": "0806.1284", "title": "the separation of duty with privilege calculus", "abstract": "this paper presents privilege calculus (pc) as a new approach of knowledge representation for separation of duty (sd) in the view of process and intents to improve the reconfigurability and traceability of sd. pc presumes that the structure of sd should be reduced to the structure of privilege and then the regulation of system should be analyzed with the help of forms of privilege.", "date_create": "2008-06-07", "area": "cs.cr cs.lo", "authors": ["lv", "wang", "liu", "you"]}, {"idpaper": "0806.1816", "title": "cardinality heterogeneities in web service composition: issues and   solutions", "abstract": "data exchanges between web services engaged in a composition raise several heterogeneities. in this paper, we address the problem of data cardinality heterogeneity in a composition. firstly, we build a theoretical framework to describe different aspects of web services that relate to data cardinality, and secondly, we solve this problem by developing a solution for cardinality mediation based on constraint logic programming.", "date_create": "2008-06-11", "area": "cs.se cs.db", "authors": ["mrissa", "thiran", "jacquet", "benslimane", "maamar"]}, {"idpaper": "0806.1816", "title": "cardinality heterogeneities in web service composition: issues and   solutions", "abstract": "data exchanges between web services engaged in a composition raise several heterogeneities. in this paper, we address the problem of data cardinality heterogeneity in a composition. firstly, we build a theoretical framework to describe different aspects of web services that relate to data cardinality, and secondly, we solve this problem by developing a solution for cardinality mediation based on constraint logic programming.", "date_create": "2008-06-11", "area": "cs.se cs.db", "authors": ["mrissa", "thiran", "jacquet", "benslimane", "maamar"]}, {"idpaper": "0806.2937", "title": "evolving complex networks with conserved clique distributions", "abstract": "we propose and study a hierarchical algorithm to generate graphs having a predetermined distribution of cliques, the fully connected subgraphs. the construction mechanism may be either random or incorporate preferential attachment. we evaluate the statistical properties of the graphs generated, such as the degree distribution and network diameters, and compare them to some real-world graphs.", "date_create": "2008-06-18", "area": "physics.soc-ph cond-mat.dis-nn cs.ni", "authors": ["kaczor", "gros"]}, {"idpaper": "0806.3201", "title": "the structure of information pathways in a social communication network", "abstract": "social networks are of interest to researchers in part because they are thought to mediate the flow of information in communities and organizations. here we study the temporal dynamics of communication using on-line data, including e-mail communication among the faculty and staff of a large university over a two-year period. we formulate a temporal notion of \"distance\" in the underlying social network by measuring the minimum time required for information to spread from one node to another -- a concept that draws on the notion of vector-clocks from the study of distributed computing systems. we find that such temporal measures provide structural insights that are not apparent from analyses of the pure social network topology. in particular, we define the network backbone to be the subgraph consisting of edges on which information has the potential to flow the quickest. we find that the backbone is a sparse graph with a concentration of both highly embedded edges and long-range bridges -- a finding that sheds new light on the relationship between tie strength and connectivity in social networks.", "date_create": "2008-06-19", "area": "physics.soc-ph cs.ds physics.data-an", "authors": ["kossinets", "kleinberg", "watts"]}, {"idpaper": "0806.3201", "title": "the structure of information pathways in a social communication network", "abstract": "social networks are of interest to researchers in part because they are thought to mediate the flow of information in communities and organizations. here we study the temporal dynamics of communication using on-line data, including e-mail communication among the faculty and staff of a large university over a two-year period. we formulate a temporal notion of \"distance\" in the underlying social network by measuring the minimum time required for information to spread from one node to another -- a concept that draws on the notion of vector-clocks from the study of distributed computing systems. we find that such temporal measures provide structural insights that are not apparent from analyses of the pure social network topology. in particular, we define the network backbone to be the subgraph consisting of edges on which information has the potential to flow the quickest. we find that the backbone is a sparse graph with a concentration of both highly embedded edges and long-range bridges -- a finding that sheds new light on the relationship between tie strength and connectivity in social networks.", "date_create": "2008-06-19", "area": "physics.soc-ph cs.ds physics.data-an", "authors": ["kossinets", "kleinberg", "watts"]}, {"idpaper": "0806.4286", "title": "implementation for blow up of tornado-type solutions for complex version   of 3d navier-stokes system", "abstract": "we consider cauchy problem for fourier transformation of 3-dimensional navier-stokes system with zero external force. using initial data purposed by dong li and ya.g.sinai we implement self-similar regime producing fast growing behavior of the energy of solution while time tends to critical value.", "date_create": "2008-06-26", "area": "cs.na", "authors": ["arnold", "khokhlov"]}, {"idpaper": "0806.4293", "title": "scalar quantization for audio data coding", "abstract": "this paper is concerned with scalar quantization of transform coefficients in an audio codec. the generalized gaussian distribution (ggd) is used as an approximation of one-dimensional probability density function for transform coefficients obtained by modulated lapped transform (mlt) or modified cosine transform (mdct) filterbank. the rationale of the model is provided in comparison with theoretically achievable rate-distortion function. the rate-distortion function computed for the random sequence obtained from a real sequence of samples from a large database is compared with that computed for random sequence obtained by a ggd random generator. a simple algorithm of constructing the extended zero zone (ezz) quantizer is proposed. simulation results show that the ezz quantizer yields a negligible loss in terms of coding efficiency compared to optimal scalar quantizers. furthermore, we describe an adaptive version of the ezz quantizer which works efficiently with low bitrate requirements for transmitting side information", "date_create": "2008-06-26", "area": "cs.mm cs.it math.it", "authors": ["kudryashov", "porov", "oh"]}, {"idpaper": "0807.1228", "title": "restricted mobility improves delay-throughput trade-offs in mobile   ad-hoc networks", "abstract": "in this paper, we analyze asymptotic delay-throughput trade-offs in mobile ad-hoc networks comprising heterogeneous nodes with restricted mobility. we show that node spatial heterogeneity has the ability to drastically improve upon existing scaling laws established under the assumption that nodes are identical and uniformly visit the entire network area. in particular, we consider the situation in which each node moves around its own home-point according to a restricted mobility process which results into a spatial stationary distribution that decays as a power law of exponent delta with the distance from the home-point. for such restricted mobility model, we propose a novel class of scheduling and routing schemes, which significantly outperforms all delay-throughput results previously obtained in the case of identical nodes. in particular, for delta = 2 it is possible to achieve almost constant delay and almost constant per-node throughput (except for a poly-logarithmic factor) as the number of nodes increases, even without resorting to sophisticated coding or signal processing techniques.", "date_create": "2008-07-08", "area": "cs.pf cs.ni", "authors": ["garetto", "leonardi"]}, {"idpaper": "0807.1228", "title": "restricted mobility improves delay-throughput trade-offs in mobile   ad-hoc networks", "abstract": "in this paper, we analyze asymptotic delay-throughput trade-offs in mobile ad-hoc networks comprising heterogeneous nodes with restricted mobility. we show that node spatial heterogeneity has the ability to drastically improve upon existing scaling laws established under the assumption that nodes are identical and uniformly visit the entire network area. in particular, we consider the situation in which each node moves around its own home-point according to a restricted mobility process which results into a spatial stationary distribution that decays as a power law of exponent delta with the distance from the home-point. for such restricted mobility model, we propose a novel class of scheduling and routing schemes, which significantly outperforms all delay-throughput results previously obtained in the case of identical nodes. in particular, for delta = 2 it is possible to achieve almost constant delay and almost constant per-node throughput (except for a poly-logarithmic factor) as the number of nodes increases, even without resorting to sophisticated coding or signal processing techniques.", "date_create": "2008-07-08", "area": "cs.pf cs.ni", "authors": ["garetto", "leonardi"]}, {"idpaper": "0807.2023", "title": "beyond node degree: evaluating as topology models", "abstract": "many models have been proposed to generate internet autonomous system (as) topologies, most of which make structural assumptions about the as graph. in this paper we compare as topology generation models with several observed as topologies. in contrast to most previous works, we avoid making assumptions about which topological properties are important to characterize the as topology. our analysis shows that, although matching degree-based properties, the existing as topology generation models fail to capture the complexity of the local interconnection structure between ass. furthermore, we use bgp data from multiple vantage points to show that additional measurement locations significantly affect local structure properties, such as clustering and node centrality. degree-based properties, however, are not notably affected by additional measurements locations. these observations are particularly valid in the core. the shortcomings of as topology generation models stems from an underestimation of the complexity of the connectivity in the core caused by inappropriate use of bgp data.", "date_create": "2008-07-13", "area": "cs.ni", "authors": ["haddadi", "fay", "jamakovic", "maennel", "moore", "mortier", "rio", "uhlig"]}, {"idpaper": "0807.2515", "title": "the dark energy survey data management system", "abstract": "the dark energy survey collaboration will study cosmic acceleration with a 5000 deg2 grizy survey in the southern sky over 525 nights from 2011-2016. the des data management (desdm) system will be used to process and archive these data and the resulting science ready data products. the desdm system consists of an integrated archive, a processing framework, an ensemble of astronomy codes and a data access framework. we are developing the desdm system for operation in the high performance computing (hpc) environments at ncsa and fermilab. operating the desdm system in an hpc environment offers both speed and flexibility. we will employ it for our regular nightly processing needs, and for more compute-intensive tasks such as large scale image coaddition campaigns, extraction of weak lensing shear from the full survey dataset, and massive seasonal reprocessing of the des data. data products will be available to the collaboration and later to the public through a virtual-observatory compatible web portal. our approach leverages investments in publicly available hpc systems, greatly reducing hardware and maintenance costs to the project, which must deploy and maintain only the storage, database platforms and orchestration and web portal nodes that are specific to desdm. in fall 2007, we tested the current desdm system on both simulated and real survey data. we used teragrid to process 10 simulated des nights (3tb of raw data), ingesting and calibrating approximately 250 million objects into the des archive database. we also used desdm to process and calibrate over 50 nights of survey data acquired with the mosaic2 camera. comparison to truth tables in the case of the simulated data and internal crosschecks in the case of the real data indicate that astrometric and photometric data quality is excellent.", "date_create": "2008-07-16", "area": "astro-ph cs.dc", "authors": ["mohr", "barkhouse", "beldica", "bertin", "cai", "da costa", "darnell", "daues", "jarvis", "gower", "lin", "martelli", "neilsen", "ngeow", "ogando", "parga", "sheldon", "tucker", "kuropatkin", "stoughton"]}, {"idpaper": "0807.2515", "title": "the dark energy survey data management system", "abstract": "the dark energy survey collaboration will study cosmic acceleration with a 5000 deg2 grizy survey in the southern sky over 525 nights from 2011-2016. the des data management (desdm) system will be used to process and archive these data and the resulting science ready data products. the desdm system consists of an integrated archive, a processing framework, an ensemble of astronomy codes and a data access framework. we are developing the desdm system for operation in the high performance computing (hpc) environments at ncsa and fermilab. operating the desdm system in an hpc environment offers both speed and flexibility. we will employ it for our regular nightly processing needs, and for more compute-intensive tasks such as large scale image coaddition campaigns, extraction of weak lensing shear from the full survey dataset, and massive seasonal reprocessing of the des data. data products will be available to the collaboration and later to the public through a virtual-observatory compatible web portal. our approach leverages investments in publicly available hpc systems, greatly reducing hardware and maintenance costs to the project, which must deploy and maintain only the storage, database platforms and orchestration and web portal nodes that are specific to desdm. in fall 2007, we tested the current desdm system on both simulated and real survey data. we used teragrid to process 10 simulated des nights (3tb of raw data), ingesting and calibrating approximately 250 million objects into the des archive database. we also used desdm to process and calibrate over 50 nights of survey data acquired with the mosaic2 camera. comparison to truth tables in the case of the simulated data and internal crosschecks in the case of the real data indicate that astrometric and photometric data quality is excellent.", "date_create": "2008-07-16", "area": "astro-ph cs.dc", "authors": ["mohr", "barkhouse", "beldica", "bertin", "cai", "da costa", "darnell", "daues", "jarvis", "gower", "lin", "martelli", "neilsen", "ngeow", "ogando", "parga", "sheldon", "tucker", "kuropatkin", "stoughton"]}, {"idpaper": "0807.2983", "title": "on probability distributions for trees: representations, inference and   learning", "abstract": "we study probability distributions over free algebras of trees. probability distributions can be seen as particular (formal power) tree series [berstel et al 82, esik et al 03], i.e. mappings from trees to a semiring k . a widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. we argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. first, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. second, the algebraic representation can be easily extended to deal with unranked trees (like xml trees where a symbol may have an unbounded number of children). both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that hidden markov models are equivalent to nondeterministic string automata); nowadays applications for web information extraction, web services and document processing consider unranked trees.", "date_create": "2008-07-18", "area": "cs.lg", "authors": ["denis", "habrard", "gilleron", "tommasi", "gilbert"]}, {"idpaper": "0807.3622", "title": "tulipa: towards a multi-formalism parsing environment for grammar   engineering", "abstract": "in this paper, we present an open-source parsing environment (tuebingen linguistic parsing architecture, tulipa) which uses range concatenation grammar (rcg) as a pivot formalism, thus opening the way to the parsing of several mildly context-sensitive formalisms. this environment currently supports tree-based grammars (namely tree-adjoining grammars, tag) and multi-component tree-adjoining grammars with tree tuples (tt-mctag)) and allows computation not only of syntactic structures, but also of the corresponding semantic representations. it is used for the development of a tree-based grammar for german.", "date_create": "2008-07-23", "area": "cs.cl", "authors": ["kallmeyer", "lichte", "maier", "parmentier", "dellert", "evang"]}, {"idpaper": "0807.4582", "title": "lower bounds for embedding into distributions over excluded minor graph   families", "abstract": "it was shown recently by fakcharoenphol et al that arbitrary finite metrics can be embedded into distributions over tree metrics with distortion o(log n). it is also known that this bound is tight since there are expander graphs which cannot be embedded into distributions over trees with better than omega(log n) distortion.   we show that this same lower bound holds for embeddings into distributions over any minor excluded family. given a family of graphs f which excludes minor m where |m|=k, we explicitly construct a family of graphs with treewidth-(k+1) which cannot be embedded into a distribution over f with better than omega(log n) distortion. thus, while these minor excluded families of graphs are more expressive than trees, they do not provide asymptotically better approximations in general. an important corollary of this is that graphs of treewidth-k cannot be embedded into distributions over graphs of treewidth-(k-3) with distortion less than omega(log n).   we also extend a result of alon et al by showing that for any k, planar graphs cannot be embedded into distributions over treewidth-k graphs with better than omega(log n) distortion.", "date_create": "2008-07-29", "area": "cs.ds cs.dm", "authors": ["carroll", "goel"]}, {"idpaper": "0807.4626", "title": "approximate kernel clustering", "abstract": "in the kernel clustering problem we are given a large $n\\times n$ positive semi-definite matrix $a=(a_{ij})$ with $\\sum_{i,j=1}^na_{ij}=0$ and a small $k\\times k$ positive semi-definite matrix $b=(b_{ij})$. the goal is to find a partition $s_1,...,s_k$ of $\\{1,... n\\}$ which maximizes the quantity $$ \\sum_{i,j=1}^k (\\sum_{(i,j)\\in s_i\\times s_j}a_{ij})b_{ij}. $$ we study the computational complexity of this generic clustering problem which originates in the theory of machine learning. we design a constant factor polynomial time approximation algorithm for this problem, answering a question posed by song, smola, gretton and borgwardt. in some cases we manage to compute the sharp approximation threshold for this problem assuming the unique games conjecture (ugc). in particular, when $b$ is the $3\\times 3$ identity matrix the ugc hardness threshold of this problem is exactly $\\frac{16\\pi}{27}$. we present and study a geometric conjecture of independent interest which we show would imply that the ugc threshold when $b$ is the $k\\times k$ identity matrix is $\\frac{8\\pi}{9}(1-\\frac{1}{k})$ for every $k\\ge 3$.", "date_create": "2008-07-29", "area": "cs.ds cs.cc math.fa", "authors": ["khot", "naor"]}, {"idpaper": "0808.0103", "title": "use of astronomical literature - a report on usage patterns", "abstract": "in this paper we present a number of metrics for usage of the sao/nasa astrophysics data system (ads). since the ads is used by the entire astronomical community, these are indicative of how the astronomical literature is used. we will show how the use of the ads has changed both quantitatively and qualitatively. we will also show that different types of users access the system in different ways. finally, we show how use of the ads has evolved over the years in various regions of the world.   the ads is funded by nasa grant nng06gg68g.", "date_create": "2008-08-01", "area": "cs.dl astro-ph", "authors": ["henneken", "kurtz", "accomazzi", "grant", "thompson", "bohlen", "murray"]}, {"idpaper": "0808.0103", "title": "use of astronomical literature - a report on usage patterns", "abstract": "in this paper we present a number of metrics for usage of the sao/nasa astrophysics data system (ads). since the ads is used by the entire astronomical community, these are indicative of how the astronomical literature is used. we will show how the use of the ads has changed both quantitatively and qualitatively. we will also show that different types of users access the system in different ways. finally, we show how use of the ads has evolved over the years in various regions of the world.   the ads is funded by nasa grant nng06gg68g.", "date_create": "2008-08-01", "area": "cs.dl astro-ph", "authors": ["henneken", "kurtz", "accomazzi", "grant", "thompson", "bohlen", "murray"]}, {"idpaper": "0808.0103", "title": "use of astronomical literature - a report on usage patterns", "abstract": "in this paper we present a number of metrics for usage of the sao/nasa astrophysics data system (ads). since the ads is used by the entire astronomical community, these are indicative of how the astronomical literature is used. we will show how the use of the ads has changed both quantitatively and qualitatively. we will also show that different types of users access the system in different ways. finally, we show how use of the ads has evolved over the years in various regions of the world.   the ads is funded by nasa grant nng06gg68g.", "date_create": "2008-08-01", "area": "cs.dl astro-ph", "authors": ["henneken", "kurtz", "accomazzi", "grant", "thompson", "bohlen", "murray"]}, {"idpaper": "0808.1128", "title": "dynamic connectivity: connecting to networks and geometry", "abstract": "dynamic connectivity is a well-studied problem, but so far the most compelling progress has been confined to the edge-update model: maintain an understanding of connectivity in an undirected graph, subject to edge insertions and deletions. in this paper, we study two more challenging, yet equally fundamental problems.   subgraph connectivity asks to maintain an understanding of connectivity under vertex updates: updates can turn vertices on and off, and queries refer to the subgraph induced by \"on\" vertices. (for instance, this is closer to applications in networks of routers, where node faults may occur.)   we describe a data structure supporting vertex updates in o (m^{2/3}) amortized time, where m denotes the number of edges in the graph. this greatly improves over the previous result [chan, stoc'02], which required fast matrix multiplication and had an update time of o(m^0.94). the new data structure is also simpler.   geometric connectivity asks to maintain a dynamic set of n geometric objects, and query connectivity in their intersection graph. (for instance, the intersection graph of balls describes connectivity in a network of sensors with bounded transmission radius.)   previously, nontrivial fully dynamic results were known only for special cases like axis-parallel line segments and rectangles. we provide similarly improved update times, o (n^{2/3}), for these special cases. moreover, we show how to obtain sublinear update bounds for virtually all families of geometric objects which allow sublinear-time range queries, such as arbitrary 2d line segments, d-dimensional simplices, and d-dimensional balls.", "date_create": "2008-08-07", "area": "cs.ds cs.cg", "authors": ["chan", "patrascu", "roditty"]}, {"idpaper": "0808.2092", "title": "on zero-rate error exponent for bsc with noisy feedback", "abstract": "for the information transmission a binary symmetric channel is used. there is also another noisy binary symmetric channel (feedback channel), and the transmitter observes without delay all the outputs of the forward channel via that feedback channel. the transmission of a nonexponential number of messages (i.e. the transmission rate equals zero) is considered. the achievable decoding error exponent for such a combination of channels is investigated. it is shown that if the crossover probability of the feedback channel is less than a certain positive value, then the achievable error exponent is better than the similar error exponent of the no-feedback channel.   the transmission method described and the corresponding lower bound for the error exponent can be strengthened, and also extended to the positive transmission rates.", "date_create": "2008-08-15", "area": "cs.it math.it", "authors": ["burnashev", "yamamoto"]}, {"idpaper": "0808.3535", "title": "data diffusion: dynamic resource provision and data-aware scheduling for   data intensive applications", "abstract": "data intensive applications often involve the analysis of large datasets that require large amounts of compute and storage resources. while dedicated compute and/or storage farms offer good task/data throughput, they suffer low resource utilization problem under varying workloads conditions. if we instead move such data to distributed computing resources, then we incur expensive data transfer cost. in this paper, we propose a data diffusion approach that combines dynamic resource provisioning, on-demand data replication and caching, and data locality-aware scheduling to achieve improved resource efficiency under varying workloads. we define an abstract \"data diffusion model\" that takes into consideration the workload characteristics, data accessing cost, application throughput and resource utilization; we validate the model using a real-world large-scale astronomy application. our results show that data diffusion can increase the performance index by as much as 34x, and improve application response time by over 506x, while achieving near-optimal throughputs and execution times.", "date_create": "2008-08-26", "area": "cs.dc", "authors": ["raicu", "zhao", "foster", "szalay"]}, {"idpaper": "0809.3415", "title": "ten weeks in the life of an edonkey server", "abstract": "this paper presents a capture of the queries managed by an edonkey server during almost 10 weeks, leading to the observation of almost 9 billion messages involving almost 90 million users and more than 275 million distinct files. acquisition and management of such data raises several challenges, which we discuss as well as the solutions we developed. we obtain a very rich dataset, orders of magnitude larger than previously avalaible ones, which we provide for public use. we finally present basic analysis of the obtained data, which already gives evidence of non-trivial features.", "date_create": "2008-09-19", "area": "cs.ni", "authors": ["aidouni", "latapy", "magnien"]}, {"idpaper": "0809.3942", "title": "a reconfigurable programmable logic block for a multi-style asynchronous   fpga resistant to side-channel attacks", "abstract": "side-channel attacks are efficient attacks against cryptographic devices. they use only quantities observable from outside, such as the duration and the power consumption. attacks against synchronous devices using electric observations are facilitated by the fact that all transitions occur simultaneously with some global clock signal. asynchronous control remove this synchronization and therefore makes it more difficult for the attacker to insulate \\emph{interesting intervals}. in addition the coding of data in an asynchronous circuit is inherently more difficult to attack. this article describes the programmable logic block of an asynchronous fpga resistant against \\emph{side-channel attacks}. additionally it can implement different styles of asynchronous control and of data representation.", "date_create": "2008-09-23", "area": "cs.cr cs.oh", "authors": ["hoogvorst", "guilley", "chaudhuri", "danger", "beyrouthy", "fesquet"]}, {"idpaper": "0809.4882", "title": "multi-armed bandits in metric spaces", "abstract": "in a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. while the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. the goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. in this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a lipschitz condition with respect to the metric. we refer to this problem as the \"lipschitz mab problem\". we present a complete solution for the multi-armed problem in this setting. that is, for every metric space (l,x) we define an isometry invariant which bounds from below the performance of lipschitz mab algorithms for x, and we present an algorithm which comes arbitrarily close to meeting this bound. furthermore, our technique gives even better results for benign payoff functions.", "date_create": "2008-09-28", "area": "cs.ds cs.lg", "authors": ["kleinberg", "slivkins", "upfal"]}, {"idpaper": "0810.1858", "title": "sosemanuk: a fast software-oriented stream cipher", "abstract": "sosemanuk is a new synchronous software-oriented stream cipher, corresponding to profile 1 of the ecrypt call for stream cipher primitives. its key length is variable between 128 and 256 bits. it ac- commodates a 128-bit initial value. any key length is claimed to achieve 128-bit security. the sosemanuk cipher uses both some basic design principles from the stream cipher snow 2.0 and some transformations derived from the block cipher serpent. sosemanuk aims at improv- ing snow 2.0 both from the security and from the efficiency points of view. most notably, it uses a faster iv-setup procedure. it also requires a reduced amount of static data, yielding better performance on several architectures.", "date_create": "2008-10-10", "area": "cs.cr", "authors": ["berbain", "billet", "canteaut", "courtois", "gilbert", "goubin", "gouget", "granboulan", "lauradoux", "minier", "pornin", "sibert"]}, {"idpaper": "0810.3227", "title": "dynamic approaches to in-network aggregation", "abstract": "collaboration between small-scale wireless devices hinges on their ability to infer properties shared across multiple nearby nodes. wireless-enabled mobile devices in particular create a highly dynamic environment not conducive to distributed reasoning about such global properties. this paper addresses a specific instance of this problem: distributed aggregation. we present extensions to existing unstructured aggregation protocols that enable estimation of count, sum, and average aggregates in highly dynamic environments. with the modified protocols, devices with only limited connectivity can maintain estimates of the aggregate, despite \\textit{unexpected} peer departures and arrivals. our analysis of these aggregate maintenance extensions demonstrates their effectiveness in unstructured environments despite high levels of node mobility.", "date_create": "2008-10-17", "area": "cs.dc cs.db cs.ds", "authors": ["kennedy", "koch", "demers"]}, {"idpaper": "0810.3671", "title": "emergency centre organization and automated triage system", "abstract": "the excessive rate of patients arriving at accident and emergency centres is a major problem facing south african hospitals. patients are prioritized for medical care through a triage process. manual systems allow for inconsistency and error. this paper proposes a novel system to automate accident and emergency centre triage and uses this triage score along with an artificial intelligence estimate of patient-doctor time to optimize the queue order. a fuzzy inference system is employed to triage patients and a similar system estimates the time but adapts continuously through fuzzy q-learning. the optimal queue order is found using a novel procedure based on genetic algorithms. these components are integrated in a simple graphical user interface. live tests could not be performed but simulations reveal that the average waiting time can be reduced by 48 minutes and priority is given to urgent patients", "date_create": "2008-10-20", "area": "cs.cy", "authors": ["golding", "wilson", "marwala"]}, {"idpaper": "0810.3715", "title": "distributed estimation over wireless sensor networks with packet losses", "abstract": "a distributed adaptive algorithm to estimate a time-varying signal, measured by a wireless sensor network, is designed and analyzed. one of the major features of the algorithm is that no central coordination among the nodes needs to be assumed. the measurements taken by the nodes of the network are affected by noise, and the communication among the nodes is subject to packet losses. nodes exchange local estimates and measurements with neighboring nodes. each node of the network locally computes adaptive weights that minimize the estimation error variance. decentralized conditions on the weights, needed for the convergence of the estimation error throughout the overall network, are presented. a lipschitz optimization problem is posed to guarantee stability and the minimization of the variance. an efficient strategy to distribute the computation of the optimal solution is investigated. a theoretical performance analysis of the distributed algorithm is carried out both in the presence of perfect and lossy links. numerical simulations illustrate performance for various network topologies and packet loss probabilities.", "date_create": "2008-10-20", "area": "cs.dc", "authors": ["fischione", "speranzon", "johansson", "sangiovanni-vincentelli"]}, {"idpaper": "0810.4934", "title": "exponential-time approximation of hard problems", "abstract": "we study optimization problems that are neither approximable in polynomial time (at least with a constant factor) nor fixed parameter tractable, under widely believed complexity assumptions. specifically, we focus on maximum independent set, vertex coloring, set cover, and bandwidth.   in recent years, many researchers design exact exponential-time algorithms for these and other hard problems. the goal is getting the time complexity still of order $o(c^n)$, but with the constant $c$ as small as possible. in this work we extend this line of research and we investigate whether the constant $c$ can be made even smaller when one allows constant factor approximation. in fact, we describe a kind of approximation schemes -- trade-offs between approximation factor and the time complexity.   we study two natural approaches. the first approach consists of designing a backtracking algorithm with a small search tree. we present one result of that kind: a $(4r-1)$-approximation of bandwidth in time $o^*(2^{n/r})$, for any positive integer $r$.   the second approach uses general transformations from exponential-time exact algorithms to approximations that are faster but still exponential-time. for example, we show that for any reduction rate $r$, one can transform any $o^*(c^n)$-time algorithm for set cover into a $(1+\\ln r)$-approximation algorithm running in time $o^*(c^{n/r})$. we believe that results of that kind extend the applicability of exact algorithms for np-hard problems.", "date_create": "2008-10-27", "area": "cs.ds", "authors": ["cygan", "kowalik", "pilipczuk", "wykurz"]}, {"idpaper": "0811.2497", "title": "computing voting power in easy weighted voting games", "abstract": "weighted voting games are ubiquitous mathematical models which are used in economics, political science, neuroscience, threshold logic, reliability theory and distributed systems. they model situations where agents with variable voting weight vote in favour of or against a decision. a coalition of agents is winning if and only if the sum of weights of the coalition exceeds or equals a specified quota. the banzhaf index is a measure of voting power of an agent in a weighted voting game. it depends on the number of coalitions in which the agent is the difference in the coalition winning or losing. it is well known that computing banzhaf indices in a weighted voting game is np-hard. we give a comprehensive classification of weighted voting games which can be solved in polynomial time. among other results, we provide a polynomial ($o(k{(\\frac{n}{k})}^k)$) algorithm to compute the banzhaf indices in weighted voting games in which the number of weight values is bounded by $k$.", "date_create": "2008-11-15", "area": "cs.gt cs.cc cs.ds", "authors": ["aziz", "paterson"]}, {"idpaper": "0811.2572", "title": "an efficient algorithm for partial order production", "abstract": "we consider the problem of partial order production: arrange the elements of an unknown totally ordered set t into a target partially ordered set s, by comparing a minimum number of pairs in t. special cases include sorting by comparisons, selection, multiple selection, and heap construction.   we give an algorithm performing itlb + o(itlb) + o(n) comparisons in the worst case. here, n denotes the size of the ground sets, and itlb denotes a natural information-theoretic lower bound on the number of comparisons needed to produce the target partial order.   our approach is to replace the target partial order by a weak order (that is, a partial order with a layered structure) extending it, without increasing the information theoretic lower bound too much. we then solve the problem by applying an efficient multiple selection algorithm. the overall complexity of our algorithm is polynomial. this answers a question of yao (siam j. comput. 18, 1989).   we base our analysis on the entropy of the target partial order, a quantity that can be efficiently computed and provides a good estimate of the information-theoretic lower bound.", "date_create": "2008-11-17", "area": "cs.ds", "authors": ["cardinal", "fiorini", "joret", "jungers", "munro"]}, {"idpaper": "0811.3247", "title": "an experimental analysis of lemke-howson algorithm", "abstract": "we present an experimental investigation of the performance of the lemke-howson algorithm, which is the most widely used algorithm for the computation of a nash equilibrium for bimatrix games. lemke-howson algorithm is based upon a simple pivoting strategy, which corresponds to following a path whose endpoint is a nash equilibrium. we analyze both the basic lemke-howson algorithm and a heuristic modification of it, which we designed to cope with the effects of a 'bad' initial choice of the pivot. our experimental findings show that, on uniformly random games, the heuristics achieves a linear running time, while the basic lemke-howson algorithm runs in time roughly proportional to a polynomial of degree seven. to conduct the experiments, we have developed our own implementation of lemke-howson algorithm, which turns out to be significantly faster than state-of-the-art software. this allowed us to run the algorithm on a much larger set of data, and on instances of much larger size, compared with previous work.", "date_create": "2008-11-19", "area": "cs.ds cs.na", "authors": ["codenotti", "de rossi", "pagan"]}, {"idpaper": "0901.0291", "title": "an algorithm for file transfer scheduling in grid environments", "abstract": "this paper addresses the data transfer scheduling problem for grid environments, presenting a centralized scheduler developed with dynamic and adaptive features. the algorithm offers a reservation system for user transfer requests that allocates them transfer times and bandwidth, according to the network topology and the constraints the user specified for the requests. this paper presents the projects related to the data transfer field, the design of the framework for which the scheduler was built, the main features of the scheduler, the steps for transfer requests rescheduling and two tests that illustrate the system's behavior for different types of transfer requests.", "date_create": "2009-01-02", "area": "cs.ni cs.dc cs.ds", "authors": ["carpen-amarie", "andreica", "cristea"]}, {"idpaper": "0901.3657", "title": "homotopy methods for multiplication modulo triangular sets", "abstract": "we study the cost of multiplication modulo triangular families of polynomials. following previous work by li, moreno maza and schost, we propose an algorithm that relies on homotopy and fast evaluation-interpolation techniques. we obtain a quasi-linear time complexity for substantial families of examples, for which no such result was known before. applications are given to notably addition of algebraic numbers in small characteristic.", "date_create": "2009-01-23", "area": "cs.sc cs.ds", "authors": ["bostan", "chowdhury", "van der hoeven", "schost"]}, {"idpaper": "0901.3657", "title": "homotopy methods for multiplication modulo triangular sets", "abstract": "we study the cost of multiplication modulo triangular families of polynomials. following previous work by li, moreno maza and schost, we propose an algorithm that relies on homotopy and fast evaluation-interpolation techniques. we obtain a quasi-linear time complexity for substantial families of examples, for which no such result was known before. applications are given to notably addition of algebraic numbers in small characteristic.", "date_create": "2009-01-23", "area": "cs.sc cs.ds", "authors": ["bostan", "chowdhury", "van der hoeven", "schost"]}, {"idpaper": "0902.0744", "title": "embedding data within knowledge spaces", "abstract": "the promise of e-science will only be realized when data is discoverable, accessible, and comprehensible within distributed teams, across disciplines, and over the long-term--without reliance on out-of-band (non-digital) means. we have developed the open-source tupelo semantic content management framework and are employing it to manage a wide range of e-science entities (including data, documents, workflows, people, and projects) and a broad range of metadata (including provenance, social networks, geospatial relationships, temporal relations, and domain descriptions). tupelo couples the use of global identifiers and resource description framework (rdf) statements with an aggregatable content repository model to provide a unified space for securely managing distributed heterogeneous content and relationships.", "date_create": "2009-02-04", "area": "cs.ai cs.hc cs.ir", "authors": ["myers", "futrelle", "gaynor", "plutchak", "bajcsy", "kastner", "kotwani", "lee", "marini", "kooper", "mcgrath", "mclaren", "rodriguez", "liu"]}, {"idpaper": "0902.0744", "title": "embedding data within knowledge spaces", "abstract": "the promise of e-science will only be realized when data is discoverable, accessible, and comprehensible within distributed teams, across disciplines, and over the long-term--without reliance on out-of-band (non-digital) means. we have developed the open-source tupelo semantic content management framework and are employing it to manage a wide range of e-science entities (including data, documents, workflows, people, and projects) and a broad range of metadata (including provenance, social networks, geospatial relationships, temporal relations, and domain descriptions). tupelo couples the use of global identifiers and resource description framework (rdf) statements with an aggregatable content repository model to provide a unified space for securely managing distributed heterogeneous content and relationships.", "date_create": "2009-02-04", "area": "cs.ai cs.hc cs.ir", "authors": ["myers", "futrelle", "gaynor", "plutchak", "bajcsy", "kastner", "kotwani", "lee", "marini", "kooper", "mcgrath", "mclaren", "rodriguez", "liu"]}, {"idpaper": "0902.3286", "title": "mds codes on the erasure-erasure wiretap channel", "abstract": "this paper considers the problem of perfectly secure communication on a modified version of wyner's wiretap channel ii where both the main and wiretapper's channels have some erasures. a secret message is to be encoded into $n$ channel symbols and transmitted. the main channel is such that the legitimate receiver receives the transmitted codeword with exactly $n - \\nu$ erasures, where the positions of the erasures are random. additionally, an eavesdropper (wire-tapper) is able to observe the transmitted codeword with $n - \\mu$ erasures in a similar fashion. this paper studies the maximum achievable information rate with perfect secrecy on this channel and gives a coding scheme using nested codes that achieves the secrecy capacity.", "date_create": "2009-02-18", "area": "cs.it math.it", "authors": ["subramanian", "mclaughlin"]}, {"idpaper": "0902.4098", "title": "coordination in multiagent systems and laplacian spectra of digraphs", "abstract": "constructing and studying distributed control systems requires the analysis of the laplacian spectra and the forest structure of directed graphs. in this paper, we present some basic results of this analysis partially obtained by the present authors. we also discuss the application of these results to decentralized control and touch upon some problems of spectral graph theory.", "date_create": "2009-02-24", "area": "cs.ma cs.dm math.co math.oc", "authors": ["chebotarev", "agaev"]}, {"idpaper": "0904.0589", "title": "fuzzy linguistic logic programming and its applications", "abstract": "the paper introduces fuzzy linguistic logic programming, which is a combination of fuzzy logic programming, introduced by p. vojtas, and hedge algebras in order to facilitate the representation and reasoning on human knowledge expressed in natural languages. in fuzzy linguistic logic programming, truth values are linguistic ones, e.g., verytrue, veryprobablytrue, and littlefalse, taken from a hedge algebra of a linguistic truth variable, and linguistic hedges (modifiers) can be used as unary connectives in formulae. this is motivated by the fact that humans reason mostly in terms of linguistic terms rather than in terms of numbers, and linguistic hedges are often used in natural languages to express different levels of emphasis. the paper presents: (i) the language of fuzzy linguistic logic programming; (ii) a declarative semantics in terms of herbrand interpretations and models; (iii) a procedural semantics which directly manipulates linguistic terms to compute a lower bound to the truth value of a query, and proves its soundness; (iv) a fixpoint semantics of logic programs, and based on it, proves the completeness of the procedural semantics; (v) several applications of fuzzy linguistic logic programming; and (vi) an idea of implementing a system to execute fuzzy linguistic logic programs.", "date_create": "2009-04-03", "area": "cs.lo", "authors": ["le", "liu", "tran"]}, {"idpaper": "0904.2160", "title": "inferring dynamic bayesian networks using frequent episode mining", "abstract": "motivation: several different threads of research have been proposed for modeling and mining temporal data. on the one hand, approaches such as dynamic bayesian networks (dbns) provide a formal probabilistic basis to model relationships between time-indexed random variables but these models are intractable to learn in the general case. on the other, algorithms such as frequent episode mining are scalable to large datasets but do not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature.   results: we present a unification of these two seemingly diverse threads of research, by demonstrating how dynamic (discrete) bayesian networks can be inferred from the results of frequent episode mining. this helps bridge the modeling emphasis of the former with the counting emphasis of the latter. first, we show how, under reasonable assumptions on data characteristics and on influences of random variables, the optimal dbn structure can be computed using a greedy, local, algorithm. next, we connect the optimality of the dbn structure with the notion of fixed-delay episodes and their counts of distinct occurrences. finally, to demonstrate the practical feasibility of our approach, we focus on a specific (but broadly applicable) class of networks, called excitatory networks, and show how the search for the optimal dbn structure can be conducted using just information from frequent episodes. application on datasets gathered from mathematical models of spiking neurons as well as real neuroscience datasets are presented.   availability: algorithmic implementations, simulator codebases, and datasets are available from our website at http://neural-code.cs.vt.edu/dbn", "date_create": "2009-04-14", "area": "cs.lg", "authors": ["patnaik", "laxman", "ramakrishnan"]}, {"idpaper": "0905.3830", "title": "tag clouds for displaying semantics: the case of filmscripts", "abstract": "we relate tag clouds to other forms of visualization, including planar or reduced dimensionality mapping, and kohonen self-organizing maps. using a modified tag cloud visualization, we incorporate other information into it, including text sequence and most pertinent words. our notion of word pertinence goes beyond just word frequency and instead takes a word in a mathematical sense as located at the average of all of its pairwise relationships. we capture semantics through context, taken as all pairwise relationships. our domain of application is that of filmscript analysis. the analysis of filmscripts, always important for cinema, is experiencing a major gain in importance in the context of television. our objective in this work is to visualize the semantics of filmscript, and beyond filmscript any other partially structured, time-ordered, sequence of text segments. in particular we develop an innovative approach to plot characterization.", "date_create": "2009-05-23", "area": "cs.ai", "authors": ["murtagh", "ganz", "mckie", "mothe", "englmeier"]}, {"idpaper": "0905.4325", "title": "updating quantum cryptography report ver. 1", "abstract": "quantum cryptographic technology (qct) is expected to be a fundamental technology for realizing long-term information security even against as-yet-unknown future technologies. more advanced security could be achieved using qct together with contemporary cryptographic technologies. to develop and spread the use of qct, it is necessary to standardize devices, protocols, and security requirements and thus enable interoperability in a multi-vendor, multi-network, and multi-service environment. this report is a technical summary of qct and related topics from the viewpoints of 1) consensual establishment of specifications and requirements of qct for standardization and commercialization and 2) the promotion of research and design to realize new-generation quantum cryptography.", "date_create": "2009-05-27", "area": "quant-ph cs.cr", "authors": ["dodson", "fujiwara", "grangier", "hayashi", "imafuku", "kitayama", "kumar", "kurtsiefer", "lenhart", "luetkenhaus", "matsumoto", "munro", "nishioka", "peev", "sasaki", "sata", "takada", "takeoka", "tamaki", "tanaka", "tokura", "tomita", "toyoshima", "van meter", "yamagishi", "yamamoto", "yamamura"]}, {"idpaper": "0905.4325", "title": "updating quantum cryptography report ver. 1", "abstract": "quantum cryptographic technology (qct) is expected to be a fundamental technology for realizing long-term information security even against as-yet-unknown future technologies. more advanced security could be achieved using qct together with contemporary cryptographic technologies. to develop and spread the use of qct, it is necessary to standardize devices, protocols, and security requirements and thus enable interoperability in a multi-vendor, multi-network, and multi-service environment. this report is a technical summary of qct and related topics from the viewpoints of 1) consensual establishment of specifications and requirements of qct for standardization and commercialization and 2) the promotion of research and design to realize new-generation quantum cryptography.", "date_create": "2009-05-27", "area": "quant-ph cs.cr", "authors": ["dodson", "fujiwara", "grangier", "hayashi", "imafuku", "kitayama", "kumar", "kurtsiefer", "lenhart", "luetkenhaus", "matsumoto", "munro", "nishioka", "peev", "sasaki", "sata", "takada", "takeoka", "tamaki", "tanaka", "tokura", "tomita", "toyoshima", "van meter", "yamagishi", "yamamoto", "yamamura"]}, {"idpaper": "0905.4325", "title": "updating quantum cryptography report ver. 1", "abstract": "quantum cryptographic technology (qct) is expected to be a fundamental technology for realizing long-term information security even against as-yet-unknown future technologies. more advanced security could be achieved using qct together with contemporary cryptographic technologies. to develop and spread the use of qct, it is necessary to standardize devices, protocols, and security requirements and thus enable interoperability in a multi-vendor, multi-network, and multi-service environment. this report is a technical summary of qct and related topics from the viewpoints of 1) consensual establishment of specifications and requirements of qct for standardization and commercialization and 2) the promotion of research and design to realize new-generation quantum cryptography.", "date_create": "2009-05-27", "area": "quant-ph cs.cr", "authors": ["dodson", "fujiwara", "grangier", "hayashi", "imafuku", "kitayama", "kumar", "kurtsiefer", "lenhart", "luetkenhaus", "matsumoto", "munro", "nishioka", "peev", "sasaki", "sata", "takada", "takeoka", "tamaki", "tanaka", "tokura", "tomita", "toyoshima", "van meter", "yamagishi", "yamamoto", "yamamura"]}, {"idpaper": "0906.1763", "title": "segmentation of facial expressions using semi-definite programming and   generalized principal component analysis", "abstract": "in this paper, we use semi-definite programming and generalized principal component analysis (gpca) to distinguish between two or more different facial expressions. in the first step, semi-definite programming is used to reduce the dimension of the image data and \"unfold\" the manifold which the data points (corresponding to facial expressions) reside on. next, gpca is used to fit a series of subspaces to the data points and associate each data point with a subspace. data points that belong to the same subspace are claimed to belong to the same facial expression category. an example is provided.", "date_create": "2009-06-09", "area": "cs.cv", "authors": ["gholami", "tannenbaum", "haddad"]}, {"idpaper": "0906.2143", "title": "dependable distributed computing for the international telecommunication   union regional radio conference rrc06", "abstract": "the international telecommunication union (itu) regional radio conference (rrc06) established in 2006 a new frequency plan for the introduction of digital broadcasting in european, african, arab, cis countries and iran. the preparation of the plan involved complex calculations under short deadline and required dependable and efficient computing capability. the itu designed and deployed in-situ a dedicated pc farm, in parallel to the european organization for nuclear research (cern) which provided and supported a system based on the egee grid. the planning cycle at the rrc06 required a periodic execution in the order of 200,000 short jobs, using several hundreds of cpu hours, in a period of less than 12 hours. the nature of the problem required dynamic workload-balancing and low-latency access to the computing resources. we present the strategy and key technical choices that delivered a reliable service to the rrc06.", "date_create": "2009-06-11", "area": "cs.dc", "authors": ["moscicki", "manara", "lamanna", "mendez", "muraru"]}, {"idpaper": "0906.3162", "title": "are stable instances easy?", "abstract": "we introduce the notion of a stable instance for a discrete optimization problem, and argue that in many practical situations only sufficiently stable instances are of interest. the question then arises whether stable instances of np--hard problems are easier to solve. in particular, whether there exist algorithms that solve correctly and in polynomial time all sufficiently stable instances of some np--hard problem. the paper focuses on the max--cut problem, for which we show that this is indeed the case.", "date_create": "2009-06-17", "area": "cs.cc", "authors": ["bilu", "linial"]}, {"idpaper": "0906.3772", "title": "xml data integrity based on concatenated hash function", "abstract": "data integrity is the fundamental for data authentication. a major problem for xml data authentication is that signed xml data can be copied to another document but still keep signature valid. this is caused by xml data integrity protecting. through investigation, the paper discovered that besides data content integrity, xml data integrity should also protect element location information, and context referential integrity under fine-grained security situation. the aim of this paper is to propose a model for xml data integrity considering xml data features. the paper presents an xml data integrity model named as csr (content integrity, structure integrity, context referential integrity) based on a concatenated hash function. xml data content integrity is ensured using an iterative hash process, structure integrity is protected by hashing an absolute path string from root node, and context referential integrity is ensured by protecting context-related elements. presented xml data integrity model can satisfy integrity requirements under situation of fine-grained security, and compatible with xml signature. through evaluation, the integrity model presented has a higher efficiency on digest value-generation than the merkle hash tree-based integrity model for xml data.", "date_create": "2009-06-19", "area": "cs.se cs.pl", "authors": ["liu", "lu", "yip"]}, {"idpaper": "0906.5393", "title": "measurable & scalable nfrs using fuzzy logic and likert scale", "abstract": "most of the research related to non functional requirements (nfrs) have presented nfrs frameworks by integrating non functional requirements with functional requirements while we proposed that measurement of nfrs is possible e.g. cost and performance and nfr like usability can be scaled. our novel hybrid approach integrates three things rather than two i.e. functional requirements (frs), measurable nfrs (m-nfrs) and scalable nfrs (s-nfrs). we have also found the use of fuzzy logic and likert scale effective for handling of discretely measurable as well as scalable nfrs as these techniques can provide a simple way to arrive at a discrete or scalable nfr in contrast to vague, ambiguous, imprecise, noisy or missing nfr. our approach can act as baseline for new nfr and aspect oriented frameworks by using all types of uml diagrams.", "date_create": "2009-06-29", "area": "cs.se", "authors": ["malik", "mushtaq", "khalid", "khalil", "malik"]}, {"idpaper": "0907.2157", "title": "on the maximal number of highly periodic runs in a string", "abstract": "a run is a maximal occurrence of a repetition $v$ with a period $p$ such that $2p \\le |v|$. the maximal number of runs in a string of length $n$ was studied by several authors and it is known to be between $0.944 n$ and $1.029 n$. we investigate highly periodic runs, in which the shortest period $p$ satisfies $3p \\le |v|$. we show the upper bound $0.5n$ on the maximal number of such runs in a string of length $n$ and construct a sequence of words for which we obtain the lower bound $0.406 n$.", "date_create": "2009-07-13", "area": "cs.ds cs.dm", "authors": ["crochemore", "iliopoulos", "kubica", "radoszewski", "rytter", "walen"]}, {"idpaper": "0908.1190", "title": "error estimation in large spreadsheets using bayesian statistics", "abstract": "spreadsheets are ubiquitous in business with the financial sector particularly heavily reliant on the technology. it is known that the level of spreadsheet error can be high and that it is often necessary to review spreadsheets based on a structured methodology which includes a cell by cell examination of the spreadsheet. this paper outlines the early research that has been carried out into the use of bayesian statistical methods to estimate the level of error in large spreadsheets during cell be cell examination based on expert knowledge and partial spreadsheet test data. the estimate can aid in the decision as to the quality of the spreadsheet and the necessity to conduct further testing or not.", "date_create": "2009-08-08", "area": "cs.se cs.hc", "authors": ["bradley", "mcdaid"]}, {"idpaper": "0908.2656", "title": "semantic robot vision challenge: current state and future directions", "abstract": "the semantic robot vision competition provided an excellent opportunity for our research lab to integrate our many ideas under one umbrella, inspiring both collaboration and new research. the task, visual search for an unknown object, is relevant to both the vision and robotics communities. moreover, since the interplay of robotics and vision is sometimes ignored, the competition provides a venue to integrate two communities. in this paper, we outline a number of modifications to the competition to both improve the state-of-the-art and increase participation.", "date_create": "2009-08-18", "area": "cs.cv cs.ro", "authors": ["helmer", "meger", "viswanathan", "mccann", "dockrey", "fazli", "southey", "muja", "joya", "little", "lowe", "mackworth"]}, {"idpaper": "0909.1786", "title": "dbmss should talk back too", "abstract": "natural language user interfaces to database systems have been studied for several decades now. they have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. we envision the reverse functionality, where the system would be able to take the internal result of that translation, say in sql form, translate it back into natural language, and show it to the initiator of the query for verification. likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. validation of the records stored with a backward translation into text would again be very powerful. verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. the problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. reality teaches us quite the opposite, however, as the resulting text should be expressive, i.e., accurate in capturing the underlying queries or data, and effective, i.e., allowing fast and unique interpretation of them. achieving both of these qualities is very difficult and raises several technical challenges that need to be addressed. in this paper, we first expose the reader to several situations and applications that need translation into natural language, thereby, motivating the problem. we then outline, by example, the research problems that need to be solved, separately for data translations and query translations.", "date_create": "2009-09-09", "area": "cs.db cs.hc", "authors": ["simitsis", "ioannidis"]}, {"idpaper": "0909.2055", "title": "business in the grid", "abstract": "from 2004 to 2007 the business in the grid (big) project took place and was driven by the following goals: firstly, make business aware of grid technology and, secondly, try to explore new business models. we disseminated grid computing by mainly concentrating on the central european market and interviewed several companies in order to gain insights into the grid acceptance in industrial environments. in this article we present the results of the project, elaborate on a critical discussion on business adaptations, and describe a novel dynamic authorization workflow for business processes in the grid.", "date_create": "2009-09-10", "area": "cs.dc cs.cy", "authors": ["schikuta", "weishaeupl", "donno", "stockinger", "vinek", "wanek", "witzany", "haq"]}, {"idpaper": "0909.3392", "title": "on the communication complexity of xor functions", "abstract": "an xor function is a function of the form g(x,y) = f(x + y), for some boolean function f on n bits. we study the quantum and classical communication complexity of xor functions. in the case of exact protocols, we completely characterise one-way communication complexity for all f. we also show that, when f is monotone, g's quantum and classical complexities are quadratically related, and that when f is a linear threshold function, g's quantum complexity is theta(n). more generally, we make a structural conjecture about the fourier spectra of boolean functions which, if true, would imply that the quantum and classical exact communication complexities of all xor functions are asymptotically equivalent. we give two randomised classical protocols for general xor functions which are efficient for certain functions, and a third protocol for linear threshold functions with high margin. these protocols operate in the symmetric message passing model with shared randomness.", "date_create": "2009-09-18", "area": "cs.cc quant-ph", "authors": ["montanaro", "osborne"]}, {"idpaper": "0909.4786", "title": "worldwide use and impact of the nasa astrophysics data system digital   library", "abstract": "by combining data from the text, citation, and reference databases with data from the ads readership logs we have been able to create second order bibliometric operators, a customizable class of collaborative filters which permits substantially improved accuracy in literature queries.   using the ads usage logs along with membership statistics from the international astronomical union and data on the population and gross domestic product (gdp) we develop an accurate model for world-wide basic research where the number of scientists in a country is proportional to the gdp of that country, and the amount of basic research done by a country is proportional to the number of scientists in that country times that country's per capita gdp.   we introduce the concept of utility time to measure the impact of the ads/urania and the electronic astronomical library on astronomical research. we find that in 2002 it amounted to the equivalent of 736 fte researchers, or $250 million, or the astronomical research done in france.   subject headings: digital libraries; bibliometrics; sociology of science; information retrieval", "date_create": "2009-09-25", "area": "cs.dl physics.soc-ph", "authors": ["kurtz", "eichhorn", "accomazzi", "grant", "demleitner", "murray"]}, {"idpaper": "0909.4789", "title": "the bibliometric properties of article readership information", "abstract": "the nasa astrophysics data system (ads), along with astronomy's journals and data centers (a collaboration dubbed urania), has developed a distributed on-line digital library which has become the dominant means by which astronomers search, access and read their technical literature. digital libraries such as the nasa astrophysics data system permit the easy accumulation of a new type of bibliometric measure, the number of electronic accesses (``reads'') of individual articles. we explore various aspects of this new measure. we examine the obsolescence function as measured by actual reads, and show that it can be well fit by the sum of four exponentials with very different time constants. we compare the obsolescence function as measured by readership with the obsolescence function as measured by citations. we find that the citation function is proportional to the sum of two of the components of the readership function. this proves that the normative theory of citation is true in the mean. we further examine in detail the similarities and differences between the citation rate, the readership rate and the total citations for individual articles, and discuss some of the causes. using the number of reads as a bibliometric measure for individuals, we introduce the read-cite diagram to provide a two-dimensional view of an individual's scientific productivity. we develop a simple model to account for an individual's reads and cites and use it to show that the position of a person in the read-cite diagram is a function of age, innate productivity, and work history. we show the age biases of both reads and cites, and develop two new bibliometric measures which have substantially less age bias than citations", "date_create": "2009-09-25", "area": "cs.dl physics.soc-ph", "authors": ["kurtz", "eichhorn", "accomazzi", "grant", "demleitner", "murray", "martimbeau", "elwell"]}, {"idpaper": "0910.1219", "title": "on the interpretation of delays in delay stochastic simulation of   biological systems", "abstract": "delays in biological systems may be used to model events for which the underlying dynamics cannot be precisely observed. mathematical modeling of biological systems with delays is usually based on delay differential equations (ddes), a kind of differential equations in which the derivative of the unknown function at a certain time is given in terms of the values of the function at previous times. in the literature, delay stochastic simulation algorithms have been proposed. these algorithms follow a \"delay as duration\" approach, namely they are based on an interpretation of a delay as the elapsing time between the start and the termination of a chemical reaction. this interpretation is not suitable for some classes of biological systems in which species involved in a delayed interaction can be involved at the same time in other interactions. we show on a dde model of tumor growth that the delay as duration approach for stochastic simulation is not precise, and we propose a simulation algorithm based on a ``purely delayed'' interpretation of delays which provides better results on the considered model.", "date_create": "2009-10-07", "area": "q-bio.qm cs.ce", "authors": ["barbuti", "caravagna", "milazzo", "maggiolo-schettini"]}, {"idpaper": "0910.1643", "title": "covering points by disjoint boxes with outliers", "abstract": "for a set of n points in the plane, we consider the axis--aligned (p,k)-box covering problem: find p axis-aligned, pairwise-disjoint boxes that together contain n-k points. in this paper, we consider the boxes to be either squares or rectangles, and we want to minimize the area of the largest box. for general p we show that the problem is np-hard for both squares and rectangles. for a small, fixed number p, we give algorithms that find the solution in the following running times:   for squares we have o(n+k log k) time for p=1, and o(n log n+k^p log^p k time for p = 2,3. for rectangles we get o(n + k^3) for p = 1 and o(n log n+k^{2+p} log^{p-1} k) time for p = 2,3.   in all cases, our algorithms use o(n) space.", "date_create": "2009-10-08", "area": "cs.cg cs.ds", "authors": ["ahn", "bae", "demaine", "demaine", "kim", "korman", "reinbacher", "son"]}, {"idpaper": "0910.2324", "title": "accelerating the execution of matrix languages on the cell broadband   engine architecture", "abstract": "matrix languages, including matlab and octave, are established standards for applications in science and engineering. they provide interactive programming environments that are easy to use due to their scripting languages with matrix data types. current implementations of matrix languages do not fully utilise high-performance, special-purpose chip architectures such as the ibm powerxcell processor (cell), which is currently used in the fastest computer in the world.   we present a new framework that extends octave to harness the computational power of the cell. with this framework the programmer is relieved of the burden of introducing explicit notions of parallelism. instead the programmer uses a new matrix data-type to execute matrix operations in parallel on the synergistic processing elements (spes) of the cell. we employ lazy evaluation semantics for our new matrix data-type to obtain execution traces of matrix operations. traces are converted to data dependence graphs; operations in the data dependence graph are lowered (split into sub-matrices), scheduled and executed on the spes. thereby we exploit (1) data parallelism, (2) instruction level parallelism, (3) pipeline parallelism and (4) task parallelism of matrix language programs. we conducted extensive experiments to show the validity of our approach. our cell-based implementation achieves speedups of up to a factor of 12 over code run on recent intel core2 quad processors.", "date_create": "2009-10-13", "area": "cs.pl cs.dc", "authors": ["khoury", "burgstaller", "scholz"]}, {"idpaper": "0910.3529", "title": "citation statistics", "abstract": "this is a report about the use and misuse of citation data in the assessment of scientific research. the idea that research assessment must be done using ``simple and objective'' methods is increasingly prevalent today. the ``simple and objective'' methods are broadly interpreted as bibliometrics, that is, citation data and the statistics derived from them. there is a belief that citation statistics are inherently more accurate because they substitute simple numbers for complex judgments, and hence overcome the possible subjectivity of peer review. but this belief is unfounded.", "date_create": "2009-10-19", "area": "stat.me cs.dl physics.soc-ph", "authors": ["adler", "ewing", "taylor"]}, {"idpaper": "0910.3529", "title": "citation statistics", "abstract": "this is a report about the use and misuse of citation data in the assessment of scientific research. the idea that research assessment must be done using ``simple and objective'' methods is increasingly prevalent today. the ``simple and objective'' methods are broadly interpreted as bibliometrics, that is, citation data and the statistics derived from them. there is a belief that citation statistics are inherently more accurate because they substitute simple numbers for complex judgments, and hence overcome the possible subjectivity of peer review. but this belief is unfounded.", "date_create": "2009-10-19", "area": "stat.me cs.dl physics.soc-ph", "authors": ["adler", "ewing", "taylor"]}, {"idpaper": "0910.5920", "title": "evaluating trust in grid certificates", "abstract": "digital certificates are used to secure international computation and data storage grids used for e-science projects, like the worldwide large hadron collider computing grid. the international grid trust federation has defined the grid certificate profile: a set of guidelines for digital certificates used for grid authentication. we have designed and implemented a program and related test suites for checking x.509 certificates against the certificate profiles and policies relevant for use on the grid. the result is a practical tool that assists implementers and users of public key infrastructures to reach appropriate trust decisions.", "date_create": "2009-10-30", "area": "cs.cr cs.dc", "authors": ["o'callaghan", "doran", "coghlan"]}, {"idpaper": "0910.5920", "title": "evaluating trust in grid certificates", "abstract": "digital certificates are used to secure international computation and data storage grids used for e-science projects, like the worldwide large hadron collider computing grid. the international grid trust federation has defined the grid certificate profile: a set of guidelines for digital certificates used for grid authentication. we have designed and implemented a program and related test suites for checking x.509 certificates against the certificate profiles and policies relevant for use on the grid. the result is a practical tool that assists implementers and users of public key infrastructures to reach appropriate trust decisions.", "date_create": "2009-10-30", "area": "cs.cr cs.dc", "authors": ["o'callaghan", "doran", "coghlan"]}, {"idpaper": "0911.1504", "title": "throughput limits of ieee 802.11 and ieee 802.15.3", "abstract": "ieee 802.11 and ieee 802.15.3 are wireless standards originally designed for wireless local area network (wlan) and wireless personal area network (wpan). this paper studies mac throughput analysis of both standards. we present a comparative analysis of both standards in terms of mac throughput and bandwidth efficiency. numerical results show that the performance of ieee 802.15.3 transcends ieee 802.11 in all cases.", "date_create": "2009-11-08", "area": "cs.ni cs.pf", "authors": ["ullah", "zhong", "islam", "nessa", "kwak"]}, {"idpaper": "0911.2174", "title": "qpace -- a qcd parallel computer based on cell processors", "abstract": "qpace is a novel parallel computer which has been developed to be primarily used for lattice qcd simulations. the compute power is provided by the ibm powerxcell 8i processor, an enhanced version of the cell processor that is used in the playstation 3. the qpace nodes are interconnected by a custom, application optimized 3-dimensional torus network implemented on an fpga. to achieve the very high packaging density of 26 tflops per rack a new water cooling concept has been developed and successfully realized. in this paper we give an overview of the architecture and highlight some important technical details of the system. furthermore, we provide initial performance results and report on the installation of 8 qpace racks providing an aggregate peak performance of 200 tflops.", "date_create": "2009-11-11", "area": "hep-lat cs.ar", "authors": ["baier", "boettiger", "drochner", "eicker", "fischer", "fodor", "frommer", "gomez", "goldrian", "heybrock", "hierl", "h\u00fcsken", "huth", "krill", "lauritsen", "lippert", "maurer", "mendl", "meyer", "nobile", "ouda", "pivanti", "pleiter", "ries", "sch\u00e4fer", "schick", "schifano", "simma", "solbrig", "streuer", "sulanke", "tripiccione", "vogt", "wettig", "winter"]}, {"idpaper": "0911.2174", "title": "qpace -- a qcd parallel computer based on cell processors", "abstract": "qpace is a novel parallel computer which has been developed to be primarily used for lattice qcd simulations. the compute power is provided by the ibm powerxcell 8i processor, an enhanced version of the cell processor that is used in the playstation 3. the qpace nodes are interconnected by a custom, application optimized 3-dimensional torus network implemented on an fpga. to achieve the very high packaging density of 26 tflops per rack a new water cooling concept has been developed and successfully realized. in this paper we give an overview of the architecture and highlight some important technical details of the system. furthermore, we provide initial performance results and report on the installation of 8 qpace racks providing an aggregate peak performance of 200 tflops.", "date_create": "2009-11-11", "area": "hep-lat cs.ar", "authors": ["baier", "boettiger", "drochner", "eicker", "fischer", "fodor", "frommer", "gomez", "goldrian", "heybrock", "hierl", "h\u00fcsken", "huth", "krill", "lauritsen", "lippert", "maurer", "mendl", "meyer", "nobile", "ouda", "pivanti", "pleiter", "ries", "sch\u00e4fer", "schick", "schifano", "simma", "solbrig", "streuer", "sulanke", "tripiccione", "vogt", "wettig", "winter"]}, {"idpaper": "0911.2174", "title": "qpace -- a qcd parallel computer based on cell processors", "abstract": "qpace is a novel parallel computer which has been developed to be primarily used for lattice qcd simulations. the compute power is provided by the ibm powerxcell 8i processor, an enhanced version of the cell processor that is used in the playstation 3. the qpace nodes are interconnected by a custom, application optimized 3-dimensional torus network implemented on an fpga. to achieve the very high packaging density of 26 tflops per rack a new water cooling concept has been developed and successfully realized. in this paper we give an overview of the architecture and highlight some important technical details of the system. furthermore, we provide initial performance results and report on the installation of 8 qpace racks providing an aggregate peak performance of 200 tflops.", "date_create": "2009-11-11", "area": "hep-lat cs.ar", "authors": ["baier", "boettiger", "drochner", "eicker", "fischer", "fodor", "frommer", "gomez", "goldrian", "heybrock", "hierl", "h\u00fcsken", "huth", "krill", "lauritsen", "lippert", "maurer", "mendl", "meyer", "nobile", "ouda", "pivanti", "pleiter", "ries", "sch\u00e4fer", "schick", "schifano", "simma", "solbrig", "streuer", "sulanke", "tripiccione", "vogt", "wettig", "winter"]}, {"idpaper": "0911.2174", "title": "qpace -- a qcd parallel computer based on cell processors", "abstract": "qpace is a novel parallel computer which has been developed to be primarily used for lattice qcd simulations. the compute power is provided by the ibm powerxcell 8i processor, an enhanced version of the cell processor that is used in the playstation 3. the qpace nodes are interconnected by a custom, application optimized 3-dimensional torus network implemented on an fpga. to achieve the very high packaging density of 26 tflops per rack a new water cooling concept has been developed and successfully realized. in this paper we give an overview of the architecture and highlight some important technical details of the system. furthermore, we provide initial performance results and report on the installation of 8 qpace racks providing an aggregate peak performance of 200 tflops.", "date_create": "2009-11-11", "area": "hep-lat cs.ar", "authors": ["baier", "boettiger", "drochner", "eicker", "fischer", "fodor", "frommer", "gomez", "goldrian", "heybrock", "hierl", "h\u00fcsken", "huth", "krill", "lauritsen", "lippert", "maurer", "mendl", "meyer", "nobile", "ouda", "pivanti", "pleiter", "ries", "sch\u00e4fer", "schick", "schifano", "simma", "solbrig", "streuer", "sulanke", "tripiccione", "vogt", "wettig", "winter"]}, {"idpaper": "0911.4642", "title": "g3 : genesis software envrionment update", "abstract": "genesis3 is the new version of the genesis software environment for musical creation by means of mass-interaction physics network modeling. it was designed, and developed from scratch, in hindsight of more than 10 years working on and using the previous version. we take the opportunity of this birth to provide in this article (1) an analysis of the peculiarities in genesis, aiming at highlighting its core ?software paradigm?; and (2) an update on the features of the new version as compared to the last.", "date_create": "2009-11-24", "area": "cs.sd cs.hc cs.mm cs.se", "authors": ["castagn\u00e9", "cadoz", "allaoui", "tache"]}, {"idpaper": "0912.1050", "title": "abstract milling with turn costs", "abstract": "the abstract milling problem is a natural and quite general graph-theoretic model for geometric milling problems. given a graph, one asks for a walk that covers all its vertices with a minimum number of turns, as specified in the graph model by a 0/1 turncost function fx at each vertex x giving, for each ordered pair of edges (e,f) incident at x, the turn cost at x of a walk that enters the vertex on edge e and departs on edge f. we describe an initial study of the parameterized complexity of the problem. our main positive result shows that abstract milling, parameterized by: number of turns, treewidth and maximum degree, is fixed-parameter tractable, we also show that abstract milling parameterized by (only) the number of turns and the pathwidth, is hard for w[1] -- one of the few parameterized intractability results for bounded pathwidth.", "date_create": "2009-12-05", "area": "cs.cc cs.cg cs.ds cs.lo", "authors": ["fellows", "giannopoulos", "knauer", "paul", "rosamond", "whitesides", "yu"]}, {"idpaper": "1001.1970", "title": "a framework for validation of object oriented design metrics", "abstract": "a large number of metrics have been proposed for the quality of object oriented software. many of these metrics have not been properly validated due to poor methods of validation and non acceptance of metrics on scientific grounds. in the literature, two types of validations namely internal (theoretical) and external (empirical) are recommended. in this study, the authors have used both theoretical as well as empirical validation for validating already proposed set of metrics for the five quality factors. these metrics were proposed by kumar and soni.", "date_create": "2010-01-12", "area": "cs.se", "authors": ["soni", "shrivastava", "kumar"]}, {"idpaper": "1001.2249", "title": "on the efficiency of fast rsa variants in modern mobile phones", "abstract": "modern mobile phones are increasingly being used for more services that require modern security mechanisms such as the public key cryptosystem rsa. it is, however, well known that public key cryptography demands considerable computing resources and that rsa encryption is much faster than rsa decryption. it is consequently an interesting question if rsa as a whole can be executed efficiently on modern mobile phones. in this paper, we explore the efficiency on modern mobile phones of variants of the rsa cryptosystem, covering crt, multiprime rsa, multipower rsa, rebalanced rsa and r prime rsa by comparing the encryption and decryption time using a simple java implementation and a typical rsa setup.", "date_create": "2010-01-13", "area": "cs.cr", "authors": ["hansen", "larsen", "olsen"]}, {"idpaper": "1001.4438", "title": "principal typings in a restricted intersection type system for beta   normal forms with de bruijn indices", "abstract": "the lambda-calculus with de bruijn indices assembles each alpha-class of lambda-terms in a unique term, using indices instead of variable names. intersection types provide finitary type polymorphism and can characterise normalisable lambda-terms through the property that a term is normalisable if and only if it is typeable. to be closer to computations and to simplify the formalisation of the atomic operations involved in beta-contractions, several calculi of explicit substitution were developed mostly with de bruijn indices. versions of explicit substitutions calculi without types and with simple type systems are well investigated in contrast to versions with more elaborate type systems such as intersection types. in previous work, we introduced a de bruijn version of the lambda-calculus with an intersection type system and proved that it preserves subject reduction, a basic property of type systems. in this paper a version with de bruijn indices of an intersection type system originally introduced to characterise principal typings for beta-normal forms is presented. we present the characterisation in this new system and the corresponding versions for the type inference and the reconstruction of normal forms from principal typings algorithms. we briefly discuss the failure of the subject reduction property and some possible solutions for it.", "date_create": "2010-01-25", "area": "cs.lo cs.pl", "authors": ["ventura", "ayala-rinc\u00f3n", "kamareddine"]}, {"idpaper": "1001.4737", "title": "optimization of planck/lfi on--board data handling", "abstract": "to asses stability against 1/f noise, the low frequency instrument (lfi) onboard the planck mission will acquire data at a rate much higher than the data rate allowed by its telemetry bandwith of 35.5 kbps. the data are processed by an onboard pipeline, followed onground by a reversing step. this paper illustrates the lfi scientific onboard processing to fit the allowed datarate. this is a lossy process tuned by using a set of 5 parameters naver, r1, r2, q, o for each of the 44 lfi detectors. the paper quantifies the level of distortion introduced by the onboard processing, epsilonq, as a function of these parameters. it describes the method of optimizing the onboard processing chain. the tuning procedure is based on a optimization algorithm applied to unprocessed and uncompressed raw data provided either by simulations, prelaunch tests or data taken from lfi operating in diagnostic mode. all the needed optimization steps are performed by an automated tool, oca2, which ends with optimized parameters and produces a set of statistical indicators, among them the compression rate cr and epsilonq. for planck/lfi the requirements are cr = 2.4 and epsilonq <= 10% of the rms of the instrumental white noise. to speedup the process an analytical model is developed that is able to extract most of the relevant information on epsilonq and cr as a function of the signal statistics and the processing parameters. this model will be of interest for the instrument data analysis. the method was applied during ground tests when the instrument was operating in conditions representative of flight. optimized parameters were obtained and the performance has been verified, the required data rate of 35.5 kbps has been achieved while keeping epsilonq at a level of 3.8% of white noise rms well within the requirements.", "date_create": "2010-01-26", "area": "astro-ph.im astro-ph.co cs.it math.it", "authors": ["maris", "tomasi", "galeotta", "miccolis", "hildebrandt", "frailis", "rohlfs", "morisset", "zacchei", "bersanelli", "binko", "burigana", "butler", "cuttaia", "chulani", "d'arcangelo", "fogliani", "franceschi", "gasparo", "gomez", "gregorio", "herreros)", "leonardi", "leutenegger", "maggio", "maino", "malaspina", "mandolesi", "manzato", "meharga", "meinhold", "mennella", "pasian", "perrotta", "rebolo", "turler", "zonca"]}, {"idpaper": "1001.4737", "title": "optimization of planck/lfi on--board data handling", "abstract": "to asses stability against 1/f noise, the low frequency instrument (lfi) onboard the planck mission will acquire data at a rate much higher than the data rate allowed by its telemetry bandwith of 35.5 kbps. the data are processed by an onboard pipeline, followed onground by a reversing step. this paper illustrates the lfi scientific onboard processing to fit the allowed datarate. this is a lossy process tuned by using a set of 5 parameters naver, r1, r2, q, o for each of the 44 lfi detectors. the paper quantifies the level of distortion introduced by the onboard processing, epsilonq, as a function of these parameters. it describes the method of optimizing the onboard processing chain. the tuning procedure is based on a optimization algorithm applied to unprocessed and uncompressed raw data provided either by simulations, prelaunch tests or data taken from lfi operating in diagnostic mode. all the needed optimization steps are performed by an automated tool, oca2, which ends with optimized parameters and produces a set of statistical indicators, among them the compression rate cr and epsilonq. for planck/lfi the requirements are cr = 2.4 and epsilonq <= 10% of the rms of the instrumental white noise. to speedup the process an analytical model is developed that is able to extract most of the relevant information on epsilonq and cr as a function of the signal statistics and the processing parameters. this model will be of interest for the instrument data analysis. the method was applied during ground tests when the instrument was operating in conditions representative of flight. optimized parameters were obtained and the performance has been verified, the required data rate of 35.5 kbps has been achieved while keeping epsilonq at a level of 3.8% of white noise rms well within the requirements.", "date_create": "2010-01-26", "area": "astro-ph.im astro-ph.co cs.it math.it", "authors": ["maris", "tomasi", "galeotta", "miccolis", "hildebrandt", "frailis", "rohlfs", "morisset", "zacchei", "bersanelli", "binko", "burigana", "butler", "cuttaia", "chulani", "d'arcangelo", "fogliani", "franceschi", "gasparo", "gomez", "gregorio", "herreros)", "leonardi", "leutenegger", "maggio", "maino", "malaspina", "mandolesi", "manzato", "meharga", "meinhold", "mennella", "pasian", "perrotta", "rebolo", "turler", "zonca"]}, {"idpaper": "1002.0298", "title": "a data capsule framework for web services: providing flexible data   access control to users", "abstract": "this paper introduces the notion of a secure data capsule, which refers to an encapsulation of sensitive user information (such as a credit card number) along with code that implements an interface suitable for the use of such information (such as charging for purchases) by a service (such as an online merchant). in our capsule framework, users provide their data in the form of such capsules to web services rather than raw data. capsules can be deployed in a variety of ways, either on a trusted third party or the user's own computer or at the service itself, through the use of a variety of hardware or software modules, such as a virtual machine monitor or trusted platform module: the only requirement is that the deployment mechanism must ensure that the user's data is only accessed via the interface sanctioned by the user. the framework further allows an user to specify policies regarding which services or machines may host her capsule, what parties are allowed to access the interface, and with what parameters. the combination of interface restrictions and policy control lets us bound the impact of an attacker who compromises the service to gain access to the user's capsule or a malicious insider at the service itself.", "date_create": "2010-02-01", "area": "cs.cr cs.os", "authors": ["kannan", "maniatis", "chun"]}, {"idpaper": "1002.0507", "title": "miniaturized wireless sensor network", "abstract": "this paper addresses an overview of the wireless sensor networks. it is shown that mems/nems technologies and sip concept are well suited for advanced architectures. it is also shown analog architectures have to be compatible with digital signal techniques to develop smart network of microsystem.", "date_create": "2010-02-02", "area": "cs.ni", "authors": ["lecointre", "dragomirescu", "dubuc", "katia", "patrick", "aubert", "muller", "berthou", "gayraud", "plana"]}, {"idpaper": "1002.0712", "title": "performance and stability of the chelonia storage cloud", "abstract": "in this paper we present the chelonia storage cloud middleware. it was designed to fill the requirements gap between those of large, sophisticated scientific collaborations which have adopted the grid paradigm for their distributed storage needs, and of corporate business communities which are gravitating towards the cloud paradigm. the similarities to and differences between chelonia and several well-known grid- and cloud-based storage solutions are commented. the design of chelonia has been chosen to optimize high reliability and scalability of an integrated system of heterogeneous, geographically dispersed storage sites and the ability to easily expand the system dynamically. the architecture and implementation in term of web-services running inside the advanced resource connector hosting environment dameon (arc hed) are described. we present results of tests in both local-area and wide-area networks that demonstrate the fault-tolerance, stability and scalability of chelonia.", "date_create": "2010-02-03", "area": "cs.dc cs.se", "authors": ["nilsen", "toor", "nagy", "mohn", "read"]}, {"idpaper": "1002.0874", "title": "madmx: a novel strategy for maximal dense motif extraction", "abstract": "we develop, analyze and experiment with a new tool, called madmx, which extracts frequent motifs, possibly including don't care characters, from biological sequences. we introduce density, a simple and flexible measure for bounding the number of don't cares in a motif, defined as the ratio of solid (i.e., different from don't care) characters to the total length of the motif. by extracting only maximal dense motifs, madmx reduces the output size and improves performance, while enhancing the quality of the discoveries. the efficiency of our approach relies on a newly defined combining operation, dubbed fusion, which allows for the construction of maximal dense motifs in a bottom-up fashion, while avoiding the generation of nonmaximal ones. we provide experimental evidence of the efficiency and the quality of the motifs returned by madmx", "date_create": "2010-02-03", "area": "cs.ds", "authors": ["grossi", "pietracaprina", "pisanti", "pucci", "upfal", "vandin"]}, {"idpaper": "1002.3711", "title": "theory of regulatory compliance for requirements engineering", "abstract": "regulatory compliance is increasingly being addressed in the practice of requirements engineering as a main stream concern. this paper points out a gap in the theoretical foundations of regulatory compliance, and presents a theory that states (i) what it means for requirements to be compliant, (ii) the compliance problem, i.e., the problem that the engineer should resolve in order to verify whether requirements are compliant, and (iii) testable hypotheses (predictions) about how compliance of requirements is verified. the theory is instantiated by presenting a requirements engineering framework that implements its principles, and is exemplified on a real-world case study.", "date_create": "2010-02-19", "area": "cs.se", "authors": ["jureta", "siena", "mylopoulos", "perini", "susi"]}, {"idpaper": "1003.3866", "title": "the cloud adoption toolkit: addressing the challenges of cloud adoption   in enterprise", "abstract": "cloud computing promises a radical shift in the provisioning of computing resource within the enterprise. this paper: i) describes the challenges that decision makers face when attempting to determine the feasibility of the adoption of cloud computing in their organisations; ii) illustrates a lack of existing work to address the feasibility challenges of cloud adoption in the enterprise; iii) introduces the cloud adoption toolkit that provides a framework to support decision makers in identifying their concerns, and matching these concerns to appropriate tools/techniques that can be used to address them. the paper adopts a position paper methodology such that case study evidence is provided, where available, to support claims. we conclude that the cloud adoption toolkit, whilst still under development, shows signs that it is a useful tool for decision makers as it helps address the feasibility challenges of cloud adoption in the enterprise.", "date_create": "2010-03-19", "area": "cs.dc", "authors": ["khajeh-hosseini", "greenwood", "smith", "sommerville"]}, {"idpaper": "1004.3327", "title": "an efficient hybrid data gathering scheme in wireless sensor networks", "abstract": "this paper has been withdrawn by the author due to a crucial sign error in equation 1", "date_create": "2010-04-19", "area": "cs.ni cs.dc", "authors": ["chakraborty", "mitra", "naskar"]}, {"idpaper": "1004.3932", "title": "modelling immunological memory", "abstract": "accurate immunological models offer the possibility of performing highthroughput experiments in silico that can predict, or at least suggest, in vivo phenomena. in this chapter, we compare various models of immunological memory. we first validate an experimental immunological simulator, developed by the authors, by simulating several theories of immunological memory with known results. we then use the same system to evaluate the predicted effects of a theory of immunological memory. the resulting model has not been explored before in artificial immune systems research, and we compare the simulated in silico output with in vivo measurements. although the theory appears valid, we suggest that there are a common set of reasons why immunological memory models are a useful support tool; not conclusive in themselves.", "date_create": "2010-04-21", "area": "cs.ai cs.ne q-bio.cb", "authors": ["garret", "robbins", "walker", "wilson", "aickelin"]}, {"idpaper": "1004.5529", "title": "high-rate vector quantization for the neyman-pearson detection of   correlated processes", "abstract": "this paper investigates the effect of quantization on the performance of the neyman-pearson test. it is assumed that a sensing unit observes samples of a correlated stationary ergodic multivariate process. each sample is passed through an n-point quantizer and transmitted to a decision device which performs a binary hypothesis test. for any false alarm level, it is shown that the miss probability of the neyman-pearson test converges to zero exponentially as the number of samples tends to infinity, assuming that the observed process satisfies certain mixing conditions. the main contribution of this paper is to provide a compact closed-form expression of the error exponent in the high-rate regime i.e., when the number n of quantization levels tends to infinity, generalizing previous results of gupta and hero to the case of non-independent observations. if d represents the dimension of one sample, it is proved that the error exponent converges at rate n^{2/d} to the one obtained in the absence of quantization. as an application, relevant high-rate quantization strategies which lead to a large error exponent are determined. numerical results indicate that the proposed quantization rule can yield better performance than existing ones in terms of detection error.", "date_create": "2010-04-30", "area": "cs.it math.it math.pr math.st stat.th", "authors": ["villard", "bianchi"]}, {"idpaper": "1005.0418", "title": "lower bounds on near neighbor search via metric expansion", "abstract": "in this paper we show how the complexity of performing nearest neighbor (nns) search on a metric space is related to the expansion of the metric space. given a metric space we look at the graph obtained by connecting every pair of points within a certain distance $r$ . we then look at various notions of expansion in this graph relating them to the cell probe complexity of nns for randomized and deterministic, exact and approximate algorithms. for example if the graph has node expansion $\\phi$ then we show that any deterministic $t$-probe data structure for $n$ points must use space $s$ where $(st/n)^t > \\phi$. we show similar results for randomized algorithms as well. these relationships can be used to derive most of the known lower bounds in the well known metric spaces such as $l_1$, $l_2$, $l_\\infty$ by simply computing their expansion. in the process, we strengthen and generalize our previous results (focs 2008). additionally, we unify the approach in that work and the communication complexity based approach. our work reduces the problem of proving cell probe lower bounds of near neighbor search to computing the appropriate expansion parameter. in our results, as in all previous results, the dependence on $t$ is weak; that is, the bound drops exponentially in $t$. we show a much stronger (tight) time-space tradeoff for the class of dynamic low contention data structures. these are data structures that supports updates in the data set and that do not look up any single cell too often.", "date_create": "2010-05-03", "area": "cs.ds cs.cg", "authors": ["panigrahy", "talwar", "wieder"]}, {"idpaper": "1005.0595", "title": "software design document, testing, and deployment and configuration   management of the uuis - a team 1 comp5541-w10 project approach", "abstract": "the document presents a detailed description of the designs for the implementation of the unified university inventory system for the imaginary university of arctica. the document, through numerous diagrams and ui samples, gives the structure of the system and the functions of its modules. it also gives test cases and reports that support the system's architecture and design.", "date_create": "2010-05-04", "area": "cs.se", "authors": ["sankaran", "samsonyuk", "attar", "parham", "zayikina", "rifai", "lepin", "hassan"]}, {"idpaper": "1006.3520", "title": "information distance", "abstract": "while kolmogorov complexity is the accepted absolute measure of information content in an individual finite object, a similarly absolute notion is needed for the information distance between two individual objects, for example, two pictures. we give several natural definitions of a universal information metric, based on length of shortest programs for either ordinary computations or reversible (dissipationless) computations. it turns out that these definitions are equivalent up to an additive logarithmic term. we show that the information distance is a universal cognitive similarity distance. we investigate the maximal correlation of the shortest programs involved, the maximal uncorrelation of programs (a generalization of the slepian-wolf theorem of classical information theory), and the density properties of the discrete metric spaces induced by the information distances. a related distance measures the amount of nonreversibility of a computation. using the physical theory of reversible computation, we give an appropriate (universal, anti-symmetric, and transitive) measure of the thermodynamic work required to transform one object in another object by the most efficient process. information distance between individual objects is needed in pattern recognition where one wants to express effective notions of \"pattern similarity\" or \"cognitive similarity\" between individual objects and in thermodynamics of computation where one wants to analyse the energy dissipation of a computation from a particular input to a particular output.", "date_create": "2010-06-17", "area": "cs.it math.it math.pr physics.data-an", "authors": ["bennett", "gacs", "li", "vitanyi", "zurek"]}, {"idpaper": "1006.5309", "title": "data partitioning for parallel entity matching", "abstract": "entity matching is an important and difficult step for integrating web data. to reduce the typically high execution time for matching we investigate how we can perform entity matching in parallel on a distributed infrastructure. we propose different strategies to partition the input data and generate multiple match tasks that can be independently executed. one of our strategies supports both, blocking to reduce the search space for matching and parallel matching to improve efficiency. special attention is given to the number and size of data partitions as they impact the overall communication overhead and memory requirements of individual match tasks. we have developed a service-based distributed infrastructure for the parallel execution of match workflows. we evaluate our approach in detail for different match strategies for matching real-world product data of different web shops. we also consider caching of in-put entities and affinity-based scheduling of match tasks.", "date_create": "2010-06-28", "area": "cs.dc", "authors": ["kirsten", "kolb", "hartung", "gro\u00df", "k\u00f6pcke", "rahm"]}, {"idpaper": "1007.0050", "title": "cloud scheduler: a resource manager for distributed compute clouds", "abstract": "the availability of infrastructure-as-a-service (iaas) computing clouds gives researchers access to a large set of new resources for running complex scientific applications. however, exploiting cloud resources for large numbers of jobs requires significant effort and expertise. in order to make it simple and transparent for researchers to deploy their applications, we have developed a virtual machine resource manager (cloud scheduler) for distributed compute clouds. cloud scheduler boots and manages the user-customized virtual machines in response to a user's job submission. we describe the motivation and design of the cloud scheduler and present results on its use on both science and commercial clouds.", "date_create": "2010-06-30", "area": "cs.dc", "authors": ["armstrong", "agarwal", "bishop", "charbonneau", "desmarais", "fransham", "hill", "gable", "gaudet", "goliath", "impey", "leavett-brown", "ouellete", "paterson", "pritchet", "penfold-brown", "podaima", "schade", "sobie"]}, {"idpaper": "1007.0050", "title": "cloud scheduler: a resource manager for distributed compute clouds", "abstract": "the availability of infrastructure-as-a-service (iaas) computing clouds gives researchers access to a large set of new resources for running complex scientific applications. however, exploiting cloud resources for large numbers of jobs requires significant effort and expertise. in order to make it simple and transparent for researchers to deploy their applications, we have developed a virtual machine resource manager (cloud scheduler) for distributed compute clouds. cloud scheduler boots and manages the user-customized virtual machines in response to a user's job submission. we describe the motivation and design of the cloud scheduler and present results on its use on both science and commercial clouds.", "date_create": "2010-06-30", "area": "cs.dc", "authors": ["armstrong", "agarwal", "bishop", "charbonneau", "desmarais", "fransham", "hill", "gable", "gaudet", "goliath", "impey", "leavett-brown", "ouellete", "paterson", "pritchet", "penfold-brown", "podaima", "schade", "sobie"]}, {"idpaper": "1007.0050", "title": "cloud scheduler: a resource manager for distributed compute clouds", "abstract": "the availability of infrastructure-as-a-service (iaas) computing clouds gives researchers access to a large set of new resources for running complex scientific applications. however, exploiting cloud resources for large numbers of jobs requires significant effort and expertise. in order to make it simple and transparent for researchers to deploy their applications, we have developed a virtual machine resource manager (cloud scheduler) for distributed compute clouds. cloud scheduler boots and manages the user-customized virtual machines in response to a user's job submission. we describe the motivation and design of the cloud scheduler and present results on its use on both science and commercial clouds.", "date_create": "2010-06-30", "area": "cs.dc", "authors": ["armstrong", "agarwal", "bishop", "charbonneau", "desmarais", "fransham", "hill", "gable", "gaudet", "goliath", "impey", "leavett-brown", "ouellete", "paterson", "pritchet", "penfold-brown", "podaima", "schade", "sobie"]}, {"idpaper": "1008.1357", "title": "social networks and spin glasses", "abstract": "the networks formed from the links between telephones observed in a month's call detail records (cdrs) in the uk are analyzed, looking for the characteristics thought to identify a communications network or a social network. some novel methods are employed. we find similarities to both types of network. we conclude that, just as analogies to spin glasses have proved fruitful for optimization of large scale practical problems, there will be opportunities to exploit a statistical mechanics of the formation and dynamics of social networks in today's electronically connected world.", "date_create": "2010-08-07", "area": "cs.si cs.cy", "authors": ["kirkpatrick", "kulakovsky", "cebrian", "pentland"]}, {"idpaper": "1008.3775", "title": "monte carlo methods for top-k personalized pagerank lists and name   disambiguation", "abstract": "we study a problem of quick detection of top-k personalized pagerank lists. this problem has a number of important applications such as finding local cuts in large graphs, estimation of similarity distance and name disambiguation. in particular, we apply our results to construct efficient algorithms for the person name disambiguation problem. we argue that when finding top-k personalized pagerank lists two observations are important. firstly, it is crucial that we detect fast the top-k most important neighbours of a node, while the exact order in the top-k list as well as the exact values of pagerank are by far not so crucial. secondly, a little number of wrong elements in top-k lists do not really degrade the quality of top-k lists, but it can lead to significant computational saving. based on these two key observations we propose monte carlo methods for fast detection of top-k personalized pagerank lists. we provide performance evaluation of the proposed methods and supply stopping criteria. then, we apply the methods to the person name disambiguation problem. the developed algorithm for the person name disambiguation problem has achieved the second place in the weps 2010 competition.", "date_create": "2010-08-23", "area": "cs.ni", "authors": ["avrachenkov", "litvak", "nemirovsky", "smirnova", "sokol"]}, {"idpaper": "1008.3775", "title": "monte carlo methods for top-k personalized pagerank lists and name   disambiguation", "abstract": "we study a problem of quick detection of top-k personalized pagerank lists. this problem has a number of important applications such as finding local cuts in large graphs, estimation of similarity distance and name disambiguation. in particular, we apply our results to construct efficient algorithms for the person name disambiguation problem. we argue that when finding top-k personalized pagerank lists two observations are important. firstly, it is crucial that we detect fast the top-k most important neighbours of a node, while the exact order in the top-k list as well as the exact values of pagerank are by far not so crucial. secondly, a little number of wrong elements in top-k lists do not really degrade the quality of top-k lists, but it can lead to significant computational saving. based on these two key observations we propose monte carlo methods for fast detection of top-k personalized pagerank lists. we provide performance evaluation of the proposed methods and supply stopping criteria. then, we apply the methods to the person name disambiguation problem. the developed algorithm for the person name disambiguation problem has achieved the second place in the weps 2010 competition.", "date_create": "2010-08-23", "area": "cs.ni", "authors": ["avrachenkov", "litvak", "nemirovsky", "smirnova", "sokol"]}, {"idpaper": "1009.3714", "title": "browser-based analysis of web framework applications", "abstract": "although web applications evolved to mature solutions providing sophisticated user experience, they also became complex for the same reason. complexity primarily affects the server-side generation of dynamic pages as they are aggregated from multiple sources and as there are lots of possible processing paths depending on parameters. browser-based tests are an adequate instrument to detect errors within generated web pages considering the server-side process and path complexity a black box. however, these tests do not detect the cause of an error which has to be located manually instead. this paper proposes to generate metadata on the paths and parts involved during server-side processing to facilitate backtracking origins of detected errors at development time. while there are several possible points of interest to observe for backtracking, this paper focuses user interface components of web frameworks.", "date_create": "2010-09-20", "area": "cs.se", "authors": ["kersten", "goedicke"]}, {"idpaper": "1009.3800", "title": "distributed work stealing for constraint solving", "abstract": "with the dissemination of affordable parallel and distributed hardware, parallel and distributed constraint solving has lately been the focus of some attention. to effectually apply the power of distributed computational systems, there must be an effective sharing of the work involved in the search for a solution to a constraint satisfaction problem (csp) between all the participating agents, and it must happen dynamically, since it is hard to predict the effort associated with the exploration of some part of the search space. we describe and provide an initial experimental assessment of an implementation of a work stealing-based approach to distributed csp solving.", "date_create": "2010-09-20", "area": "cs.pl", "authors": ["pedro", "abreu"]}, {"idpaper": "1009.5718", "title": "monitoring wild animal communities with arrays of motion sensitive   camera traps", "abstract": "studying animal movement and distribution is of critical importance to addressing environmental challenges including invasive species, infectious diseases, climate and land-use change. motion sensitive camera traps offer a visual sensor to record the presence of a broad range of species providing location -specific information on movement and behavior. modern digital camera traps that record video present new analytical opportunities, but also new data management challenges. this paper describes our experience with a terrestrial animal monitoring system at barro colorado island, panama. our camera network captured the spatio-temporal dynamics of terrestrial bird and mammal activity at the site - data relevant to immediate science questions, and long-term conservation issues. we believe that the experience gained and lessons learned during our year long deployment and testing of the camera traps as well as the developed solutions are applicable to broader sensor network applications and are valuable for the advancement of the sensor network research. we suggest that the continued development of these hardware, software, and analytical tools, in concert, offer an exciting sensor-network solution to monitoring of animal populations which could realistically scale over larger areas and time spans.", "date_create": "2010-09-28", "area": "cs.ni", "authors": ["kays", "tilak", "kranstauber", "jansen", "carbone", "rowcliffe", "fountain", "eggert", "he"]}, {"idpaper": "1010.0506", "title": "first results of the soap project. open access publishing in 2010", "abstract": "the soap (study of open access publishing) project has compiled data on the present offer for open access publishing in online peer-reviewed journals. starting from the directory of open access journals, several sources of data are considered, including inspection of journal web site and direct inquiries within the publishing industry. several results are derived and discussed, together with their correlations: the number of open access journals and articles; their subject area; the starting date of open access journals; the size and business models of open access publishers; the licensing models; the presence of an impact factor; the uptake of hybrid open access.", "date_create": "2010-10-04", "area": "cs.dl", "authors": ["dallmeier-tiessen", "darby", "goerner", "hyppoelae", "igo-kemenes", "kahn", "lambert", "lengenfelder", "leonard", "mele", "polydoratou", "ross", "ruiz-perez", "schimmer", "swaisland", "van der stelt"]}, {"idpaper": "1011.1296", "title": "privately releasing conjunctions and the statistical query barrier", "abstract": "suppose we would like to know all answers to a set of statistical queries c on a data set up to small error, but we can only access the data itself using statistical queries. a trivial solution is to exhaustively ask all queries in c. can we do any better?   + we show that the number of statistical queries necessary and sufficient for this task is---up to polynomial factors---equal to the agnostic learning complexity of c in kearns' statistical query (sq) model. this gives a complete answer to the question when running time is not a concern.   + we then show that the problem can be solved efficiently (allowing arbitrary error on a small fraction of queries) whenever the answers to c can be described by a submodular function. this includes many natural concept classes, such as graph cuts and boolean disjunctions and conjunctions.   while interesting from a learning theoretic point of view, our main applications are in privacy-preserving data analysis:   here, our second result leads to the first algorithm that efficiently releases differentially private answers to of all boolean conjunctions with 1% average error. this presents significant progress on a key open problem in privacy-preserving data analysis.   our first result on the other hand gives unconditional lower bounds on any differentially private algorithm that admits a (potentially non-privacy-preserving) implementation using only statistical queries. not only our algorithms, but also most known private algorithms can be implemented using only statistical queries, and hence are constrained by these lower bounds. our result therefore isolates the complexity of agnostic learning in the sq-model as a new barrier in the design of differentially private algorithms.", "date_create": "2010-11-04", "area": "cs.ds cs.cr cs.lg", "authors": ["gupta", "hardt", "roth", "ullman"]}, {"idpaper": "1011.1296", "title": "privately releasing conjunctions and the statistical query barrier", "abstract": "suppose we would like to know all answers to a set of statistical queries c on a data set up to small error, but we can only access the data itself using statistical queries. a trivial solution is to exhaustively ask all queries in c. can we do any better?   + we show that the number of statistical queries necessary and sufficient for this task is---up to polynomial factors---equal to the agnostic learning complexity of c in kearns' statistical query (sq) model. this gives a complete answer to the question when running time is not a concern.   + we then show that the problem can be solved efficiently (allowing arbitrary error on a small fraction of queries) whenever the answers to c can be described by a submodular function. this includes many natural concept classes, such as graph cuts and boolean disjunctions and conjunctions.   while interesting from a learning theoretic point of view, our main applications are in privacy-preserving data analysis:   here, our second result leads to the first algorithm that efficiently releases differentially private answers to of all boolean conjunctions with 1% average error. this presents significant progress on a key open problem in privacy-preserving data analysis.   our first result on the other hand gives unconditional lower bounds on any differentially private algorithm that admits a (potentially non-privacy-preserving) implementation using only statistical queries. not only our algorithms, but also most known private algorithms can be implemented using only statistical queries, and hence are constrained by these lower bounds. our result therefore isolates the complexity of agnostic learning in the sq-model as a new barrier in the design of differentially private algorithms.", "date_create": "2010-11-04", "area": "cs.ds cs.cr cs.lg", "authors": ["gupta", "hardt", "roth", "ullman"]}, {"idpaper": "1011.6224", "title": "classifying extremely imbalanced data sets", "abstract": "imbalanced data sets containing much more background than signal instances are very common in particle physics, and will also be characteristic for the upcoming analyses of lhc data. following up the work presented at acat 2008, we use the multivariate technique presented there (a rule growing algorithm with the meta-methods bagging and instance weighting) on much more imbalanced data sets, especially a selection of d0 decays without the use of particle identification. it turns out that the quality of the result strongly depends on the number of background instances used for training. we discuss methods to exploit this in order to improve the results significantly, and how to handle and reduce the size of large training sets without loss of result quality in general. we will also comment on how to take into account statistical fluctuation in receiver operation characteristic curves (roc) for comparing classifier methods.", "date_create": "2010-11-29", "area": "physics.data-an cs.lg hep-ex stat.ml", "authors": ["britsch", "gagunashvili", "schmelling"]}, {"idpaper": "1012.0253", "title": "apenet+: a 3d toroidal network enabling petaflops scale lattice qcd   simulations on commodity clusters", "abstract": "many scientific computations need multi-node parallelism for matching up both space (memory) and time (speed) ever-increasing requirements. the use of gpus as accelerators introduces yet another level of complexity for the programmer and may potentially result in large overheads due to the complex memory hierarchy. additionally, top-notch problems may easily employ more than a petaflops of sustained computing power, requiring thousands of gpus orchestrated with some parallel programming model. here we describe apenet+, the new generation of our interconnect, which scales up to tens of thousands of nodes with linear cost, thus improving the price/performance ratio on large clusters. the project target is the development of the apelink+ host adapter featuring a low latency, high bandwidth direct network, state-of-the-art wire speeds on the links and a pcie x8 gen2 host interface. it features hardware support for the rdma programming model and experimental acceleration of gpu networking. a linux kernel driver, a set of low-level rdma apis and an openmpi library driver are available, allowing for painless porting of standard applications. finally, we give an insight of future work and intended developments.", "date_create": "2010-12-01", "area": "hep-lat cs.dc", "authors": ["ammendola", "biagioni", "frezza", "cicero", "lonardo", "paolucci", "petronzio", "rossetti", "salamon", "salina", "simula", "tantalo", "tosoratto", "vicini"]}, {"idpaper": "1012.0253", "title": "apenet+: a 3d toroidal network enabling petaflops scale lattice qcd   simulations on commodity clusters", "abstract": "many scientific computations need multi-node parallelism for matching up both space (memory) and time (speed) ever-increasing requirements. the use of gpus as accelerators introduces yet another level of complexity for the programmer and may potentially result in large overheads due to the complex memory hierarchy. additionally, top-notch problems may easily employ more than a petaflops of sustained computing power, requiring thousands of gpus orchestrated with some parallel programming model. here we describe apenet+, the new generation of our interconnect, which scales up to tens of thousands of nodes with linear cost, thus improving the price/performance ratio on large clusters. the project target is the development of the apelink+ host adapter featuring a low latency, high bandwidth direct network, state-of-the-art wire speeds on the links and a pcie x8 gen2 host interface. it features hardware support for the rdma programming model and experimental acceleration of gpu networking. a linux kernel driver, a set of low-level rdma apis and an openmpi library driver are available, allowing for painless porting of standard applications. finally, we give an insight of future work and intended developments.", "date_create": "2010-12-01", "area": "hep-lat cs.dc", "authors": ["ammendola", "biagioni", "frezza", "cicero", "lonardo", "paolucci", "petronzio", "rossetti", "salamon", "salina", "simula", "tantalo", "tosoratto", "vicini"]}, {"idpaper": "1012.1621", "title": "yeastmed: an xml-based system for biological data integration of yeast", "abstract": "a key goal of bioinformatics is to create database systems and software platforms capable of storing and analysing large sets of biological data. hundreds of biological databases are now available and provide access to huge amount of biological data. sgd, yeastract, cygd-mips, biogrid and phosphogrid are five of the most visited databases by the yeast community. these sources provide complementary data on biological entities. biologists are brought systematically to query these data sources in order to analyse the results of their experiments. because of the heterogeneity of these sources, querying them separately and then manually combining the returned result is a complex and laborious task. to provide transparent and simultaneous access to these sources, we have developed a mediator-based system called yeastmed. in this paper, we present yeastmed focusing on its architecture.", "date_create": "2010-12-07", "area": "cs.db", "authors": ["briache", "marrakchi", "kerzazi", "navas-delgado", "montes", "hassani", "lairini"]}, {"idpaper": "1103.1482", "title": "the planetary system: executable science, technology, engineering and   math papers", "abstract": "executable scientific papers contain not just layouted text for reading. they contain, or link to, machine-comprehensible representations of the scientific findings or experiments they describe. client-side players can thus enable readers to \"check, manipulate and explore the result space\". we have realized executable papers in the stem domain with the planetary system. semantic annotations associate the papers with a content commons holding the background ontology, the annotations are exposed as linked data, and a frontend player application hooks modular interactive services into the semantic annotations.", "date_create": "2011-03-08", "area": "cs.dl cs.ms", "authors": ["lange", "kohlhase", "david", "ginev", "kohlhase", "matican", "mirea", "zholudev"]}, {"idpaper": "1103.3102", "title": "human-assisted graph search: it's okay to ask questions", "abstract": "we consider the problem of human-assisted graph search: given a directed acyclic graph with some (unknown) target node(s), we consider the problem of finding the target node(s) by asking an omniscient human questions of the form \"is there a target node that is reachable from the current node?\". this general problem has applications in many domains that can utilize human intelligence, including curation of hierarchies, debugging workflows, image segmentation and categorization, interactive search and filter synthesis. to our knowledge, this work provides the first formal algorithmic study of the optimization of human computation for this problem. we study various dimensions of the problem space, providing algorithms and complexity results. our framework and algorithms can be used in the design of an optimizer for crowd-sourcing platforms such as mechanical turk.", "date_create": "2011-03-16", "area": "cs.db cs.ds", "authors": ["parameswaran", "sarma", "garcia-molina", "polyzotis", "widom"]}, {"idpaper": "1103.4410", "title": "distributed inference and query processing for rfid tracking and   monitoring", "abstract": "in this paper, we present the design of a scalable, distributed stream processing system for rfid tracking and monitoring. since rfid data lacks containment and location information that is key to query processing, we propose to combine location and containment inference with stream query processing in a single architecture, with inference as an enabling mechanism for high-level query processing. we further consider challenges in instantiating such a system in large distributed settings and design techniques for distributed inference and query processing. our experimental results, using both real-world data and large synthetic traces, demonstrate the accuracy, efficiency, and scalability of our proposed techniques.", "date_create": "2011-03-22", "area": "cs.db", "authors": ["cao", "sutton", "diao", "shenoy"]}, {"idpaper": "1104.2515", "title": "close encounters in a pediatric ward: measuring face-to-face proximity   and mixing patterns with wearable sensors", "abstract": "nosocomial infections place a substantial burden on health care systems and represent a major issue in current public health, requiring notable efforts for its prevention. understanding the dynamics of infection transmission in a hospital setting is essential for tailoring interventions and predicting the spread among individuals. mathematical models need to be informed with accurate data on contacts among individuals. we used wearable active radio-frequency identification devices to detect face-to-face contacts among individuals with a spatial resolution of about 1.5 meters, and a time resolution of 20 seconds. the study was conducted in a general pediatrics hospital ward, during a one-week period, and included 119 participants. nearly 16,000 contacts were recorded during the study, with a median of approximately 20 contacts per participants per day. overall, 25% of the contacts involved a ward assistant, 23% a nurse, 22% a patient, 22% a caregiver, and 8% a physician. the majority of contacts were of brief duration, but long and frequent contacts especially between patients and caregivers were also found. in the setting under study, caregivers do not represent a significant potential for infection spread to a large number of individuals, as their interactions mainly involve the corresponding patient. nurses would deserve priority in prevention strategies due to their central role in the potential propagation paths of infections. our study shows the feasibility of accurate and reproducible measures of the pattern of contacts in a hospital setting. the results are particularly useful for the study of the spread of respiratory infections, for monitoring critical patterns, and for setting up tailored prevention strategies. proximity-sensing technology should be considered as a valuable tool for measuring such patterns and evaluating nosocomial prevention strategies in specific settings.", "date_create": "2011-04-13", "area": "q-bio.qm cs.hc", "authors": ["isella", "romano", "barrat", "cattuto", "colizza", "broeck", "gesualdo", "pandolfi", "rav\u00e0", "rizzo", "tozzi"]}, {"idpaper": "1108.5434", "title": "unit testing in aspide", "abstract": "answer set programming (asp) is a declarative logic programming formalism, which is employed nowadays in both academic and industrial real-world applications. although some tools for supporting the development of asp programs have been proposed in the last few years, the crucial task of testing asp programs received less attention, and is an achilles' heel of the available programming environments.   in this paper we present a language for specifying and running unit tests on asp programs. the testing language has been implemented in aspide, a comprehensive ide for asp, which supports the entire life-cycle of asp development with a collection of user-friendly graphical tools for program composition, testing, debugging, profiling, solver execution configuration, and output-handling.", "date_create": "2011-08-27", "area": "cs.pl", "authors": ["febbraro", "leone", "reale", "ricca"]}, {"idpaper": "1109.2174", "title": "a note on total and paired domination of cartesian product graphs", "abstract": "a dominating set $d$ for a graph $g$ is a subset of $v(g)$ such that any vertex not in $d$ has at least one neighbor in $d$. the domination number $\\gamma(g)$ is the size of a minimum dominating set in $g$. vizing's conjecture from 1968 states that for the cartesian product of graphs $g$ and $h$, $\\gamma(g) \\gamma(h) \\leq \\gamma(g \\box h)$, and clark and suen (2000) proved that $\\gamma(g) \\gamma(h) \\leq 2\\gamma(g \\box h)$. in this paper, we modify the approach of clark and suen to prove a variety of similar bounds related to total and paired domination, and also extend these bounds to the $n$-cartesian product of graphs $a^1$ through $a^n$.", "date_create": "2011-09-09", "area": "math.co cs.dm", "authors": ["choudhary", "margulies", "hicks"]}, {"idpaper": "1109.3444", "title": "large-scale complex it systems", "abstract": "this paper explores the issues around the construction of large-scale complex systems which are built as 'systems of systems' and suggests that there are fundamental reasons, derived from the inherent complexity in these systems, why our current software engineering methods and techniques cannot be scaled up to cope with the engineering challenges of constructing such systems. it then goes on to propose a research and education agenda for software engineering that identifies the major challenges and issues in the development of large-scale complex, software-intensive systems. central to this is the notion that we cannot separate software from the socio-technical environment in which it is used.", "date_create": "2011-09-15", "area": "cs.se cs.cy", "authors": ["sommerville", "cliff", "calinescu", "keen", "kelly", "kwiatkowska", "mcdermid", "paige"]}, {"idpaper": "1110.6647", "title": "on predictive modeling for optimizing transaction execution in parallel   oltp systems", "abstract": "a new emerging class of parallel database management systems (dbms) is designed to take advantage of the partitionable workloads of on-line transaction processing (oltp) applications. transactions in these systems are optimized to execute to completion on a single node in a shared-nothing cluster without needing to coordinate with other nodes or use expensive concurrency control measures. but some oltp applications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. these distributed transactions access data not stored within their local partitions and subsequently require more heavy-weight concurrency control protocols. further difficulties arise when the transaction's execution properties, such as the number of partitions it may need to access or whether it will abort, are not known beforehand. the dbms could mitigate these performance issues if it is provided with additional information about transactions. thus, in this paper we present a markov model-based approach for automatically selecting which optimizations a dbms could use, namely (1) more efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced undo logging, and (4) speculative execution. to evaluate our techniques, we implemented our models and integrated them into a parallel, main-memory oltp dbms to show that we can improve the performance of applications with diverse workloads.", "date_create": "2011-10-30", "area": "cs.db", "authors": ["pavlo", "jones", "zdonik"]}, {"idpaper": "1111.0060", "title": "a constraint programming approach for solving a queueing control problem", "abstract": "in a facility with front room and back room operations, it is useful to switch workers between the rooms in order to cope with changing customer demand. assuming stochastic customer arrival and service times, we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work. three novel constraint programming models and several shaving procedures for these models are presented. experimental results show that a model based on closed-form expressions together with a combination of shaving procedures is the most efficient. this model is able to find and prove optimal solutions for many problem instances within a reasonable run-time. previously, the only available approach was a heuristic algorithm. furthermore, a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time, while achieving the same performance in terms of proving optimality as the pure constraint programming model. this is the first work of which we are aware that solves such queueing-based problems with constraint programming.", "date_create": "2011-10-31", "area": "cs.ai", "authors": ["terekhov", "beck"]}, {"idpaper": "1111.2904", "title": "spatio-temporal analysis of topic popularity in twitter", "abstract": "we present the first comprehensive characterization of the diffusion of ideas on twitter, studying more than 4000 topics that include both popular and less popular topics. on a data set containing approximately 10 million users and a comprehensive scraping of all the tweets posted by these users between june 2009 and august 2009 (approximately 200 million tweets), we perform a rigorous temporal and spatial analysis, investigating the time-evolving properties of the subgraphs formed by the users discussing each topic. we focus on two different notions of the spatial: the network topology formed by follower-following links on twitter, and the geospatial location of the users. we investigate the effect of initiators on the popularity of topics and find that users with a high number of followers have a strong impact on popularity. we deduce that topics become popular when disjoint clusters of users discussing them begin to merge and form one giant component that grows to cover a significant fraction of the network. our geospatial analysis shows that highly popular topics are those that cross regional boundaries aggressively.", "date_create": "2011-11-12", "area": "cs.si cs.cy", "authors": ["ardon", "bagchi", "mahanti", "ruhela", "seth", "tripathy", "triukose"]}, {"idpaper": "1111.3270", "title": "mining biclusters of similar values with triadic concept analysis", "abstract": "biclustering numerical data became a popular data-mining task in the beginning of 2000's, especially for analysing gene expression data. a bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table. so called biclusters of similar values can be thought as maximal sub-tables with close values. only few methods address a complete, correct and non redundant enumeration of such patterns, which is a well-known intractable problem, while no formal framework exists. in this paper, we introduce important links between biclustering and formal concept analysis. more specifically, we originally show that triadic concept analysis (tca), provides a nice mathematical framework for biclustering. interestingly, existing algorithms of tca, that usually apply on binary data, can be used (directly or with slight modifications) after a preprocessing step for extracting maximal biclusters of similar values.", "date_create": "2011-11-14", "area": "cs.ds cs.ai cs.db", "authors": ["kaytoue", "kuznetsov", "macko", "meira", "napoli"]}]